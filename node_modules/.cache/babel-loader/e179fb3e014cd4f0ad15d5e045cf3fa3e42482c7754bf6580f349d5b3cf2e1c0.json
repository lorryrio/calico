{"ast":null,"code":"/**\n * @file Definitions of all models available in Transformers.js.\n * \n * **Example:** Load and run an `AutoModel`.\n * \n * ```javascript\n * import { AutoModel, AutoTokenizer } from '@xenova/transformers';\n *\n * let tokenizer = await AutoTokenizer.from_pretrained('Xenova/bert-base-uncased');\n * let model = await AutoModel.from_pretrained('Xenova/bert-base-uncased');\n *\n * let inputs = await tokenizer('I love transformers!');\n * let { logits } = await model(inputs);\n * // Tensor {\n * //     data: Float32Array(183132) [-7.117443084716797, -7.107812881469727, -7.092104911804199, ...]\n * //     dims: (3) [1, 6, 30522],\n * //     type: \"float32\",\n * //     size: 183132,\n * // }\n * ```\n * \n * We also provide other `AutoModel`s (listed below), which you can use in the same way as the Python library. For example:\n * \n * **Example:** Load and run an `AutoModelForSeq2SeqLM`.\n * ```javascript\n * import { AutoModelForSeq2SeqLM, AutoTokenizer } from '@xenova/transformers';\n * \n * let tokenizer = await AutoTokenizer.from_pretrained('Xenova/t5-small');\n * let model = await AutoModelForSeq2SeqLM.from_pretrained('Xenova/t5-small');\n *\n * let { input_ids } = await tokenizer('translate English to German: I love transformers!');\n * let outputs = await model.generate(input_ids);\n * let decoded = tokenizer.decode(outputs[0], { skip_special_tokens: true });\n * // 'Ich liebe Transformatoren!'\n * ```\n * \n * @module models\n */\n\nimport { AutoConfig } from './configs.js';\nimport { Callable, isIntegralNumber, isTypedArray, mergeArrays } from './utils/core.js';\nimport { getModelFile, getModelJSON } from './utils/hub.js';\nimport { LogitsProcessorList, GenerationConfig, ForceTokensLogitsProcessor, ForcedBOSTokenLogitsProcessor, ForcedEOSTokenLogitsProcessor, SuppressTokensAtBeginLogitsProcessor, WhisperTimeStampLogitsProcessor, NoRepeatNGramLogitsProcessor, RepetitionPenaltyLogitsProcessor, NoBadWordsLogitsProcessor, MinLengthLogitsProcessor, MinNewTokensLengthLogitsProcessor, Sampler } from './utils/generation.js';\nimport { cat, dynamicTimeWarping, mean, ones_like, stack, std_mean, Tensor } from './utils/tensor.js';\nimport { executionProviders, ONNX } from './backends/onnx.js';\nimport { medianFilter } from './transformers.js';\nconst {\n  InferenceSession,\n  Tensor: ONNXTensor,\n  env\n} = ONNX;\n\n/** @typedef {import('onnxruntime-web').InferenceSession} InferenceSession */\n\n//////////////////////////////////////////////////\n// Model types: used internally\nconst MODEL_TYPES = {\n  EncoderOnly: 0,\n  EncoderDecoder: 1,\n  Seq2Seq: 2,\n  Vision2Seq: 3,\n  DecoderOnly: 4,\n  MaskGeneration: 5\n};\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// Helper functions\n\n// NOTE: These will be populated fully later\nconst MODEL_TYPE_MAPPING = new Map();\nconst MODEL_NAME_TO_CLASS_MAPPING = new Map();\nconst MODEL_CLASS_TO_NAME_MAPPING = new Map();\n\n/**\n * Constructs an InferenceSession using a model file located at the specified path.\n * @param {string} pretrained_model_name_or_path The path to the directory containing the model file.\n * @param {string} fileName The name of the model file.\n * @param {import('./utils/hub.js').PretrainedOptions} options Additional options for loading the model.\n * @returns {Promise<InferenceSession>} A Promise that resolves to an InferenceSession object.\n * @private\n */\nasync function constructSession(pretrained_model_name_or_path, fileName, options) {\n  // TODO add option for user to force specify their desired execution provider\n  let modelFileName = `onnx/${fileName}${options.quantized ? '_quantized' : ''}.onnx`;\n  let buffer = await getModelFile(pretrained_model_name_or_path, modelFileName, true, options);\n  try {\n    return await InferenceSession.create(buffer, {\n      executionProviders\n    });\n  } catch (err) {\n    // If the execution provided was only wasm, throw the error\n    if (executionProviders.length === 1 && executionProviders[0] === 'wasm') {\n      throw err;\n    }\n    console.warn(err);\n    console.warn('Something went wrong during model construction (most likely a missing operation). ' + 'Using `wasm` as a fallback. ');\n    return await InferenceSession.create(buffer, {\n      executionProviders: ['wasm']\n    });\n  }\n}\n\n/**\n * Validate model inputs\n * @param {InferenceSession} session The InferenceSession object that will be run.\n * @param {Record<string, Tensor>} inputs The inputs to check.\n * @returns {Record<string, Tensor>} The checked inputs.\n * @throws {Error} If any inputs are missing.\n * @private\n */\nfunction validateInputs(session, inputs) {\n  /**\n   * NOTE: Create either a shallow or deep copy based on `onnx.wasm.proxy`\n   * @type {Record<string, Tensor>}\n   */\n  const checkedInputs = Object.create(null);\n  const missingInputs = [];\n  for (const inputName of session.inputNames) {\n    const tensor = inputs[inputName];\n    // Rare case where one of the model's input names corresponds to a built-in\n    // object name (e.g., toString), which would cause a simple (!tensor) check to fail,\n    // because it's not undefined but a function.\n    if (!(tensor instanceof Tensor)) {\n      missingInputs.push(inputName);\n      continue;\n    }\n    // NOTE: When `env.wasm.proxy is true` the tensor is moved across the Worker\n    // boundary, transferring ownership to the worker and invalidating the tensor.\n    // So, in this case, we simply sacrifice a clone for it.\n    checkedInputs[inputName] = env.wasm.proxy ? tensor.clone() : tensor;\n  }\n  if (missingInputs.length > 0) {\n    throw new Error(`An error occurred during model execution: \"Missing the following inputs: ${missingInputs.join(', ')}.`);\n  }\n  const numInputsProvided = Object.keys(inputs).length;\n  const numInputsNeeded = session.inputNames.length;\n  if (numInputsProvided > numInputsNeeded) {\n    // No missing inputs, but too many inputs were provided.\n    // Warn the user and ignore the extra inputs.\n    let ignored = Object.keys(inputs).filter(inputName => !session.inputNames.includes(inputName));\n    console.warn(`WARNING: Too many inputs were provided (${numInputsProvided} > ${numInputsNeeded}). The following inputs will be ignored: \"${ignored.join(', ')}\".`);\n  }\n  return checkedInputs;\n}\n\n/**\n * Executes an InferenceSession using the specified inputs.\n * NOTE: `inputs` must contain at least the input names of the model.\n *  - If additional inputs are passed, they will be ignored.\n *  - If inputs are missing, an error will be thrown.\n * \n * @param {InferenceSession} session The InferenceSession object to run.\n * @param {Object} inputs An object that maps input names to input tensors.\n * @returns {Promise<Object>} A Promise that resolves to an object that maps output names to output tensors.\n * @private\n */\nasync function sessionRun(session, inputs) {\n  const checkedInputs = validateInputs(session, inputs);\n  try {\n    // @ts-ignore\n    let output = await session.run(checkedInputs);\n    output = replaceTensors(output);\n    return output;\n  } catch (e) {\n    // This usually occurs when the inputs are of the wrong type.\n    console.error(`An error occurred during model execution: \"${e}\".`);\n    console.error('Inputs given to model:', checkedInputs);\n    throw e;\n  }\n}\n\n/**\n * Replaces ONNX Tensor objects with custom Tensor objects to support additional functions.\n * @param {Object} obj The object to replace tensor objects in.\n * @returns {Object} The object with tensor objects replaced by custom Tensor objects.\n * @private\n */\nfunction replaceTensors(obj) {\n  for (let prop in obj) {\n    if (obj[prop] instanceof ONNXTensor) {\n      obj[prop] = new Tensor(obj[prop]);\n    } else if (typeof obj[prop] === 'object') {\n      replaceTensors(obj[prop]);\n    }\n  }\n  return obj;\n}\n\n/**\n * Converts an array or Tensor of integers to an int64 Tensor.\n * @param {Array|Tensor} items The input integers to be converted.\n * @returns {Tensor} The int64 Tensor with the converted values.\n * @throws {Error} If the input array is empty or the input is a batched Tensor and not all sequences have the same length.\n * @private\n */\nfunction toI64Tensor(items) {\n  if (items instanceof Tensor) {\n    return items;\n  }\n  // items is an array\n  if (items.length === 0) {\n    throw Error(\"items must be non-empty\");\n  }\n  if (Array.isArray(items[0])) {\n    // batched\n    if (items.some(x => x.length !== items[0].length)) {\n      throw Error(\"Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' and/or 'truncation=True' to have batched tensors with the same length.\");\n    }\n    return new Tensor('int64', BigInt64Array.from(items.flat().map(x => BigInt(x))), [items.length, items[0].length]);\n  } else {\n    //flat\n    return new Tensor('int64', BigInt64Array.from(items.map(x => BigInt(x))), [1, items.length]);\n  }\n}\n\n/**\n * Prepares an attention mask for a sequence of tokens based on configuration options.\n * @param {Object} self The calling object instance.\n * @param {Tensor} tokens The input tokens.\n * @returns {Tensor} The attention mask tensor.\n * @private\n */\nfunction prepareAttentionMask(self, tokens) {\n  // Prepare attention mask\n  let pad_token_id = self.config.pad_token_id ?? null;\n  let eos_token_id = self.config.eos_token_id ?? null;\n  if (isIntegralNumber(eos_token_id)) {\n    eos_token_id = [eos_token_id];\n  }\n  let is_pad_token_in_inputs = tokens.indexOf(pad_token_id) !== -1;\n  let is_pad_token_not_equal_to_eos_token_id = eos_token_id === null || !eos_token_id.includes(pad_token_id);\n  if (is_pad_token_in_inputs && is_pad_token_not_equal_to_eos_token_id) {\n    let data = BigInt64Array.from(\n    // Note: != so that int matches bigint\n    // @ts-ignore\n    tokens.data.map(x => x != pad_token_id));\n    return new Tensor('int64', data, tokens.dims);\n  } else {\n    return ones_like(tokens);\n  }\n}\n\n/**\n * Add position IDs to the feeds object.\n * @param {Object} session The inference session.\n * @param {Object} feeds The input to the model.\n * @param {boolean} use_cache_branch Whether to use the cache branch of the model.\n * @returns {void}\n * @private\n */\nfunction preparePositionIds(session, feeds, use_cache_branch) {\n  if (!session.inputNames.includes('position_ids')) return;\n  const data = new BigInt64Array(feeds.attention_mask.data.length);\n\n  // Compute cumulative sum of the attention mask along the sequence length dimension\n  for (let i = 0; i < feeds.attention_mask.dims[0]; ++i) {\n    let start = i * feeds.attention_mask.dims[1];\n    let sum = BigInt(0);\n    for (let j = 0; j < feeds.attention_mask.dims[1]; ++j) {\n      const index = start + j;\n      if (feeds.attention_mask.data[index] === 0n) {\n        data[index] = BigInt(1);\n      } else {\n        // === 1n\n        data[index] = sum;\n        sum += feeds.attention_mask.data[index];\n      }\n    }\n  }\n  feeds.position_ids = new Tensor('int64', data, feeds.attention_mask.dims);\n  if (use_cache_branch) {\n    feeds.position_ids = feeds.position_ids.slice(null, -1).unsqueeze_(-1);\n  }\n}\n\n/**\n * Creates a boolean tensor with a single value.\n * @param {boolean} value The value of the tensor.\n * @returns {Tensor} The boolean tensor.\n * @private\n */\nfunction boolTensor(value) {\n  return new Tensor('bool', [value], [1]);\n}\n\n// JS doesn't support mixins, so we define some reused functions here, and allow \"this\" to be passed in\n/**\n * Perform forward pass on the seq2seq model (both encoder and decoder).\n * @param {Object} self The seq2seq model object.\n * @param {Object} model_inputs The input object for the model containing encoder and decoder inputs.\n * @returns {Promise<Seq2SeqLMOutput>} Promise that resolves with the output of the seq2seq model.\n * @private\n */\nasync function seq2seqForward(self, model_inputs) {\n  let {\n    encoder_outputs,\n    past_key_values\n  } = model_inputs;\n  if (!encoder_outputs) {\n    // Encoder outputs are not given, so we must compute them.\n    encoder_outputs = (await encoderForward(self, model_inputs)).last_hidden_state;\n  }\n  let decoderFeeds = {\n    input_ids: model_inputs.decoder_input_ids,\n    encoder_hidden_states: encoder_outputs\n  };\n  const use_cache_branch = !!past_key_values;\n  if (self.decoder_merged_session.inputNames.includes('use_cache_branch')) {\n    decoderFeeds.use_cache_branch = boolTensor(use_cache_branch);\n  }\n  if (self.decoder_merged_session.inputNames.includes('encoder_attention_mask')) {\n    decoderFeeds.encoder_attention_mask = model_inputs.attention_mask;\n  }\n  preparePositionIds(self.decoder_merged_session, decoderFeeds, use_cache_branch);\n  self.addPastKeyValues(decoderFeeds, past_key_values);\n  const decoderResults = await sessionRun(self.decoder_merged_session, decoderFeeds);\n  let logits = decoderResults.logits;\n  past_key_values = self.getPastKeyValues(decoderResults, past_key_values);\n\n  // Get cross attention and/or decoder attentions if they are present\n  const attns = self.getAttentions(decoderResults);\n  return new Seq2SeqLMOutput({\n    logits,\n    past_key_values,\n    encoder_outputs,\n    ...attns\n  });\n}\n\n/**\n * Start the beam search process for the seq2seq model.\n * @param {PreTrainedModel} self The seq2seq model object.\n * @param {Tensor} inputTokenIds Array of input token ids for each input sequence.\n * @param {Object} generation_config The generation config.\n * @param {number} numOutputTokens The maximum number of output tokens for the model.\n * @returns {Object[]} Array of beam search objects.\n * @private\n */\nfunction seq2seqStartBeams(self, inputTokenIds, generation_config, numOutputTokens) {\n  let beams = [];\n  let beamId = 0;\n\n  // @ts-ignore\n  const requires_attention_mask = self.requires_attention_mask ?? true;\n\n  // decoder_input_ids == output_token_ids\n  let decoder_input_ids = generation_config.decoder_input_ids ?? generation_config.decoder_start_token_id ?? generation_config.bos_token_id ?? generation_config.eos_token_id;\n\n  // Support input as tensor or list\n  // TODO support batched decoder_input_ids\n  if (decoder_input_ids instanceof Tensor) {\n    decoder_input_ids = decoder_input_ids.tolist().flat();\n  } else if (!Array.isArray(decoder_input_ids)) {\n    decoder_input_ids = [decoder_input_ids];\n  }\n  for (let tokens of inputTokenIds) {\n    // TODO: Improve\n    // Currently, just add back batch dimension.\n    // In future, allow for true parallel execution\n    tokens.dims = [1, ...tokens.dims];\n\n    // Create beam\n    let start = {\n      inputs: tokens,\n      encoder_outputs: null,\n      prev_model_outputs: null,\n      output_token_ids: decoder_input_ids,\n      done: false,\n      score: 0,\n      id: beamId++ // assign unique id to beams\n    };\n    if (requires_attention_mask) {\n      start.attention_mask = prepareAttentionMask(self, tokens);\n    }\n    beams.push(start);\n  }\n  return beams;\n}\n\n/**\n * Run beam search on the seq2seq model for a single beam.\n * @param {PreTrainedModel} self The seq2seq model object.\n * @param {Object} beam The beam search object for which to run the model.\n * @param {Object} options options\n * @param {string} [options.input_name='input_ids'] The name of the input tensor for the encoder.\n * @returns {Promise<Object>} Promise that resolves with the output of the seq2seq model for the given beam.\n * @private\n */\nasync function seq2seqRunBeam(self, beam) {\n  const input_name = self.main_input_name;\n  let decoder_input_ids = beam.output_token_ids;\n  if (beam.prev_model_outputs) {\n    // After the first step, `prev_model_outputs` won't be null.\n    // So, we cut decoder_input_ids if past is used\n    decoder_input_ids = decoder_input_ids.slice(-1);\n  }\n\n  // 1. Prepare\n  let model_inputs = {\n    [input_name]: beam.inputs,\n    decoder_input_ids: toI64Tensor(decoder_input_ids),\n    encoder_outputs: beam.encoder_outputs,\n    past_key_values: beam.prev_model_outputs?.past_key_values\n  };\n  if (beam.attention_mask) {\n    model_inputs.attention_mask = beam.attention_mask;\n  }\n\n  // 2. Run\n  let output = await self.forward(model_inputs);\n\n  // 3. Update\n  beam.prev_model_outputs = output;\n  beam.encoder_outputs = output.encoder_outputs;\n  return output;\n}\n\n/**\n * Update a beam with a new token ID.\n * @param {Object} beam The beam to update.\n * @param {number} newTokenId The new token ID to add to the beam's output.\n * @private\n */\nfunction seq2seqUpdatebeam(beam, newTokenId) {\n  beam.output_token_ids = [...beam.output_token_ids, newTokenId];\n}\n\n/**\n * Forward pass of an encoder model.\n * @param {Object} self The encoder model.\n * @param {Object} model_inputs The input data to be used for the forward pass.\n * @returns {Promise<Object>} Promise that resolves with an object containing the model's outputs.\n * @private\n */\nasync function encoderForward(self, model_inputs) {\n  const encoderFeeds = Object.create(null);\n  for (const key of self.session.inputNames) {\n    encoderFeeds[key] = model_inputs[key];\n  }\n  if (self.session.inputNames.includes('token_type_ids') && !encoderFeeds.token_type_ids) {\n    // Assign default `token_type_ids` (all zeroes) to the `encoderFeeds` if the model expects it,\n    // but they weren't created by the tokenizer.\n    encoderFeeds.token_type_ids = new Tensor('int64', new BigInt64Array(encoderFeeds.input_ids.data.length), encoderFeeds.input_ids.dims);\n  }\n  return await sessionRun(self.session, encoderFeeds);\n}\n\n/**\n * Forward pass of a decoder model.\n * @param {Object} self The decoder model.\n * @param {Object} model_inputs The input data to be used for the forward pass.\n * @returns {Promise<Object>} Promise that resolves with an object containing the logits and past key values.\n * @private\n */\nasync function decoderForward(self, model_inputs) {\n  let {\n    input_ids,\n    past_key_values,\n    attention_mask\n  } = model_inputs;\n  let decoderFeeds = {\n    input_ids: input_ids,\n    attention_mask: attention_mask ?? prepareAttentionMask(self, input_ids)\n  };\n  const use_cache_branch = !!past_key_values;\n  if (self.session.inputNames.includes('use_cache_branch')) {\n    decoderFeeds.use_cache_branch = boolTensor(use_cache_branch);\n  }\n  preparePositionIds(self.session, decoderFeeds, use_cache_branch);\n  self.addPastKeyValues(decoderFeeds, past_key_values);\n  let decoderResults = await sessionRun(self.session, decoderFeeds);\n  let logits = decoderResults.logits;\n  past_key_values = self.getPastKeyValues(decoderResults, past_key_values);\n  return {\n    logits,\n    past_key_values\n  };\n}\n\n/**\n * Starts the generation of text by initializing the beams for the given input token IDs.\n * @param {Object} self The text generation model object.\n * @param {Tensor} inputTokenIds An tensor of input token IDs to generate text from.\n * @param {Object} generation_config The generation config.\n * @param {number} numOutputTokens The maximum number of tokens to generate for each beam.\n * @param {Tensor} [inputs_attention_mask] The attention mask tensor for the input token IDs.\n * @returns {Object[]} An array of beams initialized with the given inputs and parameters.\n * @private\n */\nfunction decoderStartBeams(self, inputTokenIds, generation_config, numOutputTokens, inputs_attention_mask) {\n  let beams = [];\n  let beamId = 0;\n  for (let tokens of inputTokenIds) {\n    let output_token_ids = tokens.tolist().map(Number);\n\n    // TODO: Improve\n    // Currently, just add back batch dimension.\n    // In future, allow for true parallel execution\n    tokens.dims = [1, ...tokens.dims];\n    let attn_mask;\n    if (inputs_attention_mask) {\n      attn_mask = inputs_attention_mask[beamId];\n      attn_mask.dims = [1, ...attn_mask.dims];\n    } else {\n      attn_mask = prepareAttentionMask(self, tokens);\n    }\n    let start = {\n      input: tokens,\n      model_input_ids: tokens,\n      attention_mask: attn_mask,\n      prev_model_outputs: null,\n      output_token_ids: output_token_ids,\n      num_output_tokens: numOutputTokens,\n      done: false,\n      score: 0,\n      id: beamId++ // assign unique id to beams\n    };\n    beams.push(start);\n  }\n  return beams;\n}\n\n/**\n * Runs a single step of the text generation process for a given beam.\n *\n * @param {Object} self The decoder object.\n * @param {Object} beam The beam to run.\n * @param {Tensor} beam.input The input tensor.\n * @param {Tensor} beam.model_input_ids The input ids to the model.\n * @param {Tensor} beam.attention_mask The attention mask.\n * @param {Object} beam.prev_model_outputs The past key values.\n * @param {number[]} beam.output_token_ids The output token ids.\n * @returns {Promise<Object>} The output of the generation step.\n * @private\n */\nasync function decoderRunBeam(self, beam) {\n  let attnMaskData = new BigInt64Array(beam.output_token_ids.length).fill(1n);\n\n  // 1. Prepare\n  let model_inputs = {\n    input_ids: beam.model_input_ids,\n    attention_mask: new Tensor('int64', attnMaskData, [1, attnMaskData.length]),\n    past_key_values: beam.prev_model_outputs?.past_key_values\n  };\n\n  // 2. Run\n  let output = await self.forward(model_inputs);\n\n  // 3. Update\n  beam.prev_model_outputs = output;\n  return output;\n}\n\n/**\n * Update a beam with a new token ID.\n * @param {Object} beam The beam to update.\n * @param {number} newTokenId The new token ID to add to the beam's output.\n * @private\n */\nfunction decoderUpdatebeam(beam, newTokenId) {\n  beam.output_token_ids = [...beam.output_token_ids, newTokenId];\n  beam.model_input_ids = new Tensor('int64', [BigInt(newTokenId)], [1, 1]);\n}\n\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n/**\n * A base class for pre-trained models that provides the model configuration and an ONNX session.\n */\nexport class PreTrainedModel extends Callable {\n  main_input_name = 'input_ids';\n\n  /**\n   * Creates a new instance of the `PreTrainedModel` class.\n   * @param {Object} config The model configuration.\n   * @param {any} session session for the model.\n   */\n  constructor(config, session) {\n    super();\n    this.config = config;\n    this.session = session;\n    const modelName = MODEL_CLASS_TO_NAME_MAPPING.get(this.constructor);\n    const modelType = MODEL_TYPE_MAPPING.get(modelName);\n    this.can_generate = false;\n    this._runBeam = null;\n    this._getStartBeams = null;\n    this._updateBeam = null;\n    this._forward = null;\n    if (modelType === MODEL_TYPES.DecoderOnly) {\n      this.can_generate = true;\n      this._runBeam = decoderRunBeam;\n      this._getStartBeams = decoderStartBeams;\n      this._updateBeam = decoderUpdatebeam;\n      this._forward = decoderForward;\n    } else if (modelType === MODEL_TYPES.Seq2Seq || modelType === MODEL_TYPES.Vision2Seq) {\n      this.can_generate = true;\n      this._runBeam = seq2seqRunBeam;\n      this._getStartBeams = seq2seqStartBeams;\n      this._updateBeam = seq2seqUpdatebeam;\n      this._forward = seq2seqForward;\n    } else if (modelType === MODEL_TYPES.EncoderDecoder) {\n      this._forward = encoderForward;\n    } else {\n      // should be MODEL_TYPES.EncoderOnly\n      this._forward = encoderForward;\n    }\n  }\n\n  /**\n  * Disposes of all the ONNX sessions that were created during inference.\n  * @returns {Promise<unknown[]>} An array of promises, one for each ONNX session that is being disposed.\n  * @todo Use https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/FinalizationRegistry\n  */\n  async dispose() {\n    const promises = [];\n    for (let key of Object.keys(this)) {\n      const item = this[key];\n      // @ts-ignore\n      if (item instanceof InferenceSession) {\n        promises.push(item.handler.dispose());\n      }\n    }\n    return await Promise.all(promises);\n  }\n\n  /**\n   * Instantiate one of the model classes of the library from a pretrained model.\n   * \n   * The model class to instantiate is selected based on the `model_type` property of the config object\n   * (either passed as an argument or loaded from `pretrained_model_name_or_path` if possible)\n   * \n   * @param {string} pretrained_model_name_or_path The name or path of the pretrained model. Can be either:\n   * - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.\n   *   Valid model ids can be located at the root-level, like `bert-base-uncased`, or namespaced under a\n   *   user or organization name, like `dbmdz/bert-base-german-cased`.\n   * - A path to a *directory* containing model weights, e.g., `./my_model_directory/`.\n   * @param {import('./utils/hub.js').PretrainedOptions} options Additional options for loading the model.\n   * \n   * @returns {Promise<PreTrainedModel>} A new instance of the `PreTrainedModel` class.\n   */\n  static async from_pretrained(pretrained_model_name_or_path, {\n    quantized = true,\n    progress_callback = null,\n    config = null,\n    cache_dir = null,\n    local_files_only = false,\n    revision = 'main',\n    model_file_name = null\n  } = {}) {\n    let options = {\n      quantized,\n      progress_callback,\n      config,\n      cache_dir,\n      local_files_only,\n      revision,\n      model_file_name\n    };\n    const modelName = MODEL_CLASS_TO_NAME_MAPPING.get(this);\n    const modelType = MODEL_TYPE_MAPPING.get(modelName);\n    let info;\n    if (modelType === MODEL_TYPES.DecoderOnly) {\n      info = await Promise.all([AutoConfig.from_pretrained(pretrained_model_name_or_path, options), constructSession(pretrained_model_name_or_path, options.model_file_name ?? 'decoder_model_merged', options), getModelJSON(pretrained_model_name_or_path, 'generation_config.json', false, options)]);\n    } else if (modelType === MODEL_TYPES.Seq2Seq || modelType === MODEL_TYPES.Vision2Seq) {\n      info = await Promise.all([AutoConfig.from_pretrained(pretrained_model_name_or_path, options), constructSession(pretrained_model_name_or_path, 'encoder_model', options), constructSession(pretrained_model_name_or_path, 'decoder_model_merged', options), getModelJSON(pretrained_model_name_or_path, 'generation_config.json', false, options)]);\n    } else if (modelType === MODEL_TYPES.MaskGeneration) {\n      info = await Promise.all([AutoConfig.from_pretrained(pretrained_model_name_or_path, options), constructSession(pretrained_model_name_or_path, 'vision_encoder', options), constructSession(pretrained_model_name_or_path, 'prompt_encoder_mask_decoder', options)]);\n    } else if (modelType === MODEL_TYPES.EncoderDecoder) {\n      info = await Promise.all([AutoConfig.from_pretrained(pretrained_model_name_or_path, options), constructSession(pretrained_model_name_or_path, 'encoder_model', options), constructSession(pretrained_model_name_or_path, 'decoder_model_merged', options)]);\n    } else {\n      // should be MODEL_TYPES.EncoderOnly\n      if (modelType !== MODEL_TYPES.EncoderOnly) {\n        console.warn(`Model type for '${modelName ?? config?.model_type}' not found, assuming encoder-only architecture. Please report this at https://github.com/xenova/transformers.js/issues/new/choose.`);\n      }\n      info = await Promise.all([AutoConfig.from_pretrained(pretrained_model_name_or_path, options), constructSession(pretrained_model_name_or_path, options.model_file_name ?? 'model', options)]);\n    }\n\n    // @ts-ignore\n    return new this(...info);\n  }\n\n  /**\n   * Runs the model with the provided inputs\n   * @param {Object} model_inputs Object containing input tensors\n   * @returns {Promise<Object>} Object containing output tensors\n   */\n  async _call(model_inputs) {\n    return await this.forward(model_inputs);\n  }\n\n  /**\n   * Forward method for a pretrained model. If not overridden by a subclass, the correct forward method\n   * will be chosen based on the model type.\n   * @param {Object} model_inputs The input data to the model in the format specified in the ONNX model.\n   * @returns {Promise<Object>} The output data from the model in the format specified in the ONNX model.\n   * @throws {Error} This method must be implemented in subclasses.\n   */\n  async forward(model_inputs) {\n    return await this._forward(this, model_inputs);\n  }\n\n  /**\n   * @param {import('./utils/generation.js').GenerationConfigType} generation_config \n   * @param {number} input_ids_seq_length The starting sequence length for the input ids.\n   * @returns {LogitsProcessorList}\n   * @private\n   */\n  _get_logits_processor(generation_config, input_ids_seq_length,\n  // encoder_input_ids, TODO\n  // prefix_allowed_tokens_fn, TODO\n  logits_processor = null) {\n    const processors = new LogitsProcessorList();\n\n    // if (generation_config.diversity_penalty !== null && generation_config.diversity_penalty > 0.0) {\n    //     processors.push(new HammingDiversityLogitsProcessor(\n    //         generation_config.diversity_penalty,\n    //         generation_config.num_beams,\n    //         generation_config.num_beam_groups\n    //     ));\n    // }\n\n    // if (generation_config.encoder_repetition_penalty !== null && generation_config.encoder_repetition_penalty !== 1.0) {\n    //     processors.push(new EncoderRepetitionPenaltyLogitsProcessor(\n    //         generation_config.encoder_repetition_penalty,\n    //         encoder_input_ids\n    //     ));\n    // }\n\n    if (generation_config.repetition_penalty !== null && generation_config.repetition_penalty !== 1.0) {\n      processors.push(new RepetitionPenaltyLogitsProcessor(generation_config.repetition_penalty));\n    }\n    if (generation_config.no_repeat_ngram_size !== null && generation_config.no_repeat_ngram_size > 0) {\n      processors.push(new NoRepeatNGramLogitsProcessor(generation_config.no_repeat_ngram_size));\n    }\n\n    // if (generation_config.encoder_no_repeat_ngram_size !== null && generation_config.encoder_no_repeat_ngram_size > 0) {\n    //     if (this.config.is_encoder_decoder) {\n    //         processors.push(new EncoderNoRepeatNGramLogitsProcessor(\n    //             generation_config.encoder_no_repeat_ngram_size,\n    //             encoder_input_ids\n    //         ));\n    //     } else {\n    //         throw new Error(\"It's impossible to use `encoder_no_repeat_ngram_size` with decoder-only architecture\");\n    //     }\n    // }\n\n    if (generation_config.bad_words_ids !== null) {\n      processors.push(new NoBadWordsLogitsProcessor(generation_config.bad_words_ids, generation_config.eos_token_id));\n    }\n    if (generation_config.min_length !== null && generation_config.eos_token_id !== null && generation_config.min_length > 0) {\n      processors.push(new MinLengthLogitsProcessor(generation_config.min_length, generation_config.eos_token_id));\n    }\n    if (generation_config.min_new_tokens !== null && generation_config.eos_token_id !== null && generation_config.min_new_tokens > 0) {\n      processors.push(new MinNewTokensLengthLogitsProcessor(input_ids_seq_length, generation_config.min_new_tokens, generation_config.eos_token_id));\n    }\n\n    // if (prefix_allowed_tokens_fn !== null) {\n    //     processors.push(new PrefixConstrainedLogitsProcessor(\n    //         prefix_allowed_tokens_fn,\n    //         generation_config.num_beams / generation_config.num_beam_groups\n    //     ));\n    // }\n\n    if (generation_config.forced_bos_token_id !== null) {\n      processors.push(new ForcedBOSTokenLogitsProcessor(generation_config.forced_bos_token_id));\n    }\n    if (generation_config.forced_eos_token_id !== null) {\n      processors.push(new ForcedEOSTokenLogitsProcessor(generation_config.max_length, generation_config.forced_eos_token_id));\n    }\n\n    // if (generation_config.remove_invalid_values === true) {\n    //     processors.push(new InfNanRemoveLogitsProcessor());\n    // }\n\n    // if (generation_config.exponential_decay_length_penalty !== null) {\n    //     processors.push(new ExponentialDecayLengthPenalty(\n    //         generation_config.exponential_decay_length_penalty,\n    //         generation_config.eos_token_id,\n    //         input_ids_seq_length\n    //     ));\n    // }\n\n    // if (generation_config.suppress_tokens !== null) {\n    //     processors.push(new SuppressTokensLogitsProcessor(generation_config.suppress_tokens));\n    // }\n\n    if (generation_config.begin_suppress_tokens !== null) {\n      let begin_index = input_ids_seq_length > 1 || generation_config.forced_bos_token_id === null ? input_ids_seq_length : input_ids_seq_length + 1;\n      if (generation_config.forced_decoder_ids !== null) {\n        // generation starts after the last token that is forced\n        begin_index += generation_config.forced_decoder_ids[generation_config.forced_decoder_ids.length - 1][0];\n      }\n      processors.push(new SuppressTokensAtBeginLogitsProcessor(generation_config.begin_suppress_tokens, begin_index));\n    }\n    if (generation_config.forced_decoder_ids !== null) {\n      processors.push(new ForceTokensLogitsProcessor(generation_config.forced_decoder_ids));\n    }\n    if (logits_processor !== null) {\n      processors.extend(logits_processor);\n    }\n\n    // `LogitNormalization` should always be the last logit processor, when present\n    // if (generation_config.renormalize_logits === true) {\n    //     processors.push(new LogitNormalization());\n    // }\n\n    return processors;\n  }\n\n  /**\n   * This function merges multiple generation configs together to form a final generation config to be used by the model for text generation.\n   * It first creates an empty `GenerationConfig` object, then it applies the model's own `generation_config` property to it. Finally, if a `generation_config` object was passed in the arguments, it overwrites the corresponding properties in the final config with those of the passed config object.\n   * @param {import('./utils/generation.js').GenerationConfigType} generation_config A `GenerationConfig` object containing generation parameters.\n   * @returns {import('./utils/generation.js').GenerationConfigType} The final generation config object to be used by the model for text generation.\n   */\n  _get_generation_config(generation_config) {\n    // Create empty generation config (contains defaults)\n    // We pass `this.config` so that if `eos_token_id` or `bos_token_id` exist in the model's config, we will use them\n    let gen_config = new GenerationConfig(this.config);\n\n    // Apply model's generation config, if it exists\n    if ('generation_config' in this) {\n      Object.assign(gen_config, this.generation_config);\n    }\n\n    // Finally, use any generation config specified by the user\n    // when calling `generate`\n    if (generation_config !== null) {\n      Object.assign(gen_config, generation_config);\n    }\n    return gen_config;\n  }\n\n  /**\n   * @typedef {import('./utils/maths.js').TypedArray} TypedArray\n   */\n\n  /**\n   * @typedef {{ sequences: Tensor, decoder_attentions: Tensor, cross_attentions: Tensor }} EncoderDecoderOutput\n   * @typedef {Object} DecoderOutput\n   * \n   * Generates text based on the given inputs and generation configuration using the model.\n   * @param {Tensor|Array|TypedArray} inputs An array of input token IDs.\n   * @param {Object|GenerationConfig|null} generation_config The generation configuration to use. If null, default configuration will be used.\n   * @param {Object|null} logits_processor An optional logits processor to use. If null, a new LogitsProcessorList instance will be created.\n   * @param {Object} options options\n   * @param {Object} [options.inputs_attention_mask=null] An optional attention mask for the inputs.\n   * @returns {Promise<number[][]|EncoderDecoderOutput|DecoderOutput>} An array of generated output sequences, where each sequence is an array of token IDs.\n   * @throws {Error} Throws an error if the inputs array is empty.\n   */\n  async generate(inputs, generation_config = null, logits_processor = null, {\n    inputs_attention_mask = null\n  } = {}) {\n    if (!this.can_generate) {\n      const modelName = MODEL_CLASS_TO_NAME_MAPPING.get(this.constructor);\n      let errorMessage = `The current model class (${modelName}) is not compatible with \\`.generate()\\`, as it doesn't have a language model head.`;\n      const modelType = this.config.model_type;\n      const possibleInfo = MODEL_WITH_LM_HEAD_MAPPING_NAMES.get(modelType) ?? MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES.get(modelType) ?? MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING_NAMES.get(modelType)\n      // ?? MODEL_FOR_TEXT_TO_SPECTROGRAM_MAPPING_NAMES.get(modelType) // TODO\n      ?? MODEL_FOR_VISION_2_SEQ_MAPPING_NAMES.get(modelType);\n      if (possibleInfo) {\n        // TODO: support multiple possible classes\n        errorMessage += ` Please use the following class instead: '${possibleInfo[0]}'`;\n      }\n      throw Error(errorMessage);\n    }\n    if (!(inputs instanceof Tensor) && !isTypedArray(inputs) && !Array.isArray(inputs)) {\n      throw Error(`\\`inputs\\` must be a Tensor, TypedArray, or Array, but is \"${inputs.constructor.name}\".`);\n    }\n    let input_ids_seq_length;\n\n    // Prepare `input_ids` which will be used for auto-regressive generation\n    // TODO: Update to align with HF transformers' implementation\n    if (this.config.is_encoder_decoder) {\n      // Generating from the encoder outputs\n      input_ids_seq_length = 0;\n    } else {\n      input_ids_seq_length = inputs instanceof Tensor ? inputs.dims.at(-1) : inputs.length;\n\n      // decoder-only\n      if (input_ids_seq_length === 0) {\n        throw Error(\"Must supply a non-empty array of input token ids.\");\n      }\n    }\n\n    // Update generation config with defaults\n    generation_config = this._get_generation_config(generation_config);\n    logits_processor = logits_processor ?? new LogitsProcessorList();\n\n    // Update logits processor\n    logits_processor = this._get_logits_processor(generation_config, input_ids_seq_length, logits_processor);\n\n    /** @type {number[]} */\n    let eos_token_ids = generation_config.eos_token_id;\n    if (eos_token_ids !== null && !Array.isArray(eos_token_ids)) {\n      eos_token_ids = [eos_token_ids];\n    }\n\n    // TODO implement early_stopping\n    // https://huggingface.co/blog/how-to-generate\n\n    let numOutputTokens = 1;\n    const maxOutputTokens = numOutputTokens + (generation_config.max_new_tokens ?? Infinity);\n\n    // Only use max length if max_new_tokens is not provided\n    const useMaxLength = Number.isInteger(generation_config.max_length) && (generation_config.max_new_tokens ?? null) === null;\n    let sampler = Sampler.getSampler(generation_config);\n\n    // @ts-ignore\n    let beams = this.getStartBeams(inputs, generation_config, numOutputTokens, inputs_attention_mask);\n    while (beams.some(x => !x.done) && numOutputTokens < maxOutputTokens) {\n      let newest_beams = [];\n      for (let beam of beams) {\n        if (beam.done) {\n          // Add this beam back into the pool\n          newest_beams.push(beam);\n          continue;\n        }\n        if (useMaxLength && beam.output_token_ids.length >= generation_config.max_length) {\n          // Set this beam to done and add it back into the pool\n          beam.done = true;\n          newest_beams.push(beam);\n          continue;\n        }\n\n        // @ts-ignore\n        let output = await this.runBeam(beam);\n\n        // add attentions/scores to beam only if user requested\n        if (generation_config.output_attentions) {\n          this.addAttentionsToBeam(beam, output);\n        }\n        if (generation_config.output_scores) {\n          // TODO add\n        }\n\n        // Logits are of the form [batch_size, out_seq_length, vocab_size]\n        // In most cases, this will be [batch_size, 1, vocab_size]\n        // So, we select the last token's logits:\n        // (equivalent to `logits = outputs.logits[:, -1, :]`)\n        let logits = output.logits.slice(null, -1, null);\n\n        // Apply logits processor\n        logits_processor(beam.output_token_ids, logits);\n        let sampledTokens = sampler(logits);\n        for (let [newTokenId, logProb] of sampledTokens) {\n          // use previous beam as a starting point\n          let newBeam = {\n            ...beam\n          };\n\n          // update new beam\n          // @ts-ignore\n          this.updateBeam(newBeam, newTokenId);\n          newBeam.score += logProb;\n          if (eos_token_ids && eos_token_ids.includes(newTokenId)) {\n            newBeam.done = true;\n          }\n          newest_beams.push(newBeam);\n        }\n      }\n      ++numOutputTokens;\n\n      // Next, we get the best beams, per ID\n      newest_beams = this.groupBeams(newest_beams).map(group => group.sort((a, b) => b.score - a.score) // sort by score\n      .slice(0, generation_config.num_beams) // remove outside beam width\n      );\n\n      // Flatten beams\n      beams = newest_beams.flat();\n\n      // Run callback\n      if (generation_config.callback_function) {\n        generation_config.callback_function(beams);\n      }\n    }\n\n    // TODO: Ensure that we can return non-batched outputs\n\n    const groupedBeams = this.groupBeams(beams);\n    const getFlattened = key => groupedBeams.map(batch => {\n      if (generation_config.num_return_sequences > 1) {\n        return batch.slice(0, generation_config.num_return_sequences).map(x => x[key]);\n      } else {\n        return [batch[0][key]];\n      }\n    }).flat(); // Flatten across batches (depth=1)\n\n    const sequences = getFlattened('output_token_ids'); // [1, seqLength]\n\n    if (generation_config.return_dict_in_generate) {\n      // NOTE: `decoder_attentions` and `cross_attentions` should be:\n      //    list (one element for each generated token)\n      //    of list (one element for each layer of the decoder)\n      //    of torch.FloatTensor of shape (batch_size, num_heads, generated_length, sequence_length)\n      // However, since we are only generating one batch at a time, they are of the form:\n      //   list (batches)\n      //   of list (one element for each generated token)\n      //   of list (one element for each layer of the decoder)\n      //   of torch.FloatTensor of shape (1, num_heads, generated_length, sequence_length)\n      // \n      // TODO: In future (when true parallelism, we should be able to return the correct shape)\n\n      const decoder_attentions = getFlattened('decoder_attentions');\n      const cross_attentions = getFlattened('cross_attentions');\n      return {\n        sequences,\n        decoder_attentions,\n        cross_attentions\n      };\n    } else {\n      return sequences;\n    }\n  }\n\n  /**\n   * Helper function to add attentions to beam\n   * @param {Object} beam \n   * @param {Object} output\n   * @private \n   */\n  addAttentionsToBeam(beam, output) {\n    if (this.config.is_encoder_decoder) {\n      if (!output.cross_attentions || output.cross_attentions.length === 0) {\n        throw Error(\"`output_attentions` is true, but the model did not produce cross-attentions. \" + \"This is most likely because the model was not exported with `output_attentions=True`.\");\n      }\n      if (!beam.cross_attentions) {\n        beam.cross_attentions = [];\n      }\n      beam.cross_attentions.push(output.cross_attentions);\n    }\n    if (!output.decoder_attentions || output.decoder_attentions.length === 0) {\n      throw Error(\"`output_attentions` is true, but the model did not produce decoder-attentions. \" + \"This is most likely because the model was not exported with `output_attentions=True`.\");\n    }\n    if (!beam.decoder_attentions) {\n      beam.decoder_attentions = [];\n    }\n    beam.decoder_attentions.push(output.decoder_attentions);\n  }\n\n  /**\n   * Groups an array of beam objects by their ids.\n   *\n   * @param {Array} beams The array of beam objects to group.\n   * @returns {Array} An array of arrays, where each inner array contains beam objects with the same id.\n   */\n  groupBeams(beams) {\n    // Group beams by their ids\n    const groups = Object.create(null);\n    for (const obj of beams) {\n      if (groups[obj.id] === undefined) {\n        groups[obj.id] = [obj];\n      } else {\n        groups[obj.id].push(obj);\n      }\n    }\n    return Object.values(groups);\n  }\n\n  /**\n   * Returns an object containing past key values from the given decoder results object.\n   *\n   * @param {Object} decoderResults The decoder results object.\n   * @param {Object} pastKeyValues The previous past key values.\n   * @returns {Object} An object containing past key values.\n   */\n  getPastKeyValues(decoderResults, pastKeyValues) {\n    const pkvs = Object.create(null);\n    for (const name in decoderResults) {\n      if (name.startsWith('present')) {\n        let newName = name.replace('present', 'past_key_values');\n        if (pastKeyValues && name.includes('encoder')) {\n          // Optimization introduced by optimum to reuse past key values. So, we just replace the constant\n          // outputs with the previous past key values.\n          // https://github.com/huggingface/optimum/blob/0bf2c05fb7e1182b52d21b703cfc95fd9e4ea3dc/optimum/onnxruntime/base.py#L677-L704\n          pkvs[newName] = pastKeyValues[newName];\n        } else {\n          pkvs[newName] = decoderResults[name];\n        }\n      }\n    }\n    return pkvs;\n  }\n\n  /**\n   * Returns an object containing attentions from the given decoder results object.\n   *\n   * @param {Object} decoderResults The decoder results object.\n   * @returns {Object} An object containing attentions.\n   */\n  getAttentions(decoderResults) {\n    const attns = Object.create(null);\n    for (const attnName of ['cross_attentions', 'decoder_attentions']) {\n      const result = [];\n      for (const name in decoderResults) {\n        if (name.startsWith(attnName)) {\n          const index = name.split('.').pop();\n          result[index] = decoderResults[name];\n        }\n      }\n      attns[attnName] = result;\n    }\n    return attns;\n  }\n\n  /**\n   * Adds past key values to the decoder feeds object. If pastKeyValues is null, creates new tensors for past key values.\n   *\n   * @param {Object} decoderFeeds The decoder feeds object to add past key values to.\n   * @param {Object} pastKeyValues An object containing past key values.\n   */\n  addPastKeyValues(decoderFeeds, pastKeyValues) {\n    if (pastKeyValues) {\n      Object.assign(decoderFeeds, pastKeyValues);\n    } else {\n      // TODO support batches (i.e., batch_size > 1)\n      const batch_size = 1;\n\n      // @ts-ignore\n      if (this.config.is_encoder_decoder && (this.add_encoder_pkv ?? true)) {\n        // @ts-ignore\n        let encoder_dims = [batch_size, this.num_encoder_heads, 0, this.encoder_dim_kv];\n        // @ts-ignore\n        let decoder_dims = [batch_size, this.num_decoder_heads, 0, this.decoder_dim_kv];\n        // @ts-ignore\n        for (let i = 0; i < this.num_decoder_layers; ++i) {\n          decoderFeeds[`past_key_values.${i}.encoder.key`] = new Tensor('float32', [], encoder_dims);\n          decoderFeeds[`past_key_values.${i}.encoder.value`] = new Tensor('float32', [], encoder_dims);\n          decoderFeeds[`past_key_values.${i}.decoder.key`] = new Tensor('float32', [], decoder_dims);\n          decoderFeeds[`past_key_values.${i}.decoder.value`] = new Tensor('float32', [], decoder_dims);\n        }\n      } else if (this.config.model_type === 'falcon') {\n        // NOTE: Custom implementation for Falcon\n        // @ts-ignore\n        let dims = [batch_size * this.num_heads, 0, this.dim_kv];\n        // @ts-ignore\n        for (let i = 0; i < this.num_layers; ++i) {\n          decoderFeeds[`past_key_values.${i}.key`] = new Tensor('float32', [], dims);\n          decoderFeeds[`past_key_values.${i}.value`] = new Tensor('float32', [], dims);\n        }\n      } else if (this.config.multi_query) {\n        // e.g., for `gpt_bigcode`\n        // @ts-ignore\n        let dims = [batch_size * this.num_heads, 0, 2 * this.dim_kv];\n        // @ts-ignore\n        for (let i = 0; i < this.num_layers; ++i) {\n          decoderFeeds[`past_key_values.${i}.key_value`] = new Tensor('float32', [], dims);\n        }\n      } else if (this.config.model_type === 'bloom') {\n        // NOTE: Custom implementation for Bloom\n\n        // @ts-ignore\n        let keyDims = [batch_size * this.num_heads, this.dim_kv, 0]; // [batch_size x num_heads,64,past_sequence_length]\n        // @ts-ignore\n        let valueDims = [batch_size * this.num_heads, 0, this.dim_kv]; // [batch_size x num_heads,past_sequence_length,64]\n        // @ts-ignore\n        for (let i = 0; i < this.num_layers; ++i) {\n          decoderFeeds[`past_key_values.${i}.key`] = new Tensor('float32', [], keyDims);\n          decoderFeeds[`past_key_values.${i}.value`] = new Tensor('float32', [], valueDims);\n        }\n      } else {\n        // Decoder-only\n        // @ts-ignore\n        let dims = [batch_size, this.num_heads, 0, this.dim_kv];\n        // @ts-ignore\n        for (let i = 0; i < this.num_layers; ++i) {\n          decoderFeeds[`past_key_values.${i}.key`] = new Tensor('float32', [], dims);\n          decoderFeeds[`past_key_values.${i}.value`] = new Tensor('float32', [], dims);\n        }\n      }\n    }\n  }\n\n  /**\n   * Initializes and returns the beam for text generation task\n   * @param {Tensor} inputTokenIds The input token ids.\n   * @param {Object} generation_config The generation config.\n   * @param {number} numOutputTokens The number of tokens to be generated.\n   * @param {Tensor} inputs_attention_mask Optional input attention mask.\n   * @returns {any} A Beam object representing the initialized beam.\n   * @private\n   */\n  getStartBeams(inputTokenIds, generation_config, numOutputTokens, inputs_attention_mask) {\n    return this._getStartBeams(this, inputTokenIds, generation_config, numOutputTokens, inputs_attention_mask);\n  }\n\n  /**\n   * Runs a single step of the beam search generation algorithm.\n   * @param {any} beam The current beam being generated.\n   * @returns {Promise<any>} The updated beam after a single generation step.\n   * @private\n   */\n  async runBeam(beam) {\n    return await this._runBeam(this, beam);\n  }\n\n  /**\n   * Update a beam with a new token ID.\n   * @param {Object} beam The beam to update.\n   * @param {number} newTokenId The new token ID to add to the beam's output.\n   * @private\n   */\n  updateBeam(beam, newTokenId) {\n    return this._updateBeam(beam, newTokenId);\n  }\n}\n\n//////////////////////////////////////////////////\n// Base model output class\nexport class ModelOutput {}\n\n/**\n * Base class for model's outputs, with potential hidden states and attentions.\n */\nexport class BaseModelOutput extends ModelOutput {\n  /**\n   * @param {Object} output The output of the model.\n   * @param {Tensor} output.last_hidden_state Sequence of hidden-states at the output of the last layer of the model.\n   * @param {Tensor} [output.hidden_states] Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n   * @param {Tensor} [output.attentions] Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n   */\n  constructor({\n    last_hidden_state,\n    hidden_states = null,\n    attentions = null\n  }) {\n    super();\n    this.last_hidden_state = last_hidden_state;\n    this.hidden_states = hidden_states;\n    this.attentions = attentions;\n  }\n}\n//////////////////////////////////////////////////\n// Bert models\nexport class BertPreTrainedModel extends PreTrainedModel {}\nexport class BertModel extends BertPreTrainedModel {}\n\n/**\n * BertForMaskedLM is a class representing a BERT model for masked language modeling.\n */\nexport class BertForMaskedLM extends BertPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<MaskedLMOutput>} An object containing the model's output logits for masked language modeling.\n   */\n  async _call(model_inputs) {\n    return new MaskedLMOutput(await super._call(model_inputs));\n  }\n}\n\n/**\n * BertForSequenceClassification is a class representing a BERT model for sequence classification.\n */\nexport class BertForSequenceClassification extends BertPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n   */\n  async _call(model_inputs) {\n    return new SequenceClassifierOutput(await super._call(model_inputs));\n  }\n}\n\n/**\n * BertForTokenClassification is a class representing a BERT model for token classification.\n */\nexport class BertForTokenClassification extends BertPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.\n   */\n  async _call(model_inputs) {\n    return new TokenClassifierOutput(await super._call(model_inputs));\n  }\n}\n\n/**\n * BertForQuestionAnswering is a class representing a BERT model for question answering.\n */\nexport class BertForQuestionAnswering extends BertPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<QuestionAnsweringModelOutput>} An object containing the model's output logits for question answering.\n   */\n  async _call(model_inputs) {\n    return new QuestionAnsweringModelOutput(await super._call(model_inputs));\n  }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// NomicBert models\nexport class NomicBertPreTrainedModel extends PreTrainedModel {}\nexport class NomicBertModel extends NomicBertPreTrainedModel {}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// RoFormer models\nexport class RoFormerPreTrainedModel extends PreTrainedModel {}\n\n/**\n * The bare RoFormer Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class RoFormerModel extends RoFormerPreTrainedModel {}\n\n/**\n * RoFormer Model with a `language modeling` head on top.\n */\nexport class RoFormerForMaskedLM extends RoFormerPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<MaskedLMOutput>} An object containing the model's output logits for masked language modeling.\n   */\n  async _call(model_inputs) {\n    return new MaskedLMOutput(await super._call(model_inputs));\n  }\n}\n\n/**\n * RoFormer Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output)\n */\nexport class RoFormerForSequenceClassification extends RoFormerPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n   */\n  async _call(model_inputs) {\n    return new SequenceClassifierOutput(await super._call(model_inputs));\n  }\n}\n\n/**\n * RoFormer Model with a token classification head on top (a linear layer on top of the hidden-states output)\n * e.g. for Named-Entity-Recognition (NER) tasks.\n */\nexport class RoFormerForTokenClassification extends RoFormerPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.\n   */\n  async _call(model_inputs) {\n    return new TokenClassifierOutput(await super._call(model_inputs));\n  }\n}\n\n/**\n * RoFormer Model with a span classification head on top for extractive question-answering tasks like SQuAD\n * (a linear layers on top of the hidden-states output to compute `span start logits` and `span end logits`).\n */\nexport class RoFormerForQuestionAnswering extends RoFormerPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<QuestionAnsweringModelOutput>} An object containing the model's output logits for question answering.\n   */\n  async _call(model_inputs) {\n    return new QuestionAnsweringModelOutput(await super._call(model_inputs));\n  }\n}\n// TODO: Add RoFormerForCausalLM and RoFormerForMultipleChoice\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// ConvBert models\nexport class ConvBertPreTrainedModel extends PreTrainedModel {}\n\n/**\n * The bare ConvBERT Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class ConvBertModel extends ConvBertPreTrainedModel {}\n\n/**\n * ConvBERT Model with a language modeling head on top.\n */\nexport class ConvBertForMaskedLM extends ConvBertPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<MaskedLMOutput>} An object containing the model's output logits for masked language modeling.\n   */\n  async _call(model_inputs) {\n    return new MaskedLMOutput(await super._call(model_inputs));\n  }\n}\n\n/**\n * ConvBERT Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output)\n */\nexport class ConvBertForSequenceClassification extends ConvBertPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n   */\n  async _call(model_inputs) {\n    return new SequenceClassifierOutput(await super._call(model_inputs));\n  }\n}\n\n/**\n * ConvBERT Model with a token classification head on top (a linear layer on top of the hidden-states output)\n * e.g. for Named-Entity-Recognition (NER) tasks.\n */\nexport class ConvBertForTokenClassification extends ConvBertPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.\n   */\n  async _call(model_inputs) {\n    return new TokenClassifierOutput(await super._call(model_inputs));\n  }\n}\n\n/**\n * ConvBERT Model with a span classification head on top for extractive question-answering tasks like SQuAD\n * (a linear layers on top of the hidden-states output to compute `span start logits` and `span end logits`)\n */\nexport class ConvBertForQuestionAnswering extends ConvBertPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<QuestionAnsweringModelOutput>} An object containing the model's output logits for question answering.\n   */\n  async _call(model_inputs) {\n    return new QuestionAnsweringModelOutput(await super._call(model_inputs));\n  }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// Electra models\nexport class ElectraPreTrainedModel extends PreTrainedModel {}\n\n/**\n * The bare Electra Model transformer outputting raw hidden-states without any specific head on top.\n * Identical to the BERT model except that it uses an additional linear layer between the embedding\n * layer and the encoder if the hidden size and embedding size are different.\n */\nexport class ElectraModel extends ElectraPreTrainedModel {}\n// TODO add ElectraForPreTraining\n/**\n * Electra model with a language modeling head on top.\n */\nexport class ElectraForMaskedLM extends ElectraPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<MaskedLMOutput>} An object containing the model's output logits for masked language modeling.\n   */\n  async _call(model_inputs) {\n    return new MaskedLMOutput(await super._call(model_inputs));\n  }\n}\n\n/**\n * ELECTRA Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output)\n */\nexport class ElectraForSequenceClassification extends ElectraPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n   */\n  async _call(model_inputs) {\n    return new SequenceClassifierOutput(await super._call(model_inputs));\n  }\n}\n\n/**\n * Electra model with a token classification head on top.\n */\nexport class ElectraForTokenClassification extends ElectraPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.\n   */\n  async _call(model_inputs) {\n    return new TokenClassifierOutput(await super._call(model_inputs));\n  }\n}\n\n/**\n * LECTRA Model with a span classification head on top for extractive question-answering tasks like SQuAD\n * (a linear layers on top of the hidden-states output to compute `span start logits` and `span end logits`).\n */\nexport class ElectraForQuestionAnswering extends ElectraPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<QuestionAnsweringModelOutput>} An object containing the model's output logits for question answering.\n   */\n  async _call(model_inputs) {\n    return new QuestionAnsweringModelOutput(await super._call(model_inputs));\n  }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// CamemBERT models\nexport class CamembertPreTrainedModel extends PreTrainedModel {}\n\n/**\n * The bare CamemBERT Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class CamembertModel extends CamembertPreTrainedModel {}\n\n/**\n * CamemBERT Model with a `language modeling` head on top.\n */\nexport class CamembertForMaskedLM extends CamembertPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<MaskedLMOutput>} An object containing the model's output logits for masked language modeling.\n   */\n  async _call(model_inputs) {\n    return new MaskedLMOutput(await super._call(model_inputs));\n  }\n}\n\n/**\n * CamemBERT Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output) e.g. for GLUE tasks.\n */\nexport class CamembertForSequenceClassification extends CamembertPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n   */\n  async _call(model_inputs) {\n    return new SequenceClassifierOutput(await super._call(model_inputs));\n  }\n}\n\n/**\n * CamemBERT Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks.\n */\nexport class CamembertForTokenClassification extends CamembertPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.\n   */\n  async _call(model_inputs) {\n    return new TokenClassifierOutput(await super._call(model_inputs));\n  }\n}\n\n/**\n * CamemBERT Model with a span classification head on top for extractive question-answering tasks\n */\nexport class CamembertForQuestionAnswering extends CamembertPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<QuestionAnsweringModelOutput>} An object containing the model's output logits for question answering.\n   */\n  async _call(model_inputs) {\n    return new QuestionAnsweringModelOutput(await super._call(model_inputs));\n  }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// DeBERTa models\nexport class DebertaPreTrainedModel extends PreTrainedModel {}\n\n/**\n * The bare DeBERTa Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class DebertaModel extends DebertaPreTrainedModel {}\n\n/**\n * DeBERTa Model with a `language modeling` head on top.\n */\nexport class DebertaForMaskedLM extends DebertaPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<MaskedLMOutput>} An object containing the model's output logits for masked language modeling.\n   */\n  async _call(model_inputs) {\n    return new MaskedLMOutput(await super._call(model_inputs));\n  }\n}\n\n/**\n * DeBERTa Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output)\n */\nexport class DebertaForSequenceClassification extends DebertaPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n   */\n  async _call(model_inputs) {\n    return new SequenceClassifierOutput(await super._call(model_inputs));\n  }\n}\n\n/**\n * DeBERTa Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks.\n */\nexport class DebertaForTokenClassification extends DebertaPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.\n   */\n  async _call(model_inputs) {\n    return new TokenClassifierOutput(await super._call(model_inputs));\n  }\n}\n\n/**\n * DeBERTa Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear\n * layers on top of the hidden-states output to compute `span start logits` and `span end logits`).\n */\nexport class DebertaForQuestionAnswering extends DebertaPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<QuestionAnsweringModelOutput>} An object containing the model's output logits for question answering.\n   */\n  async _call(model_inputs) {\n    return new QuestionAnsweringModelOutput(await super._call(model_inputs));\n  }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// DeBERTa-v2 models\nexport class DebertaV2PreTrainedModel extends PreTrainedModel {}\n\n/**\n * The bare DeBERTa-V2 Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class DebertaV2Model extends DebertaV2PreTrainedModel {}\n\n/**\n * DeBERTa-V2 Model with a `language modeling` head on top.\n */\nexport class DebertaV2ForMaskedLM extends DebertaV2PreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<MaskedLMOutput>} An object containing the model's output logits for masked language modeling.\n   */\n  async _call(model_inputs) {\n    return new MaskedLMOutput(await super._call(model_inputs));\n  }\n}\n\n/**\n * DeBERTa-V2 Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output)\n */\nexport class DebertaV2ForSequenceClassification extends DebertaV2PreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n   */\n  async _call(model_inputs) {\n    return new SequenceClassifierOutput(await super._call(model_inputs));\n  }\n}\n\n/**\n * DeBERTa-V2 Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks.\n */\nexport class DebertaV2ForTokenClassification extends DebertaV2PreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.\n   */\n  async _call(model_inputs) {\n    return new TokenClassifierOutput(await super._call(model_inputs));\n  }\n}\n\n/**\n * DeBERTa-V2 Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear\n * layers on top of the hidden-states output to compute `span start logits` and `span end logits`).\n */\nexport class DebertaV2ForQuestionAnswering extends DebertaV2PreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<QuestionAnsweringModelOutput>} An object containing the model's output logits for question answering.\n   */\n  async _call(model_inputs) {\n    return new QuestionAnsweringModelOutput(await super._call(model_inputs));\n  }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// DistilBert models\nexport class DistilBertPreTrainedModel extends PreTrainedModel {}\nexport class DistilBertModel extends DistilBertPreTrainedModel {}\n\n/**\n * DistilBertForSequenceClassification is a class representing a DistilBERT model for sequence classification.\n */\nexport class DistilBertForSequenceClassification extends DistilBertPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n   */\n  async _call(model_inputs) {\n    return new SequenceClassifierOutput(await super._call(model_inputs));\n  }\n}\n\n/**\n * DistilBertForTokenClassification is a class representing a DistilBERT model for token classification.\n */\nexport class DistilBertForTokenClassification extends DistilBertPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.\n   */\n  async _call(model_inputs) {\n    return new TokenClassifierOutput(await super._call(model_inputs));\n  }\n}\n\n/**\n * DistilBertForQuestionAnswering is a class representing a DistilBERT model for question answering.\n */\nexport class DistilBertForQuestionAnswering extends DistilBertPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<QuestionAnsweringModelOutput>} An object containing the model's output logits for question answering.\n   */\n  async _call(model_inputs) {\n    return new QuestionAnsweringModelOutput(await super._call(model_inputs));\n  }\n}\n\n/**\n * DistilBertForMaskedLM is a class representing a DistilBERT model for masking task.\n */\nexport class DistilBertForMaskedLM extends DistilBertPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<MaskedLMOutput>} returned object\n   */\n  async _call(model_inputs) {\n    return new MaskedLMOutput(await super._call(model_inputs));\n  }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// ESM models\nexport class EsmPreTrainedModel extends PreTrainedModel {}\n\n/**\n * The bare ESM Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class EsmModel extends EsmPreTrainedModel {}\n\n/**\n * ESM Model with a `language modeling` head on top.\n */\nexport class EsmForMaskedLM extends EsmPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<MaskedLMOutput>} An object containing the model's output logits for masked language modeling.\n   */\n  async _call(model_inputs) {\n    return new MaskedLMOutput(await super._call(model_inputs));\n  }\n}\n\n/**\n * ESM Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output)\n */\nexport class EsmForSequenceClassification extends EsmPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n   */\n  async _call(model_inputs) {\n    return new SequenceClassifierOutput(await super._call(model_inputs));\n  }\n}\n\n/**\n * ESM Model with a token classification head on top (a linear layer on top of the hidden-states output)\n * e.g. for Named-Entity-Recognition (NER) tasks.\n */\nexport class EsmForTokenClassification extends EsmPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.\n   */\n  async _call(model_inputs) {\n    return new TokenClassifierOutput(await super._call(model_inputs));\n  }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// MobileBert models\nexport class MobileBertPreTrainedModel extends PreTrainedModel {}\nexport class MobileBertModel extends MobileBertPreTrainedModel {}\n\n/**\n * MobileBertForMaskedLM is a class representing a MobileBERT model for masking task.\n */\nexport class MobileBertForMaskedLM extends MobileBertPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<MaskedLMOutput>} returned object\n   */\n  async _call(model_inputs) {\n    return new MaskedLMOutput(await super._call(model_inputs));\n  }\n}\n\n/**\n * MobileBert Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output)\n */\nexport class MobileBertForSequenceClassification extends MobileBertPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<SequenceClassifierOutput>} returned object\n   */\n  async _call(model_inputs) {\n    return new SequenceClassifierOutput(await super._call(model_inputs));\n  }\n}\n\n/**\n * MobileBert Model with a span classification head on top for extractive question-answering tasks\n */\nexport class MobileBertForQuestionAnswering extends MobileBertPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<QuestionAnsweringModelOutput>} returned object\n   */\n  async _call(model_inputs) {\n    return new QuestionAnsweringModelOutput(await super._call(model_inputs));\n  }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// MPNet models\nexport class MPNetPreTrainedModel extends PreTrainedModel {}\n\n/**\n * The bare MPNet Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class MPNetModel extends MPNetPreTrainedModel {}\n\n/**\n * MPNetForMaskedLM is a class representing a MPNet model for masked language modeling.\n */\nexport class MPNetForMaskedLM extends MPNetPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<MaskedLMOutput>} An object containing the model's output logits for masked language modeling.\n   */\n  async _call(model_inputs) {\n    return new MaskedLMOutput(await super._call(model_inputs));\n  }\n}\n\n/**\n * MPNetForSequenceClassification is a class representing a MPNet model for sequence classification.\n */\nexport class MPNetForSequenceClassification extends MPNetPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n   */\n  async _call(model_inputs) {\n    return new SequenceClassifierOutput(await super._call(model_inputs));\n  }\n}\n\n/**\n * MPNetForTokenClassification is a class representing a MPNet model for token classification.\n */\nexport class MPNetForTokenClassification extends MPNetPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.\n   */\n  async _call(model_inputs) {\n    return new TokenClassifierOutput(await super._call(model_inputs));\n  }\n}\n\n/**\n * MPNetForQuestionAnswering is a class representing a MPNet model for question answering.\n */\nexport class MPNetForQuestionAnswering extends MPNetPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<QuestionAnsweringModelOutput>} An object containing the model's output logits for question answering.\n   */\n  async _call(model_inputs) {\n    return new QuestionAnsweringModelOutput(await super._call(model_inputs));\n  }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// SqueezeBert models\nexport class SqueezeBertPreTrainedModel extends PreTrainedModel {}\nexport class SqueezeBertModel extends SqueezeBertPreTrainedModel {}\nexport class SqueezeBertForMaskedLM extends SqueezeBertPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<MaskedLMOutput>} returned object\n   */\n  async _call(model_inputs) {\n    return new MaskedLMOutput(await super._call(model_inputs));\n  }\n}\nexport class SqueezeBertForSequenceClassification extends SqueezeBertPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<SequenceClassifierOutput>} returned object\n   */\n  async _call(model_inputs) {\n    return new SequenceClassifierOutput(await super._call(model_inputs));\n  }\n}\nexport class SqueezeBertForQuestionAnswering extends SqueezeBertPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<QuestionAnsweringModelOutput>} returned object\n   */\n  async _call(model_inputs) {\n    return new QuestionAnsweringModelOutput(await super._call(model_inputs));\n  }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// Albert models\nexport class AlbertPreTrainedModel extends PreTrainedModel {}\nexport class AlbertModel extends AlbertPreTrainedModel {}\nexport class AlbertForSequenceClassification extends AlbertPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<SequenceClassifierOutput>} returned object\n   */\n  async _call(model_inputs) {\n    return new SequenceClassifierOutput(await super._call(model_inputs));\n  }\n}\nexport class AlbertForQuestionAnswering extends AlbertPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<QuestionAnsweringModelOutput>} returned object\n   */\n  async _call(model_inputs) {\n    return new QuestionAnsweringModelOutput(await super._call(model_inputs));\n  }\n}\nexport class AlbertForMaskedLM extends AlbertPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<MaskedLMOutput>} returned object\n   */\n  async _call(model_inputs) {\n    return new MaskedLMOutput(await super._call(model_inputs));\n  }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// T5 models\nexport class T5PreTrainedModel extends PreTrainedModel {}\n;\nexport class T5Model extends T5PreTrainedModel {}\n\n/**\n * T5Model is a class representing a T5 model for conditional generation.\n */\nexport class T5ForConditionalGeneration extends T5PreTrainedModel {\n  /**\n   * Creates a new instance of the `T5ForConditionalGeneration` class.\n   * @param {Object} config The model configuration.\n   * @param {any} session session for the model.\n   * @param {any} decoder_merged_session session for the decoder.\n   * @param {GenerationConfig} generation_config The generation configuration.\n   */\n  constructor(config, session, decoder_merged_session, generation_config) {\n    super(config, session);\n    this.decoder_merged_session = decoder_merged_session;\n    this.generation_config = generation_config;\n    this.num_decoder_layers = this.config.num_decoder_layers;\n    this.num_decoder_heads = this.config.num_heads;\n    this.decoder_dim_kv = this.config.d_kv;\n    this.num_encoder_layers = this.config.num_layers;\n    this.num_encoder_heads = this.config.num_heads;\n    this.encoder_dim_kv = this.config.d_kv;\n  }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// LONGT5 models\n/**\n * An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained models.\n */\nexport class LongT5PreTrainedModel extends PreTrainedModel {}\n;\n\n/**\n * The bare LONGT5 Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class LongT5Model extends LongT5PreTrainedModel {}\n\n/**\n * LONGT5 Model with a `language modeling` head on top.\n */\nexport class LongT5ForConditionalGeneration extends LongT5PreTrainedModel {\n  /**\n   * Creates a new instance of the `LongT5ForConditionalGeneration` class.\n   * @param {Object} config The model configuration.\n   * @param {any} session session for the model.\n   * @param {any} decoder_merged_session session for the decoder.\n   * @param {GenerationConfig} generation_config The generation configuration.\n   */\n  constructor(config, session, decoder_merged_session, generation_config) {\n    super(config, session);\n    this.decoder_merged_session = decoder_merged_session;\n    this.generation_config = generation_config;\n    this.num_decoder_layers = this.config.num_decoder_layers;\n    this.num_decoder_heads = this.config.num_heads;\n    this.decoder_dim_kv = this.config.d_kv;\n    this.num_encoder_layers = this.config.num_layers;\n    this.num_encoder_heads = this.config.num_heads;\n    this.encoder_dim_kv = this.config.d_kv;\n  }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// MT5 models\nexport class MT5PreTrainedModel extends PreTrainedModel {}\n;\nexport class MT5Model extends MT5PreTrainedModel {}\n\n/**\n * A class representing a conditional sequence-to-sequence model based on the MT5 architecture.\n */\nexport class MT5ForConditionalGeneration extends MT5PreTrainedModel {\n  /**\n   * Creates a new instance of the `MT5ForConditionalGeneration` class.\n   * @param {any} config The model configuration.\n   * @param {any} session The ONNX session containing the encoder weights.\n   * @param {any} decoder_merged_session The ONNX session containing the merged decoder weights.\n   * @param {GenerationConfig} generation_config The generation configuration.\n   */\n  constructor(config, session, decoder_merged_session, generation_config) {\n    super(config, session);\n    this.decoder_merged_session = decoder_merged_session;\n    this.generation_config = generation_config;\n    this.num_decoder_layers = this.config.num_decoder_layers;\n    this.num_decoder_heads = this.config.num_heads;\n    this.decoder_dim_kv = this.config.d_kv;\n    this.num_encoder_layers = this.config.num_layers;\n    this.num_encoder_heads = this.config.num_heads;\n    this.encoder_dim_kv = this.config.d_kv;\n  }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// Bart models\nexport class BartPretrainedModel extends PreTrainedModel {}\n;\n\n/**\n * The bare BART Model outputting raw hidden-states without any specific head on top.\n */\nexport class BartModel extends BartPretrainedModel {}\n\n/**\n * The BART Model with a language modeling head. Can be used for summarization.\n */\nexport class BartForConditionalGeneration extends BartPretrainedModel {\n  /**\n   * Creates a new instance of the `BartForConditionalGeneration` class.\n   * @param {Object} config The configuration object for the Bart model.\n   * @param {Object} session The ONNX session used to execute the model.\n   * @param {Object} decoder_merged_session The ONNX session used to execute the decoder.\n   * @param {Object} generation_config The generation configuration object.\n   */\n  constructor(config, session, decoder_merged_session, generation_config) {\n    super(config, session);\n    this.decoder_merged_session = decoder_merged_session;\n    this.generation_config = generation_config;\n    this.num_decoder_layers = this.config.decoder_layers;\n    this.num_decoder_heads = this.config.decoder_attention_heads;\n    this.decoder_dim_kv = this.config.d_model / this.num_decoder_heads;\n    this.num_encoder_layers = this.config.encoder_layers;\n    this.num_encoder_heads = this.config.encoder_attention_heads;\n    this.encoder_dim_kv = this.config.d_model / this.num_encoder_heads;\n  }\n}\n\n/**\n * Bart model with a sequence classification/head on top (a linear layer on top of the pooled output)\n */\nexport class BartForSequenceClassification extends BartPretrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n   */\n  async _call(model_inputs) {\n    return new SequenceClassifierOutput(await super._call(model_inputs));\n  }\n}\n\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// MBart models\nexport class MBartPreTrainedModel extends PreTrainedModel {}\n;\n\n/**\n * The bare MBART Model outputting raw hidden-states without any specific head on top.\n */\nexport class MBartModel extends MBartPreTrainedModel {}\n\n/**\n * The MBART Model with a language modeling head. Can be used for summarization, after fine-tuning the pretrained models.\n */\nexport class MBartForConditionalGeneration extends MBartPreTrainedModel {\n  /**\n   * Creates a new instance of the `MBartForConditionalGeneration` class.\n   * @param {Object} config The configuration object for the Bart model.\n   * @param {Object} session The ONNX session used to execute the model.\n   * @param {Object} decoder_merged_session The ONNX session used to execute the decoder.\n   * @param {Object} generation_config The generation configuration object.\n   */\n  constructor(config, session, decoder_merged_session, generation_config) {\n    super(config, session);\n    this.decoder_merged_session = decoder_merged_session;\n    this.generation_config = generation_config;\n    this.num_decoder_layers = this.config.decoder_layers;\n    this.num_decoder_heads = this.config.decoder_attention_heads;\n    this.decoder_dim_kv = this.config.d_model / this.num_decoder_heads;\n    this.num_encoder_layers = this.config.encoder_layers;\n    this.num_encoder_heads = this.config.encoder_attention_heads;\n    this.encoder_dim_kv = this.config.d_model / this.num_encoder_heads;\n  }\n}\n\n/**\n * MBart model with a sequence classification/head on top (a linear layer on top of the pooled output).\n */\nexport class MBartForSequenceClassification extends MBartPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n   */\n  async _call(model_inputs) {\n    return new SequenceClassifierOutput(await super._call(model_inputs));\n  }\n}\nexport class MBartForCausalLM extends MBartPreTrainedModel {\n  /**\n   * Creates a new instance of the `MBartForCausalLM` class.\n   * @param {Object} config Configuration object for the model.\n   * @param {Object} decoder_merged_session ONNX Session object for the decoder.\n   * @param {Object} generation_config Configuration object for the generation process.\n   */\n  constructor(config, decoder_merged_session, generation_config) {\n    super(config, decoder_merged_session);\n    this.generation_config = generation_config;\n    this.num_decoder_layers = this.config.decoder_layers;\n    this.num_decoder_heads = this.config.decoder_attention_heads;\n    this.decoder_dim_kv = this.config.d_model / this.num_decoder_heads;\n    this.num_encoder_layers = this.config.encoder_layers;\n    this.num_encoder_heads = this.config.encoder_attention_heads;\n    this.encoder_dim_kv = this.config.d_model / this.num_encoder_heads;\n  }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// Blenderbot models\nexport class BlenderbotPreTrainedModel extends PreTrainedModel {}\n;\n\n/**\n * The bare Blenderbot Model outputting raw hidden-states without any specific head on top.\n */\nexport class BlenderbotModel extends BlenderbotPreTrainedModel {}\n\n/**\n * The Blenderbot Model with a language modeling head. Can be used for summarization.\n */\nexport class BlenderbotForConditionalGeneration extends BlenderbotPreTrainedModel {\n  /**\n   * Creates a new instance of the `BlenderbotForConditionalGeneration` class.\n   * @param {any} config The model configuration.\n   * @param {any} session The ONNX session containing the encoder weights.\n   * @param {any} decoder_merged_session The ONNX session containing the merged decoder weights.\n   * @param {GenerationConfig} generation_config The generation configuration.\n   */\n  constructor(config, session, decoder_merged_session, generation_config) {\n    super(config, session);\n    this.decoder_merged_session = decoder_merged_session;\n    this.generation_config = generation_config;\n    this.num_decoder_layers = this.config.decoder_layers;\n    this.num_decoder_heads = this.config.decoder_attention_heads;\n    this.decoder_dim_kv = this.config.d_model / this.num_decoder_heads;\n    this.num_encoder_layers = this.config.encoder_layers;\n    this.num_encoder_heads = this.config.encoder_attention_heads;\n    this.encoder_dim_kv = this.config.d_model / this.num_encoder_heads;\n  }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// Blenderbot models\nexport class BlenderbotSmallPreTrainedModel extends PreTrainedModel {}\n;\n\n/**\n * The bare BlenderbotSmall Model outputting raw hidden-states without any specific head on top.\n */\nexport class BlenderbotSmallModel extends BlenderbotSmallPreTrainedModel {}\n\n/**\n * The BlenderbotSmall Model with a language modeling head. Can be used for summarization.\n */\nexport class BlenderbotSmallForConditionalGeneration extends BlenderbotSmallPreTrainedModel {\n  /**\n   * Creates a new instance of the `BlenderbotForConditionalGeneration` class.\n   * @param {any} config The model configuration.\n   * @param {any} session The ONNX session containing the encoder weights.\n   * @param {any} decoder_merged_session The ONNX session containing the merged decoder weights.\n   * @param {GenerationConfig} generation_config The generation configuration.\n   */\n  constructor(config, session, decoder_merged_session, generation_config) {\n    super(config, session);\n    this.decoder_merged_session = decoder_merged_session;\n    this.generation_config = generation_config;\n    this.num_decoder_layers = this.config.decoder_layers;\n    this.num_decoder_heads = this.config.decoder_attention_heads;\n    this.decoder_dim_kv = this.config.d_model / this.num_decoder_heads;\n    this.num_encoder_layers = this.config.encoder_layers;\n    this.num_encoder_heads = this.config.encoder_attention_heads;\n    this.encoder_dim_kv = this.config.d_model / this.num_encoder_heads;\n  }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// Roberta models\nexport class RobertaPreTrainedModel extends PreTrainedModel {}\nexport class RobertaModel extends RobertaPreTrainedModel {}\n\n/**\n * RobertaForMaskedLM class for performing masked language modeling on Roberta models.\n */\nexport class RobertaForMaskedLM extends RobertaPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<MaskedLMOutput>} returned object\n   */\n  async _call(model_inputs) {\n    return new MaskedLMOutput(await super._call(model_inputs));\n  }\n}\n\n/**\n * RobertaForSequenceClassification class for performing sequence classification on Roberta models.\n */\nexport class RobertaForSequenceClassification extends RobertaPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<SequenceClassifierOutput>} returned object\n   */\n  async _call(model_inputs) {\n    return new SequenceClassifierOutput(await super._call(model_inputs));\n  }\n}\n\n/**\n * RobertaForTokenClassification class for performing token classification on Roberta models.\n */\nexport class RobertaForTokenClassification extends RobertaPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.\n   */\n  async _call(model_inputs) {\n    return new TokenClassifierOutput(await super._call(model_inputs));\n  }\n}\n\n/**\n * RobertaForQuestionAnswering class for performing question answering on Roberta models.\n */\nexport class RobertaForQuestionAnswering extends RobertaPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<QuestionAnsweringModelOutput>} returned object\n   */\n  async _call(model_inputs) {\n    return new QuestionAnsweringModelOutput(await super._call(model_inputs));\n  }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// XLM models\n/**\n * An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained models.\n */\nexport class XLMPreTrainedModel extends PreTrainedModel {}\n\n/**\n * The bare XLM Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class XLMModel extends XLMPreTrainedModel {}\n\n/**\n * The XLM Model transformer with a language modeling head on top (linear layer with weights tied to the input embeddings).\n */\nexport class XLMWithLMHeadModel extends XLMPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<MaskedLMOutput>} returned object\n   */\n  async _call(model_inputs) {\n    return new MaskedLMOutput(await super._call(model_inputs));\n  }\n}\n\n/**\n * XLM Model with a sequence classification/regression head on top (a linear layer on top of the pooled output)\n */\nexport class XLMForSequenceClassification extends XLMPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<SequenceClassifierOutput>} returned object\n   */\n  async _call(model_inputs) {\n    return new SequenceClassifierOutput(await super._call(model_inputs));\n  }\n}\n\n/**\n * XLM Model with a token classification head on top (a linear layer on top of the hidden-states output)\n */\nexport class XLMForTokenClassification extends XLMPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.\n   */\n  async _call(model_inputs) {\n    return new TokenClassifierOutput(await super._call(model_inputs));\n  }\n}\n\n/**\n * XLM Model with a span classification head on top for extractive question-answering tasks\n */\nexport class XLMForQuestionAnswering extends XLMPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<QuestionAnsweringModelOutput>} returned object\n   */\n  async _call(model_inputs) {\n    return new QuestionAnsweringModelOutput(await super._call(model_inputs));\n  }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// XLMRoberta models\nexport class XLMRobertaPreTrainedModel extends PreTrainedModel {}\nexport class XLMRobertaModel extends XLMRobertaPreTrainedModel {}\n\n/**\n * XLMRobertaForMaskedLM class for performing masked language modeling on XLMRoberta models.\n */\nexport class XLMRobertaForMaskedLM extends XLMRobertaPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<MaskedLMOutput>} returned object\n   */\n  async _call(model_inputs) {\n    return new MaskedLMOutput(await super._call(model_inputs));\n  }\n}\n\n/**\n * XLMRobertaForSequenceClassification class for performing sequence classification on XLMRoberta models.\n */\nexport class XLMRobertaForSequenceClassification extends XLMRobertaPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<SequenceClassifierOutput>} returned object\n   */\n  async _call(model_inputs) {\n    return new SequenceClassifierOutput(await super._call(model_inputs));\n  }\n}\n\n/**\n * XLMRobertaForTokenClassification class for performing token classification on XLMRoberta models.\n */\nexport class XLMRobertaForTokenClassification extends XLMRobertaPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.\n   */\n  async _call(model_inputs) {\n    return new TokenClassifierOutput(await super._call(model_inputs));\n  }\n}\n\n/**\n * XLMRobertaForQuestionAnswering class for performing question answering on XLMRoberta models.\n */\nexport class XLMRobertaForQuestionAnswering extends XLMRobertaPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   *\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<QuestionAnsweringModelOutput>} returned object\n   */\n  async _call(model_inputs) {\n    return new QuestionAnsweringModelOutput(await super._call(model_inputs));\n  }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// Audio Spectrogram Transformer (AST) models\nexport class ASTPreTrainedModel extends PreTrainedModel {}\n;\n\n/**\n * The bare AST Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class ASTModel extends ASTPreTrainedModel {}\n\n/**\n * Audio Spectrogram Transformer model with an audio classification head on top\n * (a linear layer on top of the pooled output) e.g. for datasets like AudioSet, Speech Commands v2.\n */\nexport class ASTForAudioClassification extends ASTPreTrainedModel {}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// Whisper models\nexport class WhisperPreTrainedModel extends PreTrainedModel {}\n;\n\n/**\n * WhisperModel class for training Whisper models without a language model head.\n */\nexport class WhisperModel extends WhisperPreTrainedModel {}\n\n/**\n * WhisperForConditionalGeneration class for generating conditional outputs from Whisper models.\n */\nexport class WhisperForConditionalGeneration extends WhisperPreTrainedModel {\n  requires_attention_mask = false;\n  main_input_name = 'input_features';\n\n  /**\n   * Creates a new instance of the `WhisperForConditionalGeneration` class.\n   * @param {Object} config Configuration object for the model.\n   * @param {Object} session ONNX Session object for the model.\n   * @param {Object} decoder_merged_session ONNX Session object for the decoder.\n   * @param {Object} generation_config Configuration object for the generation process.\n   */\n  constructor(config, session, decoder_merged_session, generation_config) {\n    super(config, session);\n    this.decoder_merged_session = decoder_merged_session;\n    this.generation_config = generation_config;\n    this.num_decoder_layers = this.config.decoder_layers;\n    this.num_decoder_heads = this.config.decoder_attention_heads;\n    this.decoder_dim_kv = this.config.d_model / this.num_decoder_heads;\n    this.num_encoder_layers = this.config.encoder_layers;\n    this.num_encoder_heads = this.config.encoder_attention_heads;\n    this.encoder_dim_kv = this.config.d_model / this.num_encoder_heads;\n  }\n\n  /**\n   * @typedef {Object} WhisperGenerationConfig\n   * @extends GenerationConfig\n   * @property {boolean} [return_timestamps=null] Whether to return the timestamps with the text. This enables the `WhisperTimestampsLogitsProcessor`.\n   * @property {boolean} [return_token_timestamps=null] Whether to return token-level timestamps\n   * with the text. This can be used with or without the `return_timestamps` option. To get word-level\n   * timestamps, use the tokenizer to group the tokens into words.\n   * @property {number} [num_frames=null]  The number of audio frames available in this chunk. This is only used generating word-level timestamps.\n   */\n\n  /**\n   * Generates outputs based on input and generation configuration.\n   * @param {Object} inputs Input data for the model.\n   * @param {WhisperGenerationConfig} generation_config Configuration object for the generation process.\n   * @param {Object} logits_processor Optional logits processor object.\n   * @returns {Promise<Object>} Promise object represents the generated outputs.\n   */\n  async generate(inputs, generation_config = null, logits_processor = null\n  // {\n  //     return_timestamps = null,\n  //     return_token_timestamps = null,\n  //     language = null,\n  //     task = null,\n  // } = {},\n  ) {\n    // Create generation config object\n    generation_config = this._get_generation_config(generation_config);\n\n    // Whisper has additional options for returning timestamps\n    generation_config.return_timestamps ??= false;\n\n    // TODO add language and task\n\n    if (generation_config.return_timestamps) {\n      logits_processor = [new WhisperTimeStampLogitsProcessor(generation_config)];\n    }\n    if (generation_config.return_token_timestamps) {\n      generation_config.output_attentions = true;\n      generation_config.return_dict_in_generate = true;\n      if (generation_config.task === 'translate') {\n        console.warn(\"Token-level timestamps may not be reliable for task 'translate'.\");\n      }\n      if (!generation_config.alignment_heads) {\n        throw new Error(\"Model generation config has no `alignment_heads`, token-level timestamps not available. \" + \"See https://gist.github.com/hollance/42e32852f24243b748ae6bc1f985b13a on how to add this property to the generation config.\");\n      }\n    }\n    const outputs = await super.generate(inputs, generation_config, logits_processor);\n    if (generation_config.return_token_timestamps && generation_config.alignment_heads) {\n      outputs[\"token_timestamps\"] = this._extract_token_timestamps(outputs, generation_config.alignment_heads, generation_config.num_frames);\n    }\n    return outputs;\n  }\n\n  /**\n   * Calculates token-level timestamps using the encoder-decoder cross-attentions and\n   * dynamic time-warping (DTW) to map each output token to a position in the input audio.\n   * @param {Object} generate_outputs Outputs generated by the model\n   * @param {Tensor[][][]} generate_outputs.cross_attentions The cross attentions output by the model\n   * @param {Tensor[][][]} generate_outputs.decoder_attentions The decoder attentions output by the model\n   * @param {number[][]} generate_outputs.sequences The sequences output by the model\n   * @param {number[][]} alignment_heads Alignment heads of the model\n   * @param {number} [num_frames=null] Number of frames in the input audio.\n   * @param {number} [time_precision=0.02] Precision of the timestamps in seconds\n   * @returns {Tensor} tensor containing the timestamps in seconds for each predicted token\n   */\n  _extract_token_timestamps(generate_outputs, alignment_heads, num_frames = null, time_precision = 0.02) {\n    if (!generate_outputs.cross_attentions) {\n      throw new Error(\"Model outputs must contain cross attentions to extract timestamps. \" + \"This is most likely because the model was not exported with `output_attentions=True`.\");\n    }\n    let median_filter_width = this.config.median_filter_width;\n    if (median_filter_width === undefined) {\n      console.warn(\"Model config has no `median_filter_width`, using default value of 7.\");\n      median_filter_width = 7;\n    }\n    const batchedMatrices = generate_outputs.cross_attentions.map(batch => {\n      // Create a list with `decoder_layers` elements, each a tensor of shape\n      // (batch size, attention_heads, output length, input length).\n      let cross_attentions = Array.from({\n        length: this.config.decoder_layers\n      }, (_, i) => cat(batch.map(x => x[i]), 2));\n      let weights = stack(alignment_heads.map(([l, h]) => {\n        return num_frames ? cross_attentions[l].slice(null, h, null, [0, num_frames]) : cross_attentions[l].slice(null, h);\n      }));\n      weights = weights.transpose(1, 0, 2, 3);\n      let [std, calculatedMean] = std_mean(weights, -2, 0, true);\n\n      // Normalize and smoothen the weights.\n      let smoothedWeights = weights.clone(); // [1, 8, seqLength, 1500]\n\n      for (let a = 0; a < smoothedWeights.dims[0]; ++a) {\n        let aTensor = smoothedWeights[a]; // [8, seqLength, 1500]\n\n        for (let b = 0; b < aTensor.dims[0]; ++b) {\n          let bTensor = aTensor[b]; // [seqLength, 1500]\n\n          const stdTensor = std[a][b][0]; // [1500]\n          const meanTensor = calculatedMean[a][b][0]; // [1500]\n\n          for (let c = 0; c < bTensor.dims[0]; ++c) {\n            let cTensor = bTensor[c]; // [1500]\n            for (let d = 0; d < cTensor.data.length; ++d) {\n              cTensor.data[d] = (cTensor.data[d] - meanTensor.data[d]) / stdTensor.data[d];\n            }\n\n            // Apply median filter.\n            cTensor.data.set(medianFilter(cTensor.data, median_filter_width));\n          }\n        }\n      }\n\n      // Average the different cross-attention heads.\n      const matrix = mean(smoothedWeights, 1);\n      return matrix;\n    });\n    const timestampsShape = [generate_outputs.sequences.length, generate_outputs.sequences[0].length];\n    const timestamps = new Tensor('float32', new Float32Array(timestampsShape[0] * timestampsShape[1]), timestampsShape);\n\n    // Perform dynamic time warping on each element of the batch.\n    for (let batch_idx = 0; batch_idx < timestampsShape[0]; ++batch_idx) {\n      // NOTE: Since we run only one batch at a time, we can squeeze to get the same dimensions\n      // as the python implementation\n      const matrix = batchedMatrices[batch_idx].neg().squeeze_(0);\n      let [text_indices, time_indices] = dynamicTimeWarping(matrix);\n      let diffs = Array.from({\n        length: text_indices.length - 1\n      }, (v, i) => text_indices[i + 1] - text_indices[i]);\n      let jumps = mergeArrays([1], diffs).map(x => !!x); // convert to boolean\n\n      let jump_times = [];\n      for (let i = 0; i < jumps.length; ++i) {\n        if (jumps[i]) {\n          jump_times.push(time_indices[i] * time_precision);\n          // NOTE: No point in rounding here, since we set to Float32Array later\n        }\n      }\n      timestamps[batch_idx].data.set(jump_times, 1);\n    }\n    return timestamps;\n  }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n/**\n * Vision Encoder-Decoder model based on OpenAI's GPT architecture for image captioning and other vision tasks\n */\nexport class VisionEncoderDecoderModel extends PreTrainedModel {\n  main_input_name = 'pixel_values';\n\n  /**\n   * Creates a new instance of the `VisionEncoderDecoderModel` class.\n   * @param {Object} config The configuration object specifying the hyperparameters and other model settings.\n   * @param {Object} session The ONNX session containing the encoder model.\n   * @param {any} decoder_merged_session The ONNX session containing the merged decoder model.\n   * @param {Object} generation_config Configuration object for the generation process.\n   */\n  constructor(config, session, decoder_merged_session, generation_config) {\n    super(config, session);\n    this.decoder_merged_session = decoder_merged_session;\n    this.generation_config = generation_config;\n\n    // Extract configs\n    const encoderConfig = this.config.encoder;\n    const decoderConfig = this.config.decoder;\n\n    // Validate encoder\n    const encoderModelType = encoderConfig.model_type;\n    const encoderModel = MODEL_MAPPING_NAMES_ENCODER_ONLY.get(encoderModelType) ?? MODEL_MAPPING_NAMES_ENCODER_DECODER.get(encoderModelType);\n    if (!encoderModel) {\n      console.warn(`Model type for encoder '${encoderModelType}' not found, assuming encoder-only architecture. Please report this at https://github.com/xenova/transformers.js/issues/new/choose.`);\n    }\n\n    // Validate decoder\n    const decoderModel = MODEL_WITH_LM_HEAD_MAPPING_NAMES.get(decoderConfig.model_type);\n    if (!decoderModel) {\n      throw new Error(`Unable to construct \\`VisionEncoderDecoder\\` due to unsupported decoder: \"${this.config.decoder.model_type}\"`);\n    }\n\n    // @ts-ignore\n    const decoderModelClass = decoderModel[1];\n    // @ts-ignore\n    const decoder = new decoderModelClass(decoderConfig, decoder_merged_session, generation_config);\n    this.add_encoder_pkv = 'num_decoder_layers' in decoder;\n    if (this.add_encoder_pkv) {\n      // Decoder is part of an encoder-decoder model\n      this.num_decoder_layers = decoder.num_decoder_layers;\n      this.num_decoder_heads = decoder.num_decoder_heads;\n      this.decoder_dim_kv = decoder.decoder_dim_kv;\n      this.num_encoder_layers = decoder.num_encoder_layers;\n      this.num_encoder_heads = decoder.num_encoder_heads;\n      this.encoder_dim_kv = decoder.encoder_dim_kv;\n    } else {\n      // Decoder is a decoder-only model\n      this.num_layers = decoder.num_layers;\n      this.num_heads = decoder.num_heads;\n      this.dim_kv = decoder.dim_kv;\n    }\n  }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// CLIP models\nexport class CLIPPreTrainedModel extends PreTrainedModel {}\n\n/**\n * CLIP Text and Vision Model with a projection layers on top\n * \n * **Example:** Perform zero-shot image classification with a `CLIPModel`.\n * \n * ```javascript\n * import { AutoTokenizer, AutoProcessor, CLIPModel, RawImage } from '@xenova/transformers';\n * \n * // Load tokenizer, processor, and model\n * let tokenizer = await AutoTokenizer.from_pretrained('Xenova/clip-vit-base-patch16');\n * let processor = await AutoProcessor.from_pretrained('Xenova/clip-vit-base-patch16');\n * let model = await CLIPModel.from_pretrained('Xenova/clip-vit-base-patch16');\n * \n * // Run tokenization\n * let texts = ['a photo of a car', 'a photo of a football match']\n * let text_inputs = tokenizer(texts, { padding: true, truncation: true });\n * \n * // Read image and run processor\n * let image = await RawImage.read('https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/football-match.jpg');\n * let image_inputs = await processor(image);\n * \n * // Run model with both text and pixel inputs\n * let output = await model({ ...text_inputs, ...image_inputs });\n * // {\n * //   logits_per_image: Tensor {\n * //     dims: [ 1, 2 ],\n * //     data: Float32Array(2) [ 18.579734802246094, 24.31830596923828 ],\n * //   },\n * //   logits_per_text: Tensor {\n * //     dims: [ 2, 1 ],\n * //     data: Float32Array(2) [ 18.579734802246094, 24.31830596923828 ],\n * //   },\n * //   text_embeds: Tensor {\n * //     dims: [ 2, 512 ],\n * //     data: Float32Array(1024) [ ... ],\n * //   },\n * //   image_embeds: Tensor {\n * //     dims: [ 1, 512 ],\n * //     data: Float32Array(512) [ ... ],\n * //   }\n * // }\n * ```\n */\nexport class CLIPModel extends CLIPPreTrainedModel {}\n\n/**\n * CLIP Text Model with a projection layer on top (a linear layer on top of the pooled output)\n * \n * **Example:** Compute text embeddings with `CLIPTextModelWithProjection`.\n * \n * ```javascript\n * import { AutoTokenizer, CLIPTextModelWithProjection } from '@xenova/transformers';\n * \n * // Load tokenizer and text model\n * const tokenizer = await AutoTokenizer.from_pretrained('Xenova/clip-vit-base-patch16');\n * const text_model = await CLIPTextModelWithProjection.from_pretrained('Xenova/clip-vit-base-patch16');\n * \n * // Run tokenization\n * let texts = ['a photo of a car', 'a photo of a football match'];\n * let text_inputs = tokenizer(texts, { padding: true, truncation: true });\n * \n * // Compute embeddings\n * const { text_embeds } = await text_model(text_inputs);\n * // Tensor {\n * //   dims: [ 2, 512 ],\n * //   type: 'float32',\n * //   data: Float32Array(1024) [ ... ],\n * //   size: 1024\n * // }\n * ```\n */\nexport class CLIPTextModelWithProjection extends CLIPPreTrainedModel {\n  /** @type {PreTrainedModel.from_pretrained} */\n  static async from_pretrained(pretrained_model_name_or_path, options = {}) {\n    // Update default model file name if not provided\n    options.model_file_name ??= 'text_model';\n    return super.from_pretrained(pretrained_model_name_or_path, options);\n  }\n}\n\n/**\n * CLIP Vision Model with a projection layer on top (a linear layer on top of the pooled output)\n * \n * **Example:** Compute vision embeddings with `CLIPVisionModelWithProjection`.\n * \n * ```javascript\n * import { AutoProcessor, CLIPVisionModelWithProjection, RawImage} from '@xenova/transformers';\n * \n * // Load processor and vision model\n * const processor = await AutoProcessor.from_pretrained('Xenova/clip-vit-base-patch16');\n * const vision_model = await CLIPVisionModelWithProjection.from_pretrained('Xenova/clip-vit-base-patch16');\n * \n * // Read image and run processor\n * let image = await RawImage.read('https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/football-match.jpg');\n * let image_inputs = await processor(image);\n * \n * // Compute embeddings\n * const { image_embeds } = await vision_model(image_inputs);\n * // Tensor {\n * //   dims: [ 1, 512 ],\n * //   type: 'float32',\n * //   data: Float32Array(512) [ ... ],\n * //   size: 512\n * // }\n * ```\n */\nexport class CLIPVisionModelWithProjection extends CLIPPreTrainedModel {\n  /** @type {PreTrainedModel.from_pretrained} */\n  static async from_pretrained(pretrained_model_name_or_path, options = {}) {\n    // Update default model file name if not provided\n    options.model_file_name ??= 'vision_model';\n    return super.from_pretrained(pretrained_model_name_or_path, options);\n  }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// SigLIP models\nexport class SiglipPreTrainedModel extends PreTrainedModel {}\n\n/**\n * SigLIP Text and Vision Model with a projection layers on top\n * \n * **Example:** Perform zero-shot image classification with a `SiglipModel`.\n * \n * ```javascript\n * import { AutoTokenizer, AutoProcessor, SiglipModel, RawImage } from '@xenova/transformers';\n * \n * // Load tokenizer, processor, and model\n * const tokenizer = await AutoTokenizer.from_pretrained('Xenova/siglip-base-patch16-224');\n * const processor = await AutoProcessor.from_pretrained('Xenova/siglip-base-patch16-224');\n * const model = await SiglipModel.from_pretrained('Xenova/siglip-base-patch16-224');\n * \n * // Run tokenization\n * const texts = ['a photo of 2 cats', 'a photo of 2 dogs'];\n * const text_inputs = tokenizer(texts, { padding: 'max_length', truncation: true });\n * \n * // Read image and run processor\n * const image = await RawImage.read('http://images.cocodataset.org/val2017/000000039769.jpg');\n * const image_inputs = await processor(image);\n * \n * // Run model with both text and pixel inputs\n * const output = await model({ ...text_inputs, ...image_inputs });\n * // {\n * //   logits_per_image: Tensor {\n * //     dims: [ 1, 2 ],\n * //     data: Float32Array(2) [ -1.6019744873046875, -10.720091819763184 ],\n * //   },\n * //   logits_per_text: Tensor {\n * //     dims: [ 2, 1 ],\n * //     data: Float32Array(2) [ -1.6019744873046875, -10.720091819763184 ],\n * //   },\n * //   text_embeds: Tensor {\n * //     dims: [ 2, 768 ],\n * //     data: Float32Array(1536) [ ... ],\n * //   },\n * //   image_embeds: Tensor {\n * //     dims: [ 1, 768 ],\n * //     data: Float32Array(768) [ ... ],\n * //   }\n * // }\n * ```\n */\nexport class SiglipModel extends SiglipPreTrainedModel {}\n\n/**\n * The text model from SigLIP without any head or projection on top.\n * \n * **Example:** Compute text embeddings with `SiglipTextModel`.\n * \n * ```javascript\n * import { AutoTokenizer, SiglipTextModel } from '@xenova/transformers';\n * \n * // Load tokenizer and text model\n * const tokenizer = await AutoTokenizer.from_pretrained('Xenova/siglip-base-patch16-224');\n * const text_model = await SiglipTextModel.from_pretrained('Xenova/siglip-base-patch16-224');\n * \n * // Run tokenization\n * const texts = ['a photo of 2 cats', 'a photo of 2 dogs'];\n * const text_inputs = tokenizer(texts, { padding: 'max_length', truncation: true });\n * \n * // Compute embeddings\n * const { pooler_output } = await text_model(text_inputs);\n * // Tensor {\n * //   dims: [ 2, 768 ],\n * //   type: 'float32',\n * //   data: Float32Array(1536) [ ... ],\n * //   size: 1536\n * // }\n * ```\n */\nexport class SiglipTextModel extends SiglipPreTrainedModel {\n  /** @type {PreTrainedModel.from_pretrained} */\n  static async from_pretrained(pretrained_model_name_or_path, options = {}) {\n    // Update default model file name if not provided\n    options.model_file_name ??= 'text_model';\n    return super.from_pretrained(pretrained_model_name_or_path, options);\n  }\n}\n\n/**\n * The vision model from SigLIP without any head or projection on top.\n * \n * **Example:** Compute vision embeddings with `SiglipVisionModel`.\n * \n * ```javascript\n * import { AutoProcessor, SiglipVisionModel, RawImage} from '@xenova/transformers';\n * \n * // Load processor and vision model\n * const processor = await AutoProcessor.from_pretrained('Xenova/siglip-base-patch16-224');\n * const vision_model = await SiglipVisionModel.from_pretrained('Xenova/siglip-base-patch16-224');\n * \n * // Read image and run processor\n * const image = await RawImage.read('https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/football-match.jpg');\n * const image_inputs = await processor(image);\n * \n * // Compute embeddings\n * const { pooler_output } = await vision_model(image_inputs);\n * // Tensor {\n * //   dims: [ 1, 768 ],\n * //   type: 'float32',\n * //   data: Float32Array(768) [ ... ],\n * //   size: 768\n * // }\n * ```\n */\nexport class SiglipVisionModel extends CLIPPreTrainedModel {\n  /** @type {PreTrainedModel.from_pretrained} */\n  static async from_pretrained(pretrained_model_name_or_path, options = {}) {\n    // Update default model file name if not provided\n    options.model_file_name ??= 'vision_model';\n    return super.from_pretrained(pretrained_model_name_or_path, options);\n  }\n}\n//////////////////////////////////////////////////\n// ChineseCLIP models\nexport class ChineseCLIPPreTrainedModel extends PreTrainedModel {}\nexport class ChineseCLIPModel extends ChineseCLIPPreTrainedModel {}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// CLIPSeg models\nexport class CLIPSegPreTrainedModel extends PreTrainedModel {}\nexport class CLIPSegModel extends CLIPSegPreTrainedModel {}\n\n/**\n * CLIPSeg model with a Transformer-based decoder on top for zero-shot and one-shot image segmentation.\n * \n * **Example:** Perform zero-shot image segmentation with a `CLIPSegForImageSegmentation` model.\n * \n * ```javascript\n * import { AutoTokenizer, AutoProcessor, CLIPSegForImageSegmentation, RawImage } from '@xenova/transformers';\n * \n * // Load tokenizer, processor, and model\n * const tokenizer = await AutoTokenizer.from_pretrained('Xenova/clipseg-rd64-refined');\n * const processor = await AutoProcessor.from_pretrained('Xenova/clipseg-rd64-refined');\n * const model = await CLIPSegForImageSegmentation.from_pretrained('Xenova/clipseg-rd64-refined');\n * \n * // Run tokenization\n * const texts = ['a glass', 'something to fill', 'wood', 'a jar'];\n * const text_inputs = tokenizer(texts, { padding: true, truncation: true });\n * \n * // Read image and run processor\n * const image = await RawImage.read('https://github.com/timojl/clipseg/blob/master/example_image.jpg?raw=true');\n * const image_inputs = await processor(image);\n * \n * // Run model with both text and pixel inputs\n * const { logits } = await model({ ...text_inputs, ...image_inputs });\n * // logits: Tensor {\n * //   dims: [4, 352, 352],\n * //   type: 'float32',\n * //   data: Float32Array(495616) [ ... ],\n * //   size: 495616\n * // }\n * ```\n * \n * You can visualize the predictions as follows:\n * ```javascript\n * const preds = logits\n *   .unsqueeze_(1)\n *   .sigmoid_()\n *   .mul_(255)\n *   .round_()\n *   .to('uint8');\n * \n * for (let i = 0; i < preds.dims[0]; ++i) {\n *   const img = RawImage.fromTensor(preds[i]);\n *   img.save(`prediction_${i}.png`);\n * }\n * ```\n */\nexport class CLIPSegForImageSegmentation extends CLIPSegPreTrainedModel {}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// GPT2 models\nexport class GPT2PreTrainedModel extends PreTrainedModel {\n  /**\n   * Creates a new instance of the `GPT2PreTrainedModel` class.\n   * @param {Object} config The configuration of the model.\n   * @param {any} session The ONNX session containing the model weights.\n   * @param {GenerationConfig} generation_config The generation configuration.\n   */\n  constructor(config, session, generation_config) {\n    super(config, session);\n    this.generation_config = generation_config;\n\n    // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n    this.config.pad_token_id = this.config.eos_token_id;\n    this.num_heads = this.config.n_head;\n    this.num_layers = this.config.n_layer;\n    this.dim_kv = this.config.n_embd / this.num_heads;\n  }\n}\nexport class GPT2Model extends GPT2PreTrainedModel {}\n\n/**\n * GPT-2 language model head on top of the GPT-2 base model. This model is suitable for text generation tasks.\n */\nexport class GPT2LMHeadModel extends GPT2PreTrainedModel {}\n// export class GPT2ForSequenceClassification extends GPT2PreTrainedModel {\n// TODO\n// }\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// GPTNeo models\nexport class GPTNeoPreTrainedModel extends PreTrainedModel {\n  /**\n   * Creates a new instance of the `GPTNeoPreTrainedModel` class.\n   * @param {Object} config The configuration of the model.\n   * @param {any} session The ONNX session containing the model weights.\n   * @param {GenerationConfig} generation_config The generation configuration.\n   */\n  constructor(config, session, generation_config) {\n    super(config, session);\n    this.generation_config = generation_config;\n\n    // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n    this.config.pad_token_id = this.config.eos_token_id;\n    this.num_heads = this.config.num_heads;\n    this.num_layers = this.config.num_layers;\n    this.dim_kv = this.config.hidden_size / this.num_heads;\n  }\n}\nexport class GPTNeoModel extends GPTNeoPreTrainedModel {}\nexport class GPTNeoForCausalLM extends GPTNeoPreTrainedModel {}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// GPTNeoX models\nexport class GPTNeoXPreTrainedModel extends PreTrainedModel {\n  /**\n   * Creates a new instance of the `GPTNeoXPreTrainedModel` class.\n   * @param {Object} config The configuration of the model.\n   * @param {any} session The ONNX session containing the model weights.\n   * @param {GenerationConfig} generation_config The generation configuration.\n   */\n  constructor(config, session, generation_config) {\n    super(config, session);\n    this.generation_config = generation_config;\n\n    // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n    this.config.pad_token_id = this.config.eos_token_id;\n    this.num_heads = this.config.num_attention_heads;\n    this.num_layers = this.config.num_hidden_layers;\n    this.dim_kv = this.config.hidden_size / this.num_heads;\n  }\n}\nexport class GPTNeoXModel extends GPTNeoXPreTrainedModel {}\nexport class GPTNeoXForCausalLM extends GPTNeoXPreTrainedModel {}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// GPT-J models\nexport class GPTJPreTrainedModel extends PreTrainedModel {\n  /**\n   * Creates a new instance of the `GPTJPreTrainedModel` class.\n   * @param {Object} config The configuration of the model.\n   * @param {any} session The ONNX session containing the model weights.\n   * @param {GenerationConfig} generation_config The generation configuration.\n   */\n  constructor(config, session, generation_config) {\n    super(config, session);\n    this.generation_config = generation_config;\n\n    // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n    this.config.pad_token_id = this.config.eos_token_id;\n    this.num_heads = this.config.n_head;\n    this.num_layers = this.config.n_layer;\n    this.dim_kv = this.config.n_embd / this.num_heads;\n  }\n}\nexport class GPTJModel extends GPTJPreTrainedModel {}\nexport class GPTJForCausalLM extends GPTJPreTrainedModel {}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// GPTBigCode models\nexport class GPTBigCodePreTrainedModel extends PreTrainedModel {\n  /**\n   * Creates a new instance of the `GPTBigCodePreTrainedModel` class.\n   * @param {Object} config The configuration of the model.\n   * @param {any} session The ONNX session containing the model weights.\n   * @param {GenerationConfig} generation_config The generation configuration.\n   */\n  constructor(config, session, generation_config) {\n    super(config, session);\n    this.generation_config = generation_config;\n\n    // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n    this.config.pad_token_id = this.config.eos_token_id;\n    this.num_heads = this.config.n_head;\n    this.num_layers = this.config.n_layer;\n    this.dim_kv = this.config.n_embd / this.num_heads;\n  }\n}\nexport class GPTBigCodeModel extends GPTBigCodePreTrainedModel {}\nexport class GPTBigCodeForCausalLM extends GPTBigCodePreTrainedModel {}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// CodeGen models\nexport class CodeGenPreTrainedModel extends PreTrainedModel {\n  /**\n   * Creates a new instance of the `CodeGenPreTrainedModel` class.\n   * @param {Object} config The model configuration object.\n   * @param {Object} session The ONNX session object.\n   * @param {GenerationConfig} generation_config The generation configuration.\n   */\n  constructor(config, session, generation_config) {\n    super(config, session);\n    this.generation_config = generation_config;\n\n    // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n    this.config.pad_token_id = this.config.eos_token_id;\n    this.num_heads = this.config.n_head;\n    this.num_layers = this.config.n_layer;\n    this.dim_kv = this.config.n_embd / this.num_heads;\n  }\n}\n/**\n * CodeGenModel is a class representing a code generation model without a language model head.\n */\nexport class CodeGenModel extends CodeGenPreTrainedModel {}\n\n/**\n * CodeGenForCausalLM is a class that represents a code generation model based on the GPT-2 architecture. It extends the `CodeGenPreTrainedModel` class.\n */\nexport class CodeGenForCausalLM extends CodeGenPreTrainedModel {}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// LLama models\n\n/**\n * The bare LLama Model outputting raw hidden-states without any specific head on top.\n */\nexport class LlamaPreTrainedModel extends PreTrainedModel {\n  /**\n   * Creates a new instance of the `LlamaPreTrainedModel` class.\n   * @param {Object} config The model configuration object.\n   * @param {Object} session The ONNX session object.\n   * @param {GenerationConfig} generation_config The generation configuration.\n   */\n  constructor(config, session, generation_config) {\n    super(config, session);\n    this.generation_config = generation_config;\n\n    // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n    this.config.pad_token_id = this.config.eos_token_id;\n    this.num_heads = this.config.num_key_value_heads ?? this.config.num_attention_heads;\n    this.num_layers = this.config.num_hidden_layers;\n    this.dim_kv = this.config.hidden_size / this.config.num_attention_heads;\n  }\n}\n/**\n * The bare LLaMA Model outputting raw hidden-states without any specific head on top.\n */\nexport class LlamaModel extends LlamaPreTrainedModel {}\nexport class LlamaForCausalLM extends LlamaPreTrainedModel {}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// Qwen2 models\n\n/**\n * The bare Qwen2 Model outputting raw hidden-states without any specific head on top.\n */\nexport class Qwen2PreTrainedModel extends PreTrainedModel {\n  /**\n   * Creates a new instance of the `Qwen2PreTrainedModel` class.\n   * @param {Object} config The model configuration object.\n   * @param {Object} session The ONNX session object.\n   * @param {GenerationConfig} generation_config The generation configuration.\n   */\n  constructor(config, session, generation_config) {\n    super(config, session);\n    this.generation_config = generation_config;\n\n    // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n    this.config.pad_token_id = this.config.eos_token_id;\n    this.num_heads = this.config.num_key_value_heads ?? this.config.num_attention_heads;\n    this.num_layers = this.config.num_hidden_layers;\n    this.dim_kv = this.config.hidden_size / this.config.num_attention_heads;\n  }\n}\n/**\n * The bare Qwen2 Model outputting raw hidden-states without any specific head on top.\n */\nexport class Qwen2Model extends Qwen2PreTrainedModel {}\nexport class Qwen2ForCausalLM extends Qwen2PreTrainedModel {}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// Phi models\n\nexport class PhiPreTrainedModel extends PreTrainedModel {\n  /**\n   * Creates a new instance of the `PhiPreTrainedModel` class.\n   * @param {Object} config The model configuration object.\n   * @param {Object} session The ONNX session object.\n   * @param {GenerationConfig} generation_config The generation configuration.\n   */\n  constructor(config, session, generation_config) {\n    super(config, session);\n    this.generation_config = generation_config;\n\n    // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n    this.config.pad_token_id = this.config.eos_token_id;\n    this.num_heads = this.config.num_attention_heads;\n    this.num_layers = this.config.num_hidden_layers;\n    this.dim_kv = this.config.hidden_size / this.num_heads;\n  }\n}\n/**\n * The bare Phi Model outputting raw hidden-states without any specific head on top.\n */\nexport class PhiModel extends PhiPreTrainedModel {}\nexport class PhiForCausalLM extends PhiPreTrainedModel {}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// Bloom models\n/**\n * The Bloom Model transformer with a language modeling head on top (linear layer with weights tied to the input embeddings).\n */\nexport class BloomPreTrainedModel extends PreTrainedModel {\n  /**\n   * Creates a new instance of the `BloomPreTrainedModel` class.\n   * @param {Object} config The configuration of the model.\n   * @param {any} session The ONNX session containing the model weights.\n   * @param {GenerationConfig} generation_config The generation configuration.\n   */\n  constructor(config, session, generation_config) {\n    super(config, session);\n    this.generation_config = generation_config;\n\n    // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n    this.config.pad_token_id = this.config.eos_token_id;\n    this.num_heads = this.config.n_head;\n    this.num_layers = this.config.n_layer;\n    this.dim_kv = this.config.hidden_size / this.num_heads;\n  }\n}\n\n/**\n * The bare Bloom Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class BloomModel extends BloomPreTrainedModel {}\n\n/**\n * The Bloom Model transformer with a language modeling head on top (linear layer with weights tied to the input embeddings).\n */\nexport class BloomForCausalLM extends BloomPreTrainedModel {}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// MPT models\nexport class MptPreTrainedModel extends PreTrainedModel {\n  /**\n   * Creates a new instance of the `MptPreTrainedModel` class.\n   * @param {Object} config The model configuration object.\n   * @param {Object} session The ONNX session object.\n   * @param {GenerationConfig} generation_config The generation configuration.\n   */\n  constructor(config, session, generation_config) {\n    super(config, session);\n    this.generation_config = generation_config;\n\n    // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n    this.config.pad_token_id = this.config.eos_token_id;\n    this.num_heads = this.config.n_heads;\n    this.num_layers = this.config.n_layers;\n    this.dim_kv = this.config.d_model / this.num_heads;\n  }\n}\n\n/**\n * The bare Mpt Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class MptModel extends MptPreTrainedModel {}\n\n/**\n * The MPT Model transformer with a language modeling head on top (linear layer with weights tied to the input embeddings).\n */\nexport class MptForCausalLM extends MptPreTrainedModel {}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// OPT models\nexport class OPTPreTrainedModel extends PreTrainedModel {\n  /**\n   * Creates a new instance of the `OPTPreTrainedModel` class.\n   * @param {Object} config The model configuration object.\n   * @param {Object} session The ONNX session object.\n   * @param {GenerationConfig} generation_config The generation configuration.\n   */\n  constructor(config, session, generation_config) {\n    super(config, session);\n    this.generation_config = generation_config;\n\n    // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n    this.config.pad_token_id = this.config.eos_token_id;\n    this.num_heads = this.config.num_attention_heads;\n    this.num_layers = this.config.num_hidden_layers;\n    this.dim_kv = this.config.hidden_size / this.num_heads;\n  }\n}\n\n/**\n * The bare OPT Model outputting raw hidden-states without any specific head on top.\n */\nexport class OPTModel extends OPTPreTrainedModel {}\n\n/**\n * The OPT Model transformer with a language modeling head on top (linear layer with weights tied to the input embeddings).\n */\nexport class OPTForCausalLM extends OPTPreTrainedModel {}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\nexport class ViTPreTrainedModel extends PreTrainedModel {}\nexport class ViTModel extends ViTPreTrainedModel {}\nexport class ViTForImageClassification extends ViTPreTrainedModel {\n  /**\n   * @param {any} model_inputs\n   */\n  async _call(model_inputs) {\n    return new SequenceClassifierOutput(await super._call(model_inputs));\n  }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\nexport class FastViTPreTrainedModel extends PreTrainedModel {}\nexport class FastViTModel extends FastViTPreTrainedModel {}\nexport class FastViTForImageClassification extends FastViTPreTrainedModel {\n  /**\n   * @param {any} model_inputs\n   */\n  async _call(model_inputs) {\n    return new SequenceClassifierOutput(await super._call(model_inputs));\n  }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\nexport class VitMattePreTrainedModel extends PreTrainedModel {}\n\n/**\n * ViTMatte framework leveraging any vision backbone e.g. for ADE20k, CityScapes.\n * \n * **Example:** Perform image matting with a `VitMatteForImageMatting` model.\n * ```javascript\n * import { AutoProcessor, VitMatteForImageMatting, RawImage } from '@xenova/transformers';\n * \n * // Load processor and model\n * const processor = await AutoProcessor.from_pretrained('Xenova/vitmatte-small-distinctions-646');\n * const model = await VitMatteForImageMatting.from_pretrained('Xenova/vitmatte-small-distinctions-646');\n * \n * // Load image and trimap\n * const image = await RawImage.fromURL('https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/vitmatte_image.png');\n * const trimap = await RawImage.fromURL('https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/vitmatte_trimap.png');\n * \n * // Prepare image + trimap for the model\n * const inputs = await processor(image, trimap);\n * \n * // Predict alpha matte\n * const { alphas } = await model(inputs);\n * // Tensor {\n * //   dims: [ 1, 1, 640, 960 ],\n * //   type: 'float32',\n * //   size: 614400,\n * //   data: Float32Array(614400) [ 0.9894027709960938, 0.9970508813858032, ... ]\n * // }\n * ```\n * \n * You can visualize the alpha matte as follows:\n * ```javascript\n * import { Tensor, cat } from '@xenova/transformers';\n * \n * // Visualize predicted alpha matte\n * const imageTensor = image.toTensor();\n * \n * // Convert float (0-1) alpha matte to uint8 (0-255)\n * const alphaChannel = alphas\n *   .squeeze(0)\n *   .mul_(255)\n *   .clamp_(0, 255)\n *   .round_()\n *   .to('uint8');\n * \n * // Concatenate original image with predicted alpha\n * const imageData = cat([imageTensor, alphaChannel], 0);\n * \n * // Save output image\n * const outputImage = RawImage.fromTensor(imageData);\n * outputImage.save('output.png');\n * ```\n */\nexport class VitMatteForImageMatting extends VitMattePreTrainedModel {\n  /**\n   * @param {any} model_inputs\n   */\n  async _call(model_inputs) {\n    return new ImageMattingOutput(await super._call(model_inputs));\n  }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\nexport class MobileViTPreTrainedModel extends PreTrainedModel {}\nexport class MobileViTModel extends MobileViTPreTrainedModel {}\nexport class MobileViTForImageClassification extends MobileViTPreTrainedModel {\n  /**\n   * @param {any} model_inputs\n   */\n  async _call(model_inputs) {\n    return new SequenceClassifierOutput(await super._call(model_inputs));\n  }\n}\n// TODO: MobileViTForSemanticSegmentation\n\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\nexport class MobileViTV2PreTrainedModel extends PreTrainedModel {}\nexport class MobileViTV2Model extends MobileViTV2PreTrainedModel {}\nexport class MobileViTV2ForImageClassification extends MobileViTV2PreTrainedModel {\n  /**\n   * @param {any} model_inputs\n   */\n  async _call(model_inputs) {\n    return new SequenceClassifierOutput(await super._call(model_inputs));\n  }\n}\n// TODO: MobileViTV2ForSemanticSegmentation\n\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\nexport class OwlViTPreTrainedModel extends PreTrainedModel {}\nexport class OwlViTModel extends OwlViTPreTrainedModel {}\nexport class OwlViTForObjectDetection extends OwlViTPreTrainedModel {}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\nexport class Owlv2PreTrainedModel extends PreTrainedModel {}\nexport class Owlv2Model extends Owlv2PreTrainedModel {}\nexport class Owlv2ForObjectDetection extends Owlv2PreTrainedModel {}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// Beit Models\nexport class BeitPreTrainedModel extends PreTrainedModel {}\nexport class BeitModel extends BeitPreTrainedModel {}\nexport class BeitForImageClassification extends BeitPreTrainedModel {\n  /**\n   * @param {any} model_inputs\n   */\n  async _call(model_inputs) {\n    return new SequenceClassifierOutput(await super._call(model_inputs));\n  }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\nexport class DetrPreTrainedModel extends PreTrainedModel {}\nexport class DetrModel extends DetrPreTrainedModel {}\nexport class DetrForObjectDetection extends DetrPreTrainedModel {\n  /**\n   * @param {any} model_inputs\n   */\n  async _call(model_inputs) {\n    return new DetrObjectDetectionOutput(await super._call(model_inputs));\n  }\n}\nexport class DetrForSegmentation extends DetrPreTrainedModel {\n  /**\n   * Runs the model with the provided inputs\n   * @param {Object} model_inputs Model inputs\n   * @returns {Promise<DetrSegmentationOutput>} Object containing segmentation outputs\n   */\n  async _call(model_inputs) {\n    return new DetrSegmentationOutput(await super._call(model_inputs));\n  }\n}\nexport class DetrObjectDetectionOutput extends ModelOutput {\n  /**\n   * @param {Object} output The output of the model.\n   * @param {Tensor} output.logits Classification logits (including no-object) for all queries.\n   * @param {Tensor} output.pred_boxes Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height).\n   * These values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding possible padding).\n   */\n  constructor({\n    logits,\n    pred_boxes\n  }) {\n    super();\n    this.logits = logits;\n    this.pred_boxes = pred_boxes;\n  }\n}\nexport class DetrSegmentationOutput extends ModelOutput {\n  /**\n   * @param {Object} output The output of the model.\n   * @param {Tensor} output.logits The output logits of the model.\n   * @param {Tensor} output.pred_boxes Predicted boxes.\n   * @param {Tensor} output.pred_masks Predicted masks.\n   */\n  constructor({\n    logits,\n    pred_boxes,\n    pred_masks\n  }) {\n    super();\n    this.logits = logits;\n    this.pred_boxes = pred_boxes;\n    this.pred_masks = pred_masks;\n  }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\nexport class TableTransformerPreTrainedModel extends PreTrainedModel {}\n\n/**\n * The bare Table Transformer Model (consisting of a backbone and encoder-decoder Transformer)\n * outputting raw hidden-states without any specific head on top.\n */\nexport class TableTransformerModel extends TableTransformerPreTrainedModel {}\n\n/**\n * Table Transformer Model (consisting of a backbone and encoder-decoder Transformer)\n * with object detection heads on top, for tasks such as COCO detection.\n */\nexport class TableTransformerForObjectDetection extends TableTransformerPreTrainedModel {\n  /**\n   * @param {any} model_inputs\n   */\n  async _call(model_inputs) {\n    return new TableTransformerObjectDetectionOutput(await super._call(model_inputs));\n  }\n}\nexport class TableTransformerObjectDetectionOutput extends DetrObjectDetectionOutput {}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\nexport class DeiTPreTrainedModel extends PreTrainedModel {}\nexport class DeiTModel extends DeiTPreTrainedModel {}\nexport class DeiTForImageClassification extends DeiTPreTrainedModel {\n  /**\n   * @param {any} model_inputs\n   */\n  async _call(model_inputs) {\n    return new SequenceClassifierOutput(await super._call(model_inputs));\n  }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n/**\n * An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained models.\n */\nexport class ResNetPreTrainedModel extends PreTrainedModel {}\n\n/**\n * The bare ResNet model outputting raw features without any specific head on top.\n */\nexport class ResNetModel extends ResNetPreTrainedModel {}\n\n/**\n * ResNet Model with an image classification head on top (a linear layer on top of the pooled features), e.g. for ImageNet.\n */\nexport class ResNetForImageClassification extends ResNetPreTrainedModel {\n  /**\n   * @param {any} model_inputs\n   */\n  async _call(model_inputs) {\n    return new SequenceClassifierOutput(await super._call(model_inputs));\n  }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\nexport class SwinPreTrainedModel extends PreTrainedModel {}\nexport class SwinModel extends SwinPreTrainedModel {}\nexport class SwinForImageClassification extends SwinPreTrainedModel {\n  /**\n   * @param {any} model_inputs\n   */\n  async _call(model_inputs) {\n    return new SequenceClassifierOutput(await super._call(model_inputs));\n  }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\nexport class Swin2SRPreTrainedModel extends PreTrainedModel {}\n\n/**\n * The bare Swin2SR Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class Swin2SRModel extends Swin2SRPreTrainedModel {}\n\n/**\n * Swin2SR Model transformer with an upsampler head on top for image super resolution and restoration.\n * \n * **Example:** Super-resolution w/ `Xenova/swin2SR-classical-sr-x2-64`.\n * \n * ```javascript\n * import { AutoProcessor, Swin2SRForImageSuperResolution, RawImage } from '@xenova/transformers';\n * \n * // Load processor and model\n * const model_id = 'Xenova/swin2SR-classical-sr-x2-64';\n * const processor = await AutoProcessor.from_pretrained(model_id);\n * const model = await Swin2SRForImageSuperResolution.from_pretrained(model_id);\n * \n * // Prepare model inputs\n * const url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/butterfly.jpg';\n * const image = await RawImage.fromURL(url);\n * const inputs = await processor(image);\n * \n * // Run model\n * const outputs = await model(inputs);\n * \n * // Convert Tensor to RawImage\n * const output = outputs.reconstruction.squeeze().clamp_(0, 1).mul_(255).round_().to('uint8');\n * const outputImage = RawImage.fromTensor(output);\n * // RawImage {\n * //   data: Uint8Array(786432) [ 41, 31, 24, ... ],\n * //   width: 512,\n * //   height: 512,\n * //   channels: 3\n * // }\n * ```\n */\nexport class Swin2SRForImageSuperResolution extends Swin2SRPreTrainedModel {}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\nexport class DPTPreTrainedModel extends PreTrainedModel {}\n\n/**\n * The bare DPT Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class DPTModel extends DPTPreTrainedModel {}\n\n/**\n * DPT Model with a depth estimation head on top (consisting of 3 convolutional layers) e.g. for KITTI, NYUv2.\n * \n * **Example:** Depth estimation w/ `Xenova/dpt-hybrid-midas`.\n * ```javascript\n * import { DPTForDepthEstimation, AutoProcessor, RawImage, interpolate, max } from '@xenova/transformers';\n * \n * // Load model and processor\n * const model_id = 'Xenova/dpt-hybrid-midas';\n * const model = await DPTForDepthEstimation.from_pretrained(model_id);\n * const processor = await AutoProcessor.from_pretrained(model_id);\n * \n * // Load image from URL\n * const url = 'http://images.cocodataset.org/val2017/000000039769.jpg';\n * const image = await RawImage.fromURL(url);\n * \n * // Prepare image for the model\n * const inputs = await processor(image);\n * \n * // Run model\n * const { predicted_depth } = await model(inputs);\n * \n * // Interpolate to original size\n * const prediction = interpolate(predicted_depth, image.size.reverse(), 'bilinear', false);\n * \n * // Visualize the prediction\n * const formatted = prediction.mul_(255 / max(prediction.data)[0]).to('uint8');\n * const depth = RawImage.fromTensor(formatted);\n * // RawImage {\n * //   data: Uint8Array(307200) [ 85, 85, 84, ... ],\n * //   width: 640,\n * //   height: 480,\n * //   channels: 1\n * // }\n * ```\n */\nexport class DPTForDepthEstimation extends DPTPreTrainedModel {}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\nexport class DepthAnythingPreTrainedModel extends PreTrainedModel {}\n\n/**\n * Depth Anything Model with a depth estimation head on top (consisting of 3 convolutional layers) e.g. for KITTI, NYUv2.\n */\nexport class DepthAnythingForDepthEstimation extends DepthAnythingPreTrainedModel {}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\nexport class GLPNPreTrainedModel extends PreTrainedModel {}\n\n/**\n * The bare GLPN encoder (Mix-Transformer) outputting raw hidden-states without any specific head on top.\n */\nexport class GLPNModel extends GLPNPreTrainedModel {}\n\n/**\n * GLPN Model transformer with a lightweight depth estimation head on top e.g. for KITTI, NYUv2.\n * \n * **Example:** Depth estimation w/ `Xenova/glpn-kitti`.\n * ```javascript\n * import { GLPNForDepthEstimation, AutoProcessor, RawImage, interpolate, max } from '@xenova/transformers';\n * \n * // Load model and processor\n * const model_id = 'Xenova/glpn-kitti';\n * const model = await GLPNForDepthEstimation.from_pretrained(model_id);\n * const processor = await AutoProcessor.from_pretrained(model_id);\n * \n * // Load image from URL\n * const url = 'http://images.cocodataset.org/val2017/000000039769.jpg';\n * const image = await RawImage.fromURL(url);\n * \n * // Prepare image for the model\n * const inputs = await processor(image);\n * \n * // Run model\n * const { predicted_depth } = await model(inputs);\n * \n * // Interpolate to original size\n * const prediction = interpolate(predicted_depth, image.size.reverse(), 'bilinear', false);\n * \n * // Visualize the prediction\n * const formatted = prediction.mul_(255 / max(prediction.data)[0]).to('uint8');\n * const depth = RawImage.fromTensor(formatted);\n * // RawImage {\n * //   data: Uint8Array(307200) [ 207, 169, 154, ... ],\n * //   width: 640,\n * //   height: 480,\n * //   channels: 1\n * // }\n * ```\n */\nexport class GLPNForDepthEstimation extends GLPNPreTrainedModel {}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\nexport class DonutSwinPreTrainedModel extends PreTrainedModel {}\n\n/**\n * The bare Donut Swin Model transformer outputting raw hidden-states without any specific head on top.\n * \n * **Example:** Step-by-step Document Parsing.\n * \n * ```javascript\n * import { AutoProcessor, AutoTokenizer, AutoModelForVision2Seq, RawImage } from '@xenova/transformers';\n * \n * // Choose model to use\n * const model_id = 'Xenova/donut-base-finetuned-cord-v2';\n * \n * // Prepare image inputs\n * const processor = await AutoProcessor.from_pretrained(model_id);\n * const url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/receipt.png';\n * const image = await RawImage.read(url);\n * const image_inputs = await processor(image);\n * \n * // Prepare decoder inputs\n * const tokenizer = await AutoTokenizer.from_pretrained(model_id);\n * const task_prompt = '<s_cord-v2>';\n * const decoder_input_ids = tokenizer(task_prompt, {\n *   add_special_tokens: false,\n * }).input_ids;\n * \n * // Create the model\n * const model = await AutoModelForVision2Seq.from_pretrained(model_id);\n * \n * // Run inference\n * const output = await model.generate(image_inputs.pixel_values, {\n *   decoder_input_ids,\n *   max_length: model.config.decoder.max_position_embeddings,\n * });\n * \n * // Decode output\n * const decoded = tokenizer.batch_decode(output)[0];\n * // <s_cord-v2><s_menu><s_nm> CINNAMON SUGAR</s_nm><s_unitprice> 17,000</s_unitprice><s_cnt> 1 x</s_cnt><s_price> 17,000</s_price></s_menu><s_sub_total><s_subtotal_price> 17,000</s_subtotal_price></s_sub_total><s_total><s_total_price> 17,000</s_total_price><s_cashprice> 20,000</s_cashprice><s_changeprice> 3,000</s_changeprice></s_total></s>\n * ```\n * \n * **Example:** Step-by-step Document Visual Question Answering (DocVQA)\n * \n * ```javascript\n * import { AutoProcessor, AutoTokenizer, AutoModelForVision2Seq, RawImage } from '@xenova/transformers';\n * \n * // Choose model to use\n * const model_id = 'Xenova/donut-base-finetuned-docvqa';\n * \n * // Prepare image inputs\n * const processor = await AutoProcessor.from_pretrained(model_id);\n * const url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/invoice.png';\n * const image = await RawImage.read(url);\n * const image_inputs = await processor(image);\n * \n * // Prepare decoder inputs\n * const tokenizer = await AutoTokenizer.from_pretrained(model_id);\n * const question = 'What is the invoice number?';\n * const task_prompt = `<s_docvqa><s_question>${question}</s_question><s_answer>`;\n * const decoder_input_ids = tokenizer(task_prompt, {\n *   add_special_tokens: false,\n * }).input_ids;\n * \n * // Create the model\n * const model = await AutoModelForVision2Seq.from_pretrained(model_id);\n * \n * // Run inference\n * const output = await model.generate(image_inputs.pixel_values, {\n *   decoder_input_ids,\n *   max_length: model.config.decoder.max_position_embeddings,\n * });\n * \n * // Decode output\n * const decoded = tokenizer.batch_decode(output)[0];\n * // <s_docvqa><s_question> What is the invoice number?</s_question><s_answer> us-001</s_answer></s>\n * ```\n */\nexport class DonutSwinModel extends DonutSwinPreTrainedModel {}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\nexport class ConvNextPreTrainedModel extends PreTrainedModel {}\n\n/**\n * The bare ConvNext model outputting raw features without any specific head on top.\n */\nexport class ConvNextModel extends ConvNextPreTrainedModel {}\n\n/**\n * ConvNext Model with an image classification head on top (a linear layer on top of the pooled features), e.g. for ImageNet.\n */\nexport class ConvNextForImageClassification extends ConvNextPreTrainedModel {\n  /**\n   * @param {any} model_inputs\n   */\n  async _call(model_inputs) {\n    return new SequenceClassifierOutput(await super._call(model_inputs));\n  }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\nexport class ConvNextV2PreTrainedModel extends PreTrainedModel {}\n\n/**\n * The bare ConvNextV2 model outputting raw features without any specific head on top.\n */\nexport class ConvNextV2Model extends ConvNextV2PreTrainedModel {}\n\n/**\n * ConvNextV2 Model with an image classification head on top (a linear layer on top of the pooled features), e.g. for ImageNet.\n */\nexport class ConvNextV2ForImageClassification extends ConvNextV2PreTrainedModel {\n  /**\n   * @param {any} model_inputs\n   */\n  async _call(model_inputs) {\n    return new SequenceClassifierOutput(await super._call(model_inputs));\n  }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\nexport class Dinov2PreTrainedModel extends PreTrainedModel {}\n\n/**\n * The bare DINOv2 Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class Dinov2Model extends Dinov2PreTrainedModel {}\n\n/**\n * Dinov2 Model transformer with an image classification head on top (a linear layer on top of the final hidden state of the [CLS] token) e.g. for ImageNet.\n */\nexport class Dinov2ForImageClassification extends Dinov2PreTrainedModel {\n  /**\n   * @param {any} model_inputs\n   */\n  async _call(model_inputs) {\n    return new SequenceClassifierOutput(await super._call(model_inputs));\n  }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\nexport class YolosPreTrainedModel extends PreTrainedModel {}\nexport class YolosModel extends YolosPreTrainedModel {}\nexport class YolosForObjectDetection extends YolosPreTrainedModel {\n  /**\n   * @param {any} model_inputs\n   */\n  async _call(model_inputs) {\n    return new YolosObjectDetectionOutput(await super._call(model_inputs));\n  }\n}\nexport class YolosObjectDetectionOutput extends ModelOutput {\n  /**\n   * @param {Object} output The output of the model.\n   * @param {Tensor} output.logits Classification logits (including no-object) for all queries.\n   * @param {Tensor} output.pred_boxes Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height).\n   * These values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding possible padding).\n   */\n  constructor({\n    logits,\n    pred_boxes\n  }) {\n    super();\n    this.logits = logits;\n    this.pred_boxes = pred_boxes;\n  }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\nexport class SamPreTrainedModel extends PreTrainedModel {}\n\n/**\n * Segment Anything Model (SAM) for generating segmentation masks, given an input image\n * and optional 2D location and bounding boxes.\n * \n * **Example:** Perform mask generation w/ `Xenova/sam-vit-base`.\n * ```javascript\n * import { SamModel, AutoProcessor, RawImage } from '@xenova/transformers';\n * \n * const model = await SamModel.from_pretrained('Xenova/sam-vit-base');\n * const processor = await AutoProcessor.from_pretrained('Xenova/sam-vit-base');\n * \n * const img_url = 'https://huggingface.co/ybelkada/segment-anything/resolve/main/assets/car.png';\n * const raw_image = await RawImage.read(img_url);\n * const input_points = [[[450, 600]]] // 2D localization of a window\n * \n * const inputs = await processor(raw_image, input_points);\n * const outputs = await model(inputs);\n * \n * const masks = await processor.post_process_masks(outputs.pred_masks, inputs.original_sizes, inputs.reshaped_input_sizes);\n * // [\n * //   Tensor {\n * //     dims: [ 1, 3, 1764, 2646 ],\n * //     type: 'bool',\n * //     data: Uint8Array(14002632) [ ... ],\n * //     size: 14002632\n * //   }\n * // ]\n * const scores = outputs.iou_scores;\n * // Tensor {\n * //   dims: [ 1, 1, 3 ],\n * //   type: 'float32',\n * //   data: Float32Array(3) [\n * //     0.8892380595207214,\n * //     0.9311248064041138,\n * //     0.983696699142456\n * //   ],\n * //   size: 3\n * // }\n * ```\n */\nexport class SamModel extends SamPreTrainedModel {\n  /**\n   * Creates a new instance of the `SamModel` class.\n   * @param {Object} config The configuration object specifying the hyperparameters and other model settings.\n   * @param {Object} vision_encoder The ONNX session containing the vision encoder model.\n   * @param {any} prompt_encoder_mask_decoder The ONNX session containing the prompt encoder and mask decoder model.\n   */\n  constructor(config, vision_encoder, prompt_encoder_mask_decoder) {\n    super(config, vision_encoder);\n    this.prompt_encoder_mask_decoder = prompt_encoder_mask_decoder;\n  }\n\n  /**\n   * Compute image embeddings and positional image embeddings, given the pixel values of an image.\n   * @param {Object} model_inputs Object containing the model inputs.\n   * @param {Tensor} model_inputs.pixel_values Pixel values obtained using a `SamProcessor`.\n   * @returns {Promise<{ image_embeddings: Tensor, image_positional_embeddings: Tensor }>} The image embeddings and positional image embeddings.\n   */\n  async get_image_embeddings({\n    pixel_values\n  }) {\n    // in:\n    //  - pixel_values: tensor.float32[batch_size,3,1024,1024]\n    // \n    // out:\n    //  - image_embeddings: tensor.float32[batch_size,256,64,64]\n    //  - image_positional_embeddings: tensor.float32[batch_size,256,64,64]\n    return await encoderForward(this, {\n      pixel_values\n    });\n  }\n\n  /**\n   * @typedef {Object} SamModelInputs Object containing the model inputs.\n   * @property {Tensor} pixel_values Pixel values as a Tensor with shape `(batch_size, num_channels, height, width)`.\n   * These can be obtained using a `SamProcessor`.\n   * @property {Tensor} input_points Input 2D spatial points with shape `(batch_size, num_points, 2)`.\n   * This is used by the prompt encoder to encode the prompt.\n   * @property {Tensor} [input_labels] Input labels for the points, as a Tensor of shape `(batch_size, point_batch_size, num_points)`.\n   * This is used by the prompt encoder to encode the prompt. There are 4 types of labels:\n   *  - `1`: the point is a point that contains the object of interest\n   *  - `0`: the point is a point that does not contain the object of interest\n   *  - `-1`: the point corresponds to the background\n   *  - `-10`: the point is a padding point, thus should be ignored by the prompt encoder\n   * @property {Tensor} [image_embeddings] Image embeddings used by the mask decoder.\n   * @property {Tensor} [image_positional_embeddings] Image positional embeddings used by the mask decoder.\n   */\n\n  /**\n   * @param {SamModelInputs} model_inputs Object containing the model inputs.\n   * @returns {Promise<Object>} The output of the model.\n   */\n  async forward(model_inputs) {\n    if (!model_inputs.image_embeddings || !model_inputs.image_positional_embeddings) {\n      // Compute the image embeddings if they are missing\n      model_inputs = {\n        ...model_inputs,\n        ...(await this.get_image_embeddings(model_inputs))\n      };\n    }\n    if (!model_inputs.input_labels) {\n      // Set default input labels if they are missing\n      const shape = model_inputs.input_points.dims.slice(0, -1);\n      const numElements = shape.reduce((a, b) => a * b, 1);\n      model_inputs.input_labels = new Tensor('int64', new BigInt64Array(numElements).fill(1n), shape);\n    }\n\n    // Returns:\n    //  - iou_scores: tensor.float32[batch_size,point_batch_size,3]\n    //  - pred_masks: tensor.float32[batch_size,point_batch_size,3,256,256]\n    return await sessionRun(this.prompt_encoder_mask_decoder, {\n      input_points: model_inputs.input_points,\n      input_labels: model_inputs.input_labels,\n      image_embeddings: model_inputs.image_embeddings,\n      image_positional_embeddings: model_inputs.image_positional_embeddings\n    });\n  }\n\n  /**\n   * Runs the model with the provided inputs\n   * @param {Object} model_inputs Model inputs\n   * @returns {Promise<SamImageSegmentationOutput>} Object containing segmentation outputs\n   */\n  async _call(model_inputs) {\n    return new SamImageSegmentationOutput(await super._call(model_inputs));\n  }\n}\n\n/**\n * Base class for Segment-Anything model's output.\n */\nexport class SamImageSegmentationOutput extends ModelOutput {\n  /**\n   * @param {Object} output The output of the model.\n   * @param {Tensor} output.iou_scores The output logits of the model.\n   * @param {Tensor} output.pred_masks Predicted boxes.\n   */\n  constructor({\n    iou_scores,\n    pred_masks\n  }) {\n    super();\n    this.iou_scores = iou_scores;\n    this.pred_masks = pred_masks;\n  }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// MarianMT models\nexport class MarianPreTrainedModel extends PreTrainedModel {}\n;\nexport class MarianModel extends MarianPreTrainedModel {}\nexport class MarianMTModel extends MarianPreTrainedModel {\n  /**\n   * Creates a new instance of the `MarianMTModel` class.\n  * @param {Object} config The model configuration object.\n  * @param {Object} session The ONNX session object.\n  * @param {any} decoder_merged_session \n  * @param {any} generation_config \n  */\n  constructor(config, session, decoder_merged_session, generation_config) {\n    super(config, session);\n    this.decoder_merged_session = decoder_merged_session;\n    this.generation_config = generation_config;\n    this.num_decoder_layers = this.config.decoder_layers;\n    this.num_decoder_heads = this.config.decoder_attention_heads;\n    this.decoder_dim_kv = this.config.d_model / this.num_decoder_heads;\n    this.num_encoder_layers = this.config.encoder_layers;\n    this.num_encoder_heads = this.config.encoder_attention_heads;\n    this.encoder_dim_kv = this.config.d_model / this.num_encoder_heads;\n  }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// M2M100 models\nexport class M2M100PreTrainedModel extends PreTrainedModel {}\n;\nexport class M2M100Model extends M2M100PreTrainedModel {}\nexport class M2M100ForConditionalGeneration extends M2M100PreTrainedModel {\n  /**\n   * Creates a new instance of the `M2M100ForConditionalGeneration` class.\n  * @param {Object} config The model configuration object.\n  * @param {Object} session The ONNX session object.\n  * @param {any} decoder_merged_session \n  * @param {any} generation_config \n  */\n  constructor(config, session, decoder_merged_session, generation_config) {\n    super(config, session);\n    this.decoder_merged_session = decoder_merged_session;\n    this.generation_config = generation_config;\n    this.num_decoder_layers = this.config.decoder_layers;\n    this.num_decoder_heads = this.config.decoder_attention_heads;\n    this.decoder_dim_kv = this.config.d_model / this.num_decoder_heads;\n    this.num_encoder_layers = this.config.encoder_layers;\n    this.num_encoder_heads = this.config.encoder_attention_heads;\n    this.encoder_dim_kv = this.config.d_model / this.num_encoder_heads;\n  }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// Wav2Vec2 models\nexport class Wav2Vec2PreTrainedModel extends PreTrainedModel {}\n;\n\n/**\n * The bare Wav2Vec2 Model transformer outputting raw hidden-states without any specific head on top.\n * \n * **Example:** Load and run a `Wav2Vec2Model` for feature extraction.\n * \n * ```javascript\n * import { AutoProcessor, AutoModel, read_audio } from '@xenova/transformers';\n * \n * // Read and preprocess audio\n * const processor = await AutoProcessor.from_pretrained('Xenova/mms-300m');\n * const audio = await read_audio('https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac', 16000);\n * const inputs = await processor(audio);\n * \n * // Run model with inputs\n * const model = await AutoModel.from_pretrained('Xenova/mms-300m');\n * const output = await model(inputs);\n * // {\n * //   last_hidden_state: Tensor {\n * //     dims: [ 1, 1144, 1024 ],\n * //     type: 'float32',\n * //     data: Float32Array(1171456) [ ... ],\n * //     size: 1171456\n * //   }\n * // }\n * ```\n */\nexport class Wav2Vec2Model extends Wav2Vec2PreTrainedModel {}\nexport class Wav2Vec2ForCTC extends Wav2Vec2PreTrainedModel {\n  /**\n   * @param {Object} model_inputs\n   * @param {Tensor} model_inputs.input_values Float values of input raw speech waveform.\n   * @param {Tensor} model_inputs.attention_mask Mask to avoid performing convolution and attention on padding token indices. Mask values selected in [0, 1]\n   */\n  async _call(model_inputs) {\n    return new CausalLMOutput(await super._call(model_inputs));\n  }\n}\nexport class Wav2Vec2ForSequenceClassification extends Wav2Vec2PreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n   */\n  async _call(model_inputs) {\n    return new SequenceClassifierOutput(await super._call(model_inputs));\n  }\n}\n\n/**\n * Wav2Vec2 Model with a frame classification head on top for tasks like Speaker Diarization.\n */\nexport class Wav2Vec2ForAudioFrameClassification extends Wav2Vec2PreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for sequence classification.\n   */\n  async _call(model_inputs) {\n    return new TokenClassifierOutput(await super._call(model_inputs));\n  }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// UniSpeech models\nexport class UniSpeechPreTrainedModel extends PreTrainedModel {}\n;\n\n/**\n * The bare UniSpeech Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class UniSpeechModel extends UniSpeechPreTrainedModel {}\n\n/**\n * UniSpeech Model with a `language modeling` head on top for Connectionist Temporal Classification (CTC).\n */\nexport class UniSpeechForCTC extends UniSpeechPreTrainedModel {\n  /**\n   * @param {Object} model_inputs\n   * @param {Tensor} model_inputs.input_values Float values of input raw speech waveform.\n   * @param {Tensor} model_inputs.attention_mask Mask to avoid performing convolution and attention on padding token indices. Mask values selected in [0, 1]\n   */\n  async _call(model_inputs) {\n    return new CausalLMOutput(await super._call(model_inputs));\n  }\n}\n\n/**\n * UniSpeech Model with a sequence classification head on top (a linear layer over the pooled output).\n */\nexport class UniSpeechForSequenceClassification extends UniSpeechPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n   */\n  async _call(model_inputs) {\n    return new SequenceClassifierOutput(await super._call(model_inputs));\n  }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// UniSpeechSat models\nexport class UniSpeechSatPreTrainedModel extends PreTrainedModel {}\n;\n\n/**\n * The bare UniSpeechSat Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class UniSpeechSatModel extends UniSpeechSatPreTrainedModel {}\n\n/**\n * UniSpeechSat Model with a `language modeling` head on top for Connectionist Temporal Classification (CTC).\n */\nexport class UniSpeechSatForCTC extends UniSpeechSatPreTrainedModel {\n  /**\n   * @param {Object} model_inputs\n   * @param {Tensor} model_inputs.input_values Float values of input raw speech waveform.\n   * @param {Tensor} model_inputs.attention_mask Mask to avoid performing convolution and attention on padding token indices. Mask values selected in [0, 1]\n   */\n  async _call(model_inputs) {\n    return new CausalLMOutput(await super._call(model_inputs));\n  }\n}\n\n/**\n * UniSpeechSat Model with a sequence classification head on top (a linear layer over the pooled output).\n */\nexport class UniSpeechSatForSequenceClassification extends UniSpeechSatPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n   */\n  async _call(model_inputs) {\n    return new SequenceClassifierOutput(await super._call(model_inputs));\n  }\n}\n\n/**\n * UniSpeechSat Model with a frame classification head on top for tasks like Speaker Diarization.\n */\nexport class UniSpeechSatForAudioFrameClassification extends UniSpeechSatPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for sequence classification.\n   */\n  async _call(model_inputs) {\n    return new TokenClassifierOutput(await super._call(model_inputs));\n  }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// Wav2Vec2Bert models\nexport class Wav2Vec2BertPreTrainedModel extends PreTrainedModel {}\n;\n\n/**\n * The bare Wav2Vec2Bert Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class Wav2Vec2BertModel extends Wav2Vec2BertPreTrainedModel {}\n\n/**\n * Wav2Vec2Bert Model with a `language modeling` head on top for Connectionist Temporal Classification (CTC).\n */\nexport class Wav2Vec2BertForCTC extends Wav2Vec2BertPreTrainedModel {\n  /**\n   * @param {Object} model_inputs\n   * @param {Tensor} model_inputs.input_features Float values of input mel-spectrogram.\n   * @param {Tensor} model_inputs.attention_mask Mask to avoid performing convolution and attention on padding token indices. Mask values selected in [0, 1]\n   */\n  async _call(model_inputs) {\n    return new CausalLMOutput(await super._call(model_inputs));\n  }\n}\n\n/**\n * Wav2Vec2Bert Model with a sequence classification head on top (a linear layer over the pooled output).\n */\nexport class Wav2Vec2BertForSequenceClassification extends Wav2Vec2BertPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n   */\n  async _call(model_inputs) {\n    return new SequenceClassifierOutput(await super._call(model_inputs));\n  }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// Hubert models\nexport class HubertPreTrainedModel extends PreTrainedModel {}\n\n/**\n * The bare Hubert Model transformer outputting raw hidden-states without any specific head on top.\n * \n * **Example:** Load and run a `HubertModel` for feature extraction.\n * \n * ```javascript\n * import { AutoProcessor, AutoModel, read_audio } from '@xenova/transformers';\n * \n * // Read and preprocess audio\n * const processor = await AutoProcessor.from_pretrained('Xenova/hubert-base-ls960');\n * const audio = await read_audio('https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/jfk.wav', 16000);\n * const inputs = await processor(audio);\n * \n * // Load and run model with inputs\n * const model = await AutoModel.from_pretrained('Xenova/hubert-base-ls960');\n * const output = await model(inputs);\n * // {\n * //   last_hidden_state: Tensor {\n * //     dims: [ 1, 549, 768 ],\n * //     type: 'float32',\n * //     data: Float32Array(421632) [0.0682469978928566, 0.08104046434164047, -0.4975186586380005, ...],\n * //     size: 421632\n * //   }\n * // }\n * ```\n */\nexport class HubertModel extends Wav2Vec2PreTrainedModel {}\n\n/**\n * Hubert Model with a `language modeling` head on top for Connectionist Temporal Classification (CTC).\n */\nexport class HubertForCTC extends Wav2Vec2PreTrainedModel {\n  /**\n   * @param {Object} model_inputs\n   * @param {Tensor} model_inputs.input_values Float values of input raw speech waveform.\n   * @param {Tensor} model_inputs.attention_mask Mask to avoid performing convolution and attention on padding token indices. Mask values selected in [0, 1]\n   */\n  async _call(model_inputs) {\n    return new CausalLMOutput(await super._call(model_inputs));\n  }\n}\n\n/**\n * Hubert Model with a sequence classification head on top (a linear layer over the pooled output) for tasks like SUPERB Keyword Spotting.\n */\nexport class HubertForSequenceClassification extends Wav2Vec2PreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n   */\n  async _call(model_inputs) {\n    return new SequenceClassifierOutput(await super._call(model_inputs));\n  }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// WavLM models\n/**\n * An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained models.\n */\nexport class WavLMPreTrainedModel extends PreTrainedModel {}\n;\n\n/**\n * The bare WavLM Model transformer outputting raw hidden-states without any specific head on top.\n * \n * **Example:** Load and run a `WavLMModel` for feature extraction.\n * \n * ```javascript\n * import { AutoProcessor, AutoModel, read_audio } from '@xenova/transformers';\n * \n * // Read and preprocess audio\n * const processor = await AutoProcessor.from_pretrained('Xenova/wavlm-base');\n * const audio = await read_audio('https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/jfk.wav', 16000);\n * const inputs = await processor(audio);\n * \n * // Run model with inputs\n * const model = await AutoModel.from_pretrained('Xenova/wavlm-base');\n * const output = await model(inputs);\n * // {\n * //   last_hidden_state: Tensor {\n * //     dims: [ 1, 549, 768 ],\n * //     type: 'float32',\n * //     data: Float32Array(421632) [-0.349443256855011, -0.39341306686401367,  0.022836603224277496, ...],\n * //     size: 421632\n * //   }\n * // }\n * ```\n */\nexport class WavLMModel extends WavLMPreTrainedModel {}\n\n/**\n * WavLM Model with a `language modeling` head on top for Connectionist Temporal Classification (CTC).\n */\nexport class WavLMForCTC extends WavLMPreTrainedModel {\n  /**\n   * @param {Object} model_inputs\n   * @param {Tensor} model_inputs.input_values Float values of input raw speech waveform.\n   * @param {Tensor} model_inputs.attention_mask Mask to avoid performing convolution and attention on padding token indices. Mask values selected in [0, 1]\n   */\n  async _call(model_inputs) {\n    return new CausalLMOutput(await super._call(model_inputs));\n  }\n}\n\n/**\n * WavLM Model with a sequence classification head on top (a linear layer over the pooled output).\n */\nexport class WavLMForSequenceClassification extends WavLMPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n   */\n  async _call(model_inputs) {\n    return new SequenceClassifierOutput(await super._call(model_inputs));\n  }\n}\n\n/**\n * WavLM Model with an XVector feature extraction head on top for tasks like Speaker Verification.\n * \n * **Example:** Extract speaker embeddings with `WavLMForXVector`.\n * ```javascript\n * import { AutoProcessor, AutoModel, read_audio } from '@xenova/transformers';\n * \n * // Read and preprocess audio\n * const processor = await AutoProcessor.from_pretrained('Xenova/wavlm-base-plus-sv');\n * const url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/jfk.wav';\n * const audio = await read_audio(url, 16000);\n * const inputs = await processor(audio);\n * \n * // Run model with inputs\n * const model = await AutoModel.from_pretrained('Xenova/wavlm-base-plus-sv');\n * const outputs = await model(inputs);\n * // {\n * //   logits: Tensor {\n * //     dims: [ 1, 512 ],\n * //     type: 'float32',\n * //     data: Float32Array(512) [0.5847219228744507, ...],\n * //     size: 512\n * //   },\n * //   embeddings: Tensor {\n * //     dims: [ 1, 512 ],\n * //     type: 'float32',\n * //     data: Float32Array(512) [-0.09079201519489288, ...],\n * //     size: 512\n * //   }\n * // }\n * ```\n */\nexport class WavLMForXVector extends WavLMPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<XVectorOutput>} An object containing the model's output logits and speaker embeddings.\n   */\n  async _call(model_inputs) {\n    return new XVectorOutput(await super._call(model_inputs));\n  }\n}\n\n/**\n * WavLM Model with a frame classification head on top for tasks like Speaker Diarization.\n * \n * **Example:** Perform speaker diarization with `WavLMForAudioFrameClassification`.\n * ```javascript\n * import { AutoProcessor, AutoModelForAudioFrameClassification, read_audio } from '@xenova/transformers';\n * \n * // Read and preprocess audio\n * const processor = await AutoProcessor.from_pretrained('Xenova/wavlm-base-plus-sd');\n * const url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/jfk.wav';\n * const audio = await read_audio(url, 16000);\n * const inputs = await processor(audio);\n * \n * // Run model with inputs\n * const model = await AutoModelForAudioFrameClassification.from_pretrained('Xenova/wavlm-base-plus-sd');\n * const { logits } = await model(inputs);\n * // {\n * //   logits: Tensor {\n * //     dims: [ 1, 549, 2 ],  // [batch_size, num_frames, num_speakers]\n * //     type: 'float32',\n * //     data: Float32Array(1098) [-3.5301010608673096, ...],\n * //     size: 1098\n * //   }\n * // }\n * \n * const labels = logits[0].sigmoid().tolist().map(\n *     frames => frames.map(speaker => speaker > 0.5 ? 1 : 0)\n * );\n * console.log(labels); // labels is a one-hot array of shape (num_frames, num_speakers)\n * // [\n * //     [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0],\n * //     [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0],\n * //     [0, 0], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1],\n * //     ...\n * // ]\n * ```\n */\nexport class WavLMForAudioFrameClassification extends WavLMPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for sequence classification.\n   */\n  async _call(model_inputs) {\n    return new TokenClassifierOutput(await super._call(model_inputs));\n  }\n}\n\n//////////////////////////////////////////////////\n// SpeechT5 models\n/**\n * An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained models.\n */\nexport class SpeechT5PreTrainedModel extends PreTrainedModel {}\n;\n\n/**\n * The bare SpeechT5 Encoder-Decoder Model outputting raw hidden-states without any specific pre- or post-nets.\n */\nexport class SpeechT5Model extends SpeechT5PreTrainedModel {}\n;\n\n/**\n * SpeechT5 Model with a speech encoder and a text decoder.\n * \n * **Example:** Generate speech from text with `SpeechT5ForSpeechToText`.\n * ```javascript\n * import { AutoTokenizer, AutoProcessor, SpeechT5ForTextToSpeech, SpeechT5HifiGan, Tensor } from '@xenova/transformers';\n * \n * // Load the tokenizer and processor\n * const tokenizer = await AutoTokenizer.from_pretrained('Xenova/speecht5_tts');\n * const processor = await AutoProcessor.from_pretrained('Xenova/speecht5_tts');\n * \n * // Load the models\n * // NOTE: We use the unquantized versions as they are more accurate\n * const model = await SpeechT5ForTextToSpeech.from_pretrained('Xenova/speecht5_tts', { quantized: false });\n * const vocoder = await SpeechT5HifiGan.from_pretrained('Xenova/speecht5_hifigan', { quantized: false });\n * \n * // Load speaker embeddings from URL\n * const speaker_embeddings_data = new Float32Array(\n *     await (await fetch('https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/speaker_embeddings.bin')).arrayBuffer()\n * );\n * const speaker_embeddings = new Tensor(\n *     'float32',\n *     speaker_embeddings_data,\n *     [1, speaker_embeddings_data.length]\n * )\n * \n * // Run tokenization\n * const { input_ids } = tokenizer('Hello, my dog is cute');\n * \n * // Generate waveform\n * const { waveform } = await model.generate_speech(input_ids, speaker_embeddings, { vocoder });\n * console.log(waveform)\n * // Tensor {\n * //   dims: [ 26112 ],\n * //   type: 'float32',\n * //   size: 26112,\n * //   data: Float32Array(26112) [ -0.00043630177970044315, -0.00018082228780258447, ... ],\n * // }\n * ```\n */\nexport class SpeechT5ForSpeechToText extends SpeechT5PreTrainedModel {}\n\n/**\n * SpeechT5 Model with a text encoder and a speech decoder.\n */\nexport class SpeechT5ForTextToSpeech extends SpeechT5PreTrainedModel {\n  /**\n   * Creates a new instance of the `SpeechT5ForTextToSpeech` class.\n   * @param {Object} config The model configuration.\n   * @param {any} session session for the model.\n   * @param {any} decoder_merged_session session for the decoder.\n   * @param {GenerationConfig} generation_config The generation configuration.\n   */\n  constructor(config, session, decoder_merged_session, generation_config) {\n    super(config, session);\n    this.decoder_merged_session = decoder_merged_session;\n    this.generation_config = generation_config;\n    this.num_decoder_layers = this.config.decoder_layers;\n    this.num_decoder_heads = this.config.decoder_attention_heads;\n    this.decoder_dim_kv = this.config.hidden_size / this.num_decoder_heads;\n    this.num_encoder_layers = this.config.encoder_layers;\n    this.num_encoder_heads = this.config.encoder_attention_heads;\n    this.encoder_dim_kv = this.config.hidden_size / this.num_encoder_heads;\n  }\n\n  /**\n   * @typedef {Object} SpeechOutput\n   * @property {Tensor} [spectrogram] The predicted log-mel spectrogram of shape\n   * `(output_sequence_length, config.num_mel_bins)`. Returned when no `vocoder` is provided\n   * @property {Tensor} [waveform] The predicted waveform of shape `(num_frames,)`. Returned when a `vocoder` is provided.\n   * @property {Tensor} [cross_attentions] The outputs of the decoder's cross-attention layers of shape\n   * `(config.decoder_layers, config.decoder_attention_heads, output_sequence_length, input_sequence_length)`. returned when `output_cross_attentions` is `true`.\n   */\n\n  /**\n   * Converts a sequence of input tokens into a sequence of mel spectrograms, which are subsequently turned into a speech waveform using a vocoder.\n   * @param {Tensor} input_values Indices of input sequence tokens in the vocabulary.\n   * @param {Tensor} speaker_embeddings Tensor containing the speaker embeddings.\n   * @param {Object} options Optional parameters for generating speech.\n   * @param {number} [options.threshold=0.5] The generated sequence ends when the predicted stop token probability exceeds this value.\n   * @param {number} [options.minlenratio=0.0] Used to calculate the minimum required length for the output sequence.\n   * @param {number} [options.maxlenratio=20.0] Used to calculate the maximum allowed length for the output sequence.\n   * @param {Object} [options.vocoder=null] The vocoder that converts the mel spectrogram into a speech waveform. If `null`, the output is the mel spectrogram.\n   * @param {boolean} [options.output_cross_attentions=false] Whether or not to return the attentions tensors of the decoder's cross-attention layers.\n   * @returns {Promise<SpeechOutput>} A promise which resolves to an object containing the spectrogram, waveform, and cross-attention tensors.\n   */\n  async generate_speech(input_values, speaker_embeddings, {\n    threshold = 0.5,\n    minlenratio = 0.0,\n    maxlenratio = 20.0,\n    vocoder = null\n    // output_cross_attentions = false, // TODO add\n  } = {}) {\n    const model_inputs = {\n      input_ids: input_values\n    };\n    const {\n      encoder_outputs,\n      encoder_attention_mask\n    } = await encoderForward(this, model_inputs);\n    const r = encoder_outputs.dims[1] / this.config.reduction_factor;\n    const maxlen = Math.floor(r * maxlenratio);\n    const minlen = Math.floor(r * minlenratio);\n    const num_mel_bins = this.config.num_mel_bins;\n    let spectrogramParts = [];\n    let past_key_values = null;\n    let decoder_outputs = null;\n    let idx = 0;\n    while (true) {\n      ++idx;\n      const use_cache_branch = boolTensor(!!decoder_outputs);\n      let output_sequence;\n      if (decoder_outputs) {\n        output_sequence = decoder_outputs.output_sequence_out;\n      } else {\n        output_sequence = new Tensor('float32', new Float32Array(num_mel_bins), [1, 1, num_mel_bins]);\n      }\n      let decoderFeeds = {\n        use_cache_branch,\n        output_sequence,\n        encoder_attention_mask: encoder_attention_mask,\n        speaker_embeddings: speaker_embeddings,\n        encoder_hidden_states: encoder_outputs\n      };\n      this.addPastKeyValues(decoderFeeds, past_key_values);\n      decoder_outputs = await sessionRun(this.decoder_merged_session, decoderFeeds);\n      past_key_values = this.getPastKeyValues(decoder_outputs, past_key_values);\n      const {\n        prob,\n        spectrum\n      } = decoder_outputs;\n      spectrogramParts.push(spectrum);\n      if (idx >= minlen && (\n      // Finished when stop token or maximum length is reached.\n      Array.from(prob.data).filter(p => p >= threshold).length > 0 || idx >= maxlen)) {\n        break;\n      }\n    }\n    const spectrogram = cat(spectrogramParts);\n    const {\n      waveform\n    } = await sessionRun(vocoder.session, {\n      spectrogram\n    });\n    return {\n      spectrogram,\n      waveform\n      // cross_attentions: null, // TODO add\n    };\n  }\n}\n\n/**\n * HiFi-GAN vocoder.\n * \n * See [SpeechT5ForSpeechToText](./models#module_models.SpeechT5ForSpeechToText) for example usage.\n */\nexport class SpeechT5HifiGan extends PreTrainedModel {\n  main_input_name = 'spectrogram';\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// TrOCR models\nexport class TrOCRPreTrainedModel extends PreTrainedModel {\n  /**\n   * Creates a new instance of the `TrOCRPreTrainedModel` class.\n   * @param {Object} config The configuration of the model.\n   * @param {any} session The ONNX session containing the model weights.\n   * @param {GenerationConfig} generation_config The generation configuration.\n   */\n  constructor(config, session, generation_config) {\n    super(config, session);\n    this.generation_config = generation_config;\n\n    // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n    this.config.pad_token_id = this.config.eos_token_id;\n    this.num_encoder_layers = this.num_decoder_layers = this.config.decoder_layers;\n    this.num_encoder_heads = this.num_decoder_heads = this.config.decoder_attention_heads;\n    this.encoder_dim_kv = this.decoder_dim_kv = this.config.d_model / this.num_decoder_heads;\n  }\n}\n\n/**\n * The TrOCR Decoder with a language modeling head.\n */\nexport class TrOCRForCausalLM extends TrOCRPreTrainedModel {}\n\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// Mistral models\n/**\n * The bare Mistral Model outputting raw hidden-states without any specific head on top.\n */\nexport class MistralPreTrainedModel extends PreTrainedModel {\n  /**\n   * Creates a new instance of the `MistralPreTrainedModel` class.\n   * @param {Object} config The configuration of the model.\n   * @param {any} session The ONNX session containing the model weights.\n   * @param {GenerationConfig} generation_config The generation configuration.\n   */\n  constructor(config, session, generation_config) {\n    super(config, session);\n    this.generation_config = generation_config;\n\n    // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n    this.config.pad_token_id = this.config.eos_token_id;\n    this.num_heads = this.config.num_key_value_heads;\n    this.num_layers = this.config.num_hidden_layers;\n    this.dim_kv = this.config.hidden_size / this.config.num_attention_heads;\n  }\n}\nexport class MistralModel extends MistralPreTrainedModel {}\nexport class MistralForCausalLM extends MistralPreTrainedModel {}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// Starcoder2 models\n/**\n * The bare Starcoder2 Model outputting raw hidden-states without any specific head on top.\n */\nexport class Starcoder2PreTrainedModel extends PreTrainedModel {\n  /**\n   * Creates a new instance of the `Starcoder2PreTrainedModel` class.\n   * @param {Object} config The configuration of the model.\n   * @param {any} session The ONNX session containing the model weights.\n   * @param {GenerationConfig} generation_config The generation configuration.\n   */\n  constructor(config, session, generation_config) {\n    super(config, session);\n    this.generation_config = generation_config;\n\n    // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n    this.config.pad_token_id = this.config.eos_token_id;\n    this.num_heads = this.config.num_key_value_heads;\n    this.num_layers = this.config.num_hidden_layers;\n    this.dim_kv = this.config.hidden_size / this.config.num_attention_heads;\n  }\n}\nexport class Starcoder2Model extends Starcoder2PreTrainedModel {}\nexport class Starcoder2ForCausalLM extends Starcoder2PreTrainedModel {}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// Falcon models\n/**\n * The bare Falcon Model outputting raw hidden-states without any specific head on top.\n */\nexport class FalconPreTrainedModel extends PreTrainedModel {\n  /**\n   * Creates a new instance of the `FalconPreTrainedModel` class.\n   * @param {Object} config The configuration of the model.\n   * @param {any} session The ONNX session containing the model weights.\n   * @param {GenerationConfig} generation_config The generation configuration.\n   */\n  constructor(config, session, generation_config) {\n    super(config, session);\n    this.generation_config = generation_config;\n\n    // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n    this.config.pad_token_id = this.config.eos_token_id;\n    this.num_heads = this.config.num_attention_heads;\n    this.num_layers = this.config.num_hidden_layers;\n    this.dim_kv = this.config.hidden_size / this.config.num_attention_heads;\n  }\n}\nexport class FalconModel extends FalconPreTrainedModel {}\nexport class FalconForCausalLM extends FalconPreTrainedModel {}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// CLAP models\nexport class ClapPreTrainedModel extends PreTrainedModel {}\nexport class ClapModel extends ClapPreTrainedModel {}\n\n/**\n * CLAP Text Model with a projection layer on top (a linear layer on top of the pooled output).\n * \n * **Example:** Compute text embeddings with `ClapTextModelWithProjection`.\n * \n * ```javascript\n * import { AutoTokenizer, ClapTextModelWithProjection } from '@xenova/transformers';\n * \n * // Load tokenizer and text model\n * const tokenizer = await AutoTokenizer.from_pretrained('Xenova/clap-htsat-unfused');\n * const text_model = await ClapTextModelWithProjection.from_pretrained('Xenova/clap-htsat-unfused');\n * \n * // Run tokenization\n * const texts = ['a sound of a cat', 'a sound of a dog'];\n * const text_inputs = tokenizer(texts, { padding: true, truncation: true });\n * \n * // Compute embeddings\n * const { text_embeds } = await text_model(text_inputs);\n * // Tensor {\n * //   dims: [ 2, 512 ],\n * //   type: 'float32',\n * //   data: Float32Array(1024) [ ... ],\n * //   size: 1024\n * // }\n * ```\n */\nexport class ClapTextModelWithProjection extends ClapPreTrainedModel {\n  /** @type {PreTrainedModel.from_pretrained} */\n  static async from_pretrained(pretrained_model_name_or_path, options = {}) {\n    // Update default model file name if not provided\n    options.model_file_name ??= 'text_model';\n    return super.from_pretrained(pretrained_model_name_or_path, options);\n  }\n}\n\n/**\n * CLAP Audio Model with a projection layer on top (a linear layer on top of the pooled output).\n * \n * **Example:** Compute audio embeddings with `ClapAudioModelWithProjection`.\n * \n * ```javascript\n * import { AutoProcessor, ClapAudioModelWithProjection, read_audio } from '@xenova/transformers';\n * \n * // Load processor and audio model\n * const processor = await AutoProcessor.from_pretrained('Xenova/clap-htsat-unfused');\n * const audio_model = await ClapAudioModelWithProjection.from_pretrained('Xenova/clap-htsat-unfused');\n * \n * // Read audio and run processor\n * const audio = await read_audio('https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/cat_meow.wav');\n * const audio_inputs = await processor(audio);\n * \n * // Compute embeddings\n * const { audio_embeds } = await audio_model(audio_inputs);\n * // Tensor {\n * //   dims: [ 1, 512 ],\n * //   type: 'float32',\n * //   data: Float32Array(512) [ ... ],\n * //   size: 512\n * // }\n * ```\n */\nexport class ClapAudioModelWithProjection extends ClapPreTrainedModel {\n  /** @type {PreTrainedModel.from_pretrained} */\n  static async from_pretrained(pretrained_model_name_or_path, options = {}) {\n    // Update default model file name if not provided\n    options.model_file_name ??= 'audio_model';\n    return super.from_pretrained(pretrained_model_name_or_path, options);\n  }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// VITS models\nexport class VitsPreTrainedModel extends PreTrainedModel {}\n\n/**\n * The complete VITS model, for text-to-speech synthesis.\n * \n * **Example:** Generate speech from text with `VitsModel`.\n * ```javascript\n * import { AutoTokenizer, VitsModel } from '@xenova/transformers';\n * \n * // Load the tokenizer and model\n * const tokenizer = await AutoTokenizer.from_pretrained('Xenova/mms-tts-eng');\n * const model = await VitsModel.from_pretrained('Xenova/mms-tts-eng');\n * \n * // Run tokenization\n * const inputs = tokenizer('I love transformers');\n * \n * // Generate waveform\n * const { waveform } = await model(inputs);\n * // Tensor {\n * //   dims: [ 1, 35328 ],\n * //   type: 'float32',\n * //   data: Float32Array(35328) [ ... ],\n * //   size: 35328,\n * // }\n * ```\n */\nexport class VitsModel extends VitsPreTrainedModel {\n  /**\n   * Calls the model on new inputs.\n   * @param {Object} model_inputs The inputs to the model.\n   * @returns {Promise<VitsModelOutput>} The outputs for the VITS model.\n   */\n  async _call(model_inputs) {\n    return new VitsModelOutput(await super._call(model_inputs));\n  }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// Segformer models\nexport class SegformerPreTrainedModel extends PreTrainedModel {}\n\n/**\n * The bare SegFormer encoder (Mix-Transformer) outputting raw hidden-states without any specific head on top.\n */\nexport class SegformerModel extends SegformerPreTrainedModel {}\n\n/**\n * SegFormer Model transformer with an image classification head on top (a linear layer on top of the final hidden states) e.g. for ImageNet.\n */\nexport class SegformerForImageClassification extends SegformerPreTrainedModel {}\n\n/**\n * SegFormer Model transformer with an all-MLP decode head on top e.g. for ADE20k, CityScapes.\n */\nexport class SegformerForSemanticSegmentation extends SegformerPreTrainedModel {}\n\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// StableLm models\nexport class StableLmPreTrainedModel extends PreTrainedModel {\n  /**\n   * Creates a new instance of the `StableLmPreTrainedModel` class.\n   * @param {Object} config The configuration of the model.\n   * @param {any} session The ONNX session containing the model weights.\n   * @param {GenerationConfig} generation_config The generation configuration.\n   */\n  constructor(config, session, generation_config) {\n    super(config, session);\n    this.generation_config = generation_config;\n\n    // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n    this.config.pad_token_id = this.config.eos_token_id;\n    this.num_heads = this.config.num_attention_heads;\n    this.num_layers = this.config.num_hidden_layers;\n    this.dim_kv = this.config.hidden_size / this.num_heads;\n  }\n}\n\n/**\n * The bare StableLm Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class StableLmModel extends StableLmPreTrainedModel {}\n\n/**\n * StableLm Model with a `language modeling` head on top for Causal Language Modeling (with past).\n */\nexport class StableLmForCausalLM extends StableLmPreTrainedModel {}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\nexport class EfficientNetPreTrainedModel extends PreTrainedModel {}\n\n/**\n * The bare EfficientNet model outputting raw features without any specific head on top.\n */\nexport class EfficientNetModel extends EfficientNetPreTrainedModel {}\n\n/**\n * EfficientNet Model with an image classification head on top (a linear layer on top of the pooled features).\n */\nexport class EfficientNetForImageClassification extends EfficientNetPreTrainedModel {\n  /**\n   * @param {any} model_inputs\n   */\n  async _call(model_inputs) {\n    return new SequenceClassifierOutput(await super._call(model_inputs));\n  }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// AutoModels, used to simplify construction of PreTrainedModels\n// (uses config to instantiate correct class)\n\n/**\n * Base class of all AutoModels. Contains the `from_pretrained` function\n * which is used to instantiate pretrained models.\n */\nexport class PretrainedMixin {\n  /**\n   * Mapping from model type to model class.\n   * @type {Map<string, Object>[]}\n   */\n  static MODEL_CLASS_MAPPINGS = null;\n\n  /**\n   * Whether to attempt to instantiate the base class (`PretrainedModel`) if \n   * the model type is not found in the mapping.\n   */\n  static BASE_IF_FAIL = false;\n\n  /** @type {PreTrainedModel.from_pretrained} */\n  static async from_pretrained(pretrained_model_name_or_path, {\n    quantized = true,\n    progress_callback = null,\n    config = null,\n    cache_dir = null,\n    local_files_only = false,\n    revision = 'main',\n    model_file_name = null\n  } = {}) {\n    let options = {\n      quantized,\n      progress_callback,\n      config,\n      cache_dir,\n      local_files_only,\n      revision,\n      model_file_name\n    };\n    config = await AutoConfig.from_pretrained(pretrained_model_name_or_path, options);\n    if (!options.config) {\n      // If no config was passed, reuse this config for future processing\n      options.config = config;\n    }\n    if (!this.MODEL_CLASS_MAPPINGS) {\n      throw new Error(\"`MODEL_CLASS_MAPPINGS` not implemented for this type of `AutoClass`: \" + this.name);\n    }\n    for (let MODEL_CLASS_MAPPING of this.MODEL_CLASS_MAPPINGS) {\n      const modelInfo = MODEL_CLASS_MAPPING.get(config.model_type);\n      if (!modelInfo) {\n        continue; // Item not found in this mapping\n      }\n      return await modelInfo[1].from_pretrained(pretrained_model_name_or_path, options);\n    }\n    if (this.BASE_IF_FAIL) {\n      console.warn(`Unknown model class \"${config.model_type}\", attempting to construct from base class.`);\n      return await PreTrainedModel.from_pretrained(pretrained_model_name_or_path, options);\n    } else {\n      throw Error(`Unsupported model type: ${config.model_type}`);\n    }\n  }\n}\nconst MODEL_MAPPING_NAMES_ENCODER_ONLY = new Map([['bert', ['BertModel', BertModel]], ['nomic_bert', ['NomicBertModel', NomicBertModel]], ['roformer', ['RoFormerModel', RoFormerModel]], ['electra', ['ElectraModel', ElectraModel]], ['esm', ['EsmModel', EsmModel]], ['convbert', ['ConvBertModel', ConvBertModel]], ['camembert', ['CamembertModel', CamembertModel]], ['deberta', ['DebertaModel', DebertaModel]], ['deberta-v2', ['DebertaV2Model', DebertaV2Model]], ['mpnet', ['MPNetModel', MPNetModel]], ['albert', ['AlbertModel', AlbertModel]], ['distilbert', ['DistilBertModel', DistilBertModel]], ['roberta', ['RobertaModel', RobertaModel]], ['xlm', ['XLMModel', XLMModel]], ['xlm-roberta', ['XLMRobertaModel', XLMRobertaModel]], ['clap', ['ClapModel', ClapModel]], ['clip', ['CLIPModel', CLIPModel]], ['clipseg', ['CLIPSegModel', CLIPSegModel]], ['chinese_clip', ['ChineseCLIPModel', ChineseCLIPModel]], ['siglip', ['SiglipModel', SiglipModel]], ['mobilebert', ['MobileBertModel', MobileBertModel]], ['squeezebert', ['SqueezeBertModel', SqueezeBertModel]], ['wav2vec2', ['Wav2Vec2Model', Wav2Vec2Model]], ['wav2vec2-bert', ['Wav2Vec2BertModel', Wav2Vec2BertModel]], ['unispeech', ['UniSpeechModel', UniSpeechModel]], ['unispeech-sat', ['UniSpeechSatModel', UniSpeechSatModel]], ['hubert', ['HubertModel', HubertModel]], ['wavlm', ['WavLMModel', WavLMModel]], ['audio-spectrogram-transformer', ['ASTModel', ASTModel]], ['vits', ['VitsModel', VitsModel]], ['detr', ['DetrModel', DetrModel]], ['table-transformer', ['TableTransformerModel', TableTransformerModel]], ['vit', ['ViTModel', ViTModel]], ['fastvit', ['FastViTModel', FastViTModel]], ['mobilevit', ['MobileViTModel', MobileViTModel]], ['mobilevitv2', ['MobileViTV2Model', MobileViTV2Model]], ['owlvit', ['OwlViTModel', OwlViTModel]], ['owlv2', ['Owlv2Model', Owlv2Model]], ['beit', ['BeitModel', BeitModel]], ['deit', ['DeiTModel', DeiTModel]], ['convnext', ['ConvNextModel', ConvNextModel]], ['convnextv2', ['ConvNextV2Model', ConvNextV2Model]], ['dinov2', ['Dinov2Model', Dinov2Model]], ['resnet', ['ResNetModel', ResNetModel]], ['swin', ['SwinModel', SwinModel]], ['swin2sr', ['Swin2SRModel', Swin2SRModel]], ['donut-swin', ['DonutSwinModel', DonutSwinModel]], ['yolos', ['YolosModel', YolosModel]], ['dpt', ['DPTModel', DPTModel]], ['glpn', ['GLPNModel', GLPNModel]], ['hifigan', ['SpeechT5HifiGan', SpeechT5HifiGan]], ['efficientnet', ['EfficientNetModel', EfficientNetModel]]]);\nconst MODEL_MAPPING_NAMES_ENCODER_DECODER = new Map([['t5', ['T5Model', T5Model]], ['longt5', ['LongT5Model', LongT5Model]], ['mt5', ['MT5Model', MT5Model]], ['bart', ['BartModel', BartModel]], ['mbart', ['MBartModel', MBartModel]], ['marian', ['MarianModel', MarianModel]], ['whisper', ['WhisperModel', WhisperModel]], ['m2m_100', ['M2M100Model', M2M100Model]], ['blenderbot', ['BlenderbotModel', BlenderbotModel]], ['blenderbot-small', ['BlenderbotSmallModel', BlenderbotSmallModel]]]);\nconst MODEL_MAPPING_NAMES_DECODER_ONLY = new Map([['bloom', ['BloomModel', BloomModel]], ['gpt2', ['GPT2Model', GPT2Model]], ['gptj', ['GPTJModel', GPTJModel]], ['gpt_bigcode', ['GPTBigCodeModel', GPTBigCodeModel]], ['gpt_neo', ['GPTNeoModel', GPTNeoModel]], ['gpt_neox', ['GPTNeoXModel', GPTNeoXModel]], ['codegen', ['CodeGenModel', CodeGenModel]], ['llama', ['LlamaModel', LlamaModel]], ['qwen2', ['Qwen2Model', Qwen2Model]], ['phi', ['PhiModel', PhiModel]], ['mpt', ['MptModel', MptModel]], ['opt', ['OPTModel', OPTModel]], ['mistral', ['MistralModel', MistralModel]], ['starcoder2', ['Starcoder2Model', Starcoder2Model]], ['falcon', ['FalconModel', FalconModel]]]);\nconst MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING_NAMES = new Map([['speecht5', ['SpeechT5ForSpeechToText', SpeechT5ForSpeechToText]], ['whisper', ['WhisperForConditionalGeneration', WhisperForConditionalGeneration]]]);\nconst MODEL_FOR_TEXT_TO_SPECTROGRAM_MAPPING_NAMES = new Map([['speecht5', ['SpeechT5ForTextToSpeech', SpeechT5ForTextToSpeech]]]);\nconst MODEL_FOR_TEXT_TO_WAVEFORM_MAPPING_NAMES = new Map([['vits', ['VitsModel', VitsModel]]]);\nconst MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES = new Map([['bert', ['BertForSequenceClassification', BertForSequenceClassification]], ['roformer', ['RoFormerForSequenceClassification', RoFormerForSequenceClassification]], ['electra', ['ElectraForSequenceClassification', ElectraForSequenceClassification]], ['esm', ['EsmForSequenceClassification', EsmForSequenceClassification]], ['convbert', ['ConvBertForSequenceClassification', ConvBertForSequenceClassification]], ['camembert', ['CamembertForSequenceClassification', CamembertForSequenceClassification]], ['deberta', ['DebertaForSequenceClassification', DebertaForSequenceClassification]], ['deberta-v2', ['DebertaV2ForSequenceClassification', DebertaV2ForSequenceClassification]], ['mpnet', ['MPNetForSequenceClassification', MPNetForSequenceClassification]], ['albert', ['AlbertForSequenceClassification', AlbertForSequenceClassification]], ['distilbert', ['DistilBertForSequenceClassification', DistilBertForSequenceClassification]], ['roberta', ['RobertaForSequenceClassification', RobertaForSequenceClassification]], ['xlm', ['XLMForSequenceClassification', XLMForSequenceClassification]], ['xlm-roberta', ['XLMRobertaForSequenceClassification', XLMRobertaForSequenceClassification]], ['bart', ['BartForSequenceClassification', BartForSequenceClassification]], ['mbart', ['MBartForSequenceClassification', MBartForSequenceClassification]], ['mobilebert', ['MobileBertForSequenceClassification', MobileBertForSequenceClassification]], ['squeezebert', ['SqueezeBertForSequenceClassification', SqueezeBertForSequenceClassification]]]);\nconst MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING_NAMES = new Map([['bert', ['BertForTokenClassification', BertForTokenClassification]], ['roformer', ['RoFormerForTokenClassification', RoFormerForTokenClassification]], ['electra', ['ElectraForTokenClassification', ElectraForTokenClassification]], ['esm', ['EsmForTokenClassification', EsmForTokenClassification]], ['convbert', ['ConvBertForTokenClassification', ConvBertForTokenClassification]], ['camembert', ['CamembertForTokenClassification', CamembertForTokenClassification]], ['deberta', ['DebertaForTokenClassification', DebertaForTokenClassification]], ['deberta-v2', ['DebertaV2ForTokenClassification', DebertaV2ForTokenClassification]], ['mpnet', ['MPNetForTokenClassification', MPNetForTokenClassification]], ['distilbert', ['DistilBertForTokenClassification', DistilBertForTokenClassification]], ['roberta', ['RobertaForTokenClassification', RobertaForTokenClassification]], ['xlm', ['XLMForTokenClassification', XLMForTokenClassification]], ['xlm-roberta', ['XLMRobertaForTokenClassification', XLMRobertaForTokenClassification]]]);\nconst MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES = new Map([['t5', ['T5ForConditionalGeneration', T5ForConditionalGeneration]], ['longt5', ['LongT5ForConditionalGeneration', LongT5ForConditionalGeneration]], ['mt5', ['MT5ForConditionalGeneration', MT5ForConditionalGeneration]], ['bart', ['BartForConditionalGeneration', BartForConditionalGeneration]], ['mbart', ['MBartForConditionalGeneration', MBartForConditionalGeneration]], ['marian', ['MarianMTModel', MarianMTModel]], ['m2m_100', ['M2M100ForConditionalGeneration', M2M100ForConditionalGeneration]], ['blenderbot', ['BlenderbotForConditionalGeneration', BlenderbotForConditionalGeneration]], ['blenderbot-small', ['BlenderbotSmallForConditionalGeneration', BlenderbotSmallForConditionalGeneration]]]);\nconst MODEL_WITH_LM_HEAD_MAPPING_NAMES = new Map([['bloom', ['BloomForCausalLM', BloomForCausalLM]], ['gpt2', ['GPT2LMHeadModel', GPT2LMHeadModel]], ['gptj', ['GPTJForCausalLM', GPTJForCausalLM]], ['gpt_bigcode', ['GPTBigCodeForCausalLM', GPTBigCodeForCausalLM]], ['gpt_neo', ['GPTNeoForCausalLM', GPTNeoForCausalLM]], ['gpt_neox', ['GPTNeoXForCausalLM', GPTNeoXForCausalLM]], ['codegen', ['CodeGenForCausalLM', CodeGenForCausalLM]], ['llama', ['LlamaForCausalLM', LlamaForCausalLM]], ['qwen2', ['Qwen2ForCausalLM', Qwen2ForCausalLM]], ['phi', ['PhiForCausalLM', PhiForCausalLM]], ['mpt', ['MptForCausalLM', MptForCausalLM]], ['opt', ['OPTForCausalLM', OPTForCausalLM]], ['mbart', ['MBartForCausalLM', MBartForCausalLM]], ['mistral', ['MistralForCausalLM', MistralForCausalLM]], ['starcoder2', ['Starcoder2ForCausalLM', Starcoder2ForCausalLM]], ['falcon', ['FalconForCausalLM', FalconForCausalLM]], ['trocr', ['TrOCRForCausalLM', TrOCRForCausalLM]], ['stablelm', ['StableLmForCausalLM', StableLmForCausalLM]]]);\nconst MODEL_FOR_MASKED_LM_MAPPING_NAMES = new Map([['bert', ['BertForMaskedLM', BertForMaskedLM]], ['roformer', ['RoFormerForMaskedLM', RoFormerForMaskedLM]], ['electra', ['ElectraForMaskedLM', ElectraForMaskedLM]], ['esm', ['EsmForMaskedLM', EsmForMaskedLM]], ['convbert', ['ConvBertForMaskedLM', ConvBertForMaskedLM]], ['camembert', ['CamembertForMaskedLM', CamembertForMaskedLM]], ['deberta', ['DebertaForMaskedLM', DebertaForMaskedLM]], ['deberta-v2', ['DebertaV2ForMaskedLM', DebertaV2ForMaskedLM]], ['mpnet', ['MPNetForMaskedLM', MPNetForMaskedLM]], ['albert', ['AlbertForMaskedLM', AlbertForMaskedLM]], ['distilbert', ['DistilBertForMaskedLM', DistilBertForMaskedLM]], ['roberta', ['RobertaForMaskedLM', RobertaForMaskedLM]], ['xlm', ['XLMWithLMHeadModel', XLMWithLMHeadModel]], ['xlm-roberta', ['XLMRobertaForMaskedLM', XLMRobertaForMaskedLM]], ['mobilebert', ['MobileBertForMaskedLM', MobileBertForMaskedLM]], ['squeezebert', ['SqueezeBertForMaskedLM', SqueezeBertForMaskedLM]]]);\nconst MODEL_FOR_QUESTION_ANSWERING_MAPPING_NAMES = new Map([['bert', ['BertForQuestionAnswering', BertForQuestionAnswering]], ['roformer', ['RoFormerForQuestionAnswering', RoFormerForQuestionAnswering]], ['electra', ['ElectraForQuestionAnswering', ElectraForQuestionAnswering]], ['convbert', ['ConvBertForQuestionAnswering', ConvBertForQuestionAnswering]], ['camembert', ['CamembertForQuestionAnswering', CamembertForQuestionAnswering]], ['deberta', ['DebertaForQuestionAnswering', DebertaForQuestionAnswering]], ['deberta-v2', ['DebertaV2ForQuestionAnswering', DebertaV2ForQuestionAnswering]], ['mpnet', ['MPNetForQuestionAnswering', MPNetForQuestionAnswering]], ['albert', ['AlbertForQuestionAnswering', AlbertForQuestionAnswering]], ['distilbert', ['DistilBertForQuestionAnswering', DistilBertForQuestionAnswering]], ['roberta', ['RobertaForQuestionAnswering', RobertaForQuestionAnswering]], ['xlm', ['XLMForQuestionAnswering', XLMForQuestionAnswering]], ['xlm-roberta', ['XLMRobertaForQuestionAnswering', XLMRobertaForQuestionAnswering]], ['mobilebert', ['MobileBertForQuestionAnswering', MobileBertForQuestionAnswering]], ['squeezebert', ['SqueezeBertForQuestionAnswering', SqueezeBertForQuestionAnswering]]]);\nconst MODEL_FOR_VISION_2_SEQ_MAPPING_NAMES = new Map([['vision-encoder-decoder', ['VisionEncoderDecoderModel', VisionEncoderDecoderModel]]]);\nconst MODEL_FOR_DOCUMENT_QUESTION_ANSWERING_MAPPING_NAMES = new Map([['vision-encoder-decoder', ['VisionEncoderDecoderModel', VisionEncoderDecoderModel]]]);\nconst MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING_NAMES = new Map([['vit', ['ViTForImageClassification', ViTForImageClassification]], ['fastvit', ['FastViTForImageClassification', FastViTForImageClassification]], ['mobilevit', ['MobileViTForImageClassification', MobileViTForImageClassification]], ['mobilevitv2', ['MobileViTV2ForImageClassification', MobileViTV2ForImageClassification]], ['beit', ['BeitForImageClassification', BeitForImageClassification]], ['deit', ['DeiTForImageClassification', DeiTForImageClassification]], ['convnext', ['ConvNextForImageClassification', ConvNextForImageClassification]], ['convnextv2', ['ConvNextV2ForImageClassification', ConvNextV2ForImageClassification]], ['dinov2', ['Dinov2ForImageClassification', Dinov2ForImageClassification]], ['resnet', ['ResNetForImageClassification', ResNetForImageClassification]], ['swin', ['SwinForImageClassification', SwinForImageClassification]], ['segformer', ['SegformerForImageClassification', SegformerForImageClassification]], ['efficientnet', ['EfficientNetForImageClassification', EfficientNetForImageClassification]]]);\nconst MODEL_FOR_OBJECT_DETECTION_MAPPING_NAMES = new Map([['detr', ['DetrForObjectDetection', DetrForObjectDetection]], ['table-transformer', ['TableTransformerForObjectDetection', TableTransformerForObjectDetection]], ['yolos', ['YolosForObjectDetection', YolosForObjectDetection]]]);\nconst MODEL_FOR_ZERO_SHOT_OBJECT_DETECTION_MAPPING_NAMES = new Map([['owlvit', ['OwlViTForObjectDetection', OwlViTForObjectDetection]], ['owlv2', ['Owlv2ForObjectDetection', Owlv2ForObjectDetection]]]);\nconst MODEL_FOR_IMAGE_SEGMENTATION_MAPPING_NAMES = new Map([['detr', ['DetrForSegmentation', DetrForSegmentation]], ['clipseg', ['CLIPSegForImageSegmentation', CLIPSegForImageSegmentation]]]);\nconst MODEL_FOR_SEMANTIC_SEGMENTATION_MAPPING_NAMES = new Map([['segformer', ['SegformerForSemanticSegmentation', SegformerForSemanticSegmentation]]]);\nconst MODEL_FOR_MASK_GENERATION_MAPPING_NAMES = new Map([['sam', ['SamModel', SamModel]]]);\nconst MODEL_FOR_CTC_MAPPING_NAMES = new Map([['wav2vec2', ['Wav2Vec2ForCTC', Wav2Vec2ForCTC]], ['wav2vec2-bert', ['Wav2Vec2BertForCTC', Wav2Vec2BertForCTC]], ['unispeech', ['UniSpeechForCTC', UniSpeechForCTC]], ['unispeech-sat', ['UniSpeechSatForCTC', UniSpeechSatForCTC]], ['wavlm', ['WavLMForCTC', WavLMForCTC]], ['hubert', ['HubertForCTC', HubertForCTC]]]);\nconst MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING_NAMES = new Map([['wav2vec2', ['Wav2Vec2ForSequenceClassification', Wav2Vec2ForSequenceClassification]], ['wav2vec2-bert', ['Wav2Vec2BertForSequenceClassification', Wav2Vec2BertForSequenceClassification]], ['unispeech', ['UniSpeechForSequenceClassification', UniSpeechForSequenceClassification]], ['unispeech-sat', ['UniSpeechSatForSequenceClassification', UniSpeechSatForSequenceClassification]], ['wavlm', ['WavLMForSequenceClassification', WavLMForSequenceClassification]], ['hubert', ['HubertForSequenceClassification', HubertForSequenceClassification]], ['audio-spectrogram-transformer', ['ASTForAudioClassification', ASTForAudioClassification]]]);\nconst MODEL_FOR_AUDIO_XVECTOR_MAPPING_NAMES = new Map([['wavlm', ['WavLMForXVector', WavLMForXVector]]]);\nconst MODEL_FOR_AUDIO_FRAME_CLASSIFICATION_MAPPING_NAMES = new Map([['unispeech-sat', ['UniSpeechSatForAudioFrameClassification', UniSpeechSatForAudioFrameClassification]], ['wavlm', ['WavLMForAudioFrameClassification', WavLMForAudioFrameClassification]], ['wav2vec2', ['Wav2Vec2ForAudioFrameClassification', Wav2Vec2ForAudioFrameClassification]]]);\nconst MODEL_FOR_IMAGE_MATTING_MAPPING_NAMES = new Map([['vitmatte', ['VitMatteForImageMatting', VitMatteForImageMatting]]]);\nconst MODEL_FOR_IMAGE_TO_IMAGE_MAPPING_NAMES = new Map([['swin2sr', ['Swin2SRForImageSuperResolution', Swin2SRForImageSuperResolution]]]);\nconst MODEL_FOR_DEPTH_ESTIMATION_MAPPING_NAMES = new Map([['dpt', ['DPTForDepthEstimation', DPTForDepthEstimation]], ['depth_anything', ['DepthAnythingForDepthEstimation', DepthAnythingForDepthEstimation]], ['glpn', ['GLPNForDepthEstimation', GLPNForDepthEstimation]]]);\n\n// NOTE: This is custom to Transformers.js, and is necessary because certain models\n// (e.g., CLIP) are split into vision and text components\nconst MODEL_FOR_IMAGE_FEATURE_EXTRACTION_MAPPING_NAMES = new Map([['clip', ['CLIPVisionModelWithProjection', CLIPVisionModelWithProjection]], ['siglip', ['SiglipVisionModel', SiglipVisionModel]]]);\nconst MODEL_CLASS_TYPE_MAPPING = [[MODEL_MAPPING_NAMES_ENCODER_ONLY, MODEL_TYPES.EncoderOnly], [MODEL_MAPPING_NAMES_ENCODER_DECODER, MODEL_TYPES.EncoderDecoder], [MODEL_MAPPING_NAMES_DECODER_ONLY, MODEL_TYPES.DecoderOnly], [MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES, MODEL_TYPES.EncoderOnly], [MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING_NAMES, MODEL_TYPES.EncoderOnly], [MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES, MODEL_TYPES.Seq2Seq], [MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING_NAMES, MODEL_TYPES.Seq2Seq], [MODEL_WITH_LM_HEAD_MAPPING_NAMES, MODEL_TYPES.DecoderOnly], [MODEL_FOR_MASKED_LM_MAPPING_NAMES, MODEL_TYPES.EncoderOnly], [MODEL_FOR_QUESTION_ANSWERING_MAPPING_NAMES, MODEL_TYPES.EncoderOnly], [MODEL_FOR_VISION_2_SEQ_MAPPING_NAMES, MODEL_TYPES.Vision2Seq], [MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING_NAMES, MODEL_TYPES.EncoderOnly], [MODEL_FOR_IMAGE_SEGMENTATION_MAPPING_NAMES, MODEL_TYPES.EncoderOnly], [MODEL_FOR_SEMANTIC_SEGMENTATION_MAPPING_NAMES, MODEL_TYPES.EncoderOnly], [MODEL_FOR_IMAGE_MATTING_MAPPING_NAMES, MODEL_TYPES.EncoderOnly], [MODEL_FOR_IMAGE_TO_IMAGE_MAPPING_NAMES, MODEL_TYPES.EncoderOnly], [MODEL_FOR_DEPTH_ESTIMATION_MAPPING_NAMES, MODEL_TYPES.EncoderOnly], [MODEL_FOR_OBJECT_DETECTION_MAPPING_NAMES, MODEL_TYPES.EncoderOnly], [MODEL_FOR_ZERO_SHOT_OBJECT_DETECTION_MAPPING_NAMES, MODEL_TYPES.EncoderOnly], [MODEL_FOR_MASK_GENERATION_MAPPING_NAMES, MODEL_TYPES.MaskGeneration], [MODEL_FOR_CTC_MAPPING_NAMES, MODEL_TYPES.EncoderOnly], [MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING_NAMES, MODEL_TYPES.EncoderOnly], [MODEL_FOR_TEXT_TO_SPECTROGRAM_MAPPING_NAMES, MODEL_TYPES.Seq2Seq], [MODEL_FOR_TEXT_TO_WAVEFORM_MAPPING_NAMES, MODEL_TYPES.EncoderOnly], [MODEL_FOR_AUDIO_XVECTOR_MAPPING_NAMES, MODEL_TYPES.EncoderOnly], [MODEL_FOR_AUDIO_FRAME_CLASSIFICATION_MAPPING_NAMES, MODEL_TYPES.EncoderOnly],\n// Custom:\n[MODEL_FOR_IMAGE_FEATURE_EXTRACTION_MAPPING_NAMES, MODEL_TYPES.EncoderOnly]];\nfor (const [mappings, type] of MODEL_CLASS_TYPE_MAPPING) {\n  // @ts-ignore\n  for (const [name, model] of mappings.values()) {\n    MODEL_TYPE_MAPPING.set(name, type);\n    MODEL_CLASS_TO_NAME_MAPPING.set(model, name);\n    MODEL_NAME_TO_CLASS_MAPPING.set(name, model);\n  }\n}\nconst CUSTOM_MAPPING = [['CLIPTextModelWithProjection', CLIPTextModelWithProjection, MODEL_TYPES.EncoderOnly], ['SiglipTextModel', SiglipTextModel, MODEL_TYPES.EncoderOnly], ['ClapTextModelWithProjection', ClapTextModelWithProjection, MODEL_TYPES.EncoderOnly], ['ClapAudioModelWithProjection', ClapAudioModelWithProjection, MODEL_TYPES.EncoderOnly]];\nfor (const [name, model, type] of CUSTOM_MAPPING) {\n  MODEL_TYPE_MAPPING.set(name, type);\n  MODEL_CLASS_TO_NAME_MAPPING.set(model, name);\n  MODEL_NAME_TO_CLASS_MAPPING.set(name, model);\n}\n\n/**\n * Helper class which is used to instantiate pretrained models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModel.from_pretrained('bert-base-uncased');\n */\nexport class AutoModel extends PretrainedMixin {\n  /** @type {Map<string, Object>[]} */\n  // @ts-ignore\n  static MODEL_CLASS_MAPPINGS = MODEL_CLASS_TYPE_MAPPING.map(x => x[0]);\n  static BASE_IF_FAIL = true;\n}\n\n/**\n * Helper class which is used to instantiate pretrained sequence classification models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english');\n */\nexport class AutoModelForSequenceClassification extends PretrainedMixin {\n  static MODEL_CLASS_MAPPINGS = [MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES];\n}\n\n/**\n * Helper class which is used to instantiate pretrained token classification models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModelForTokenClassification.from_pretrained('Davlan/distilbert-base-multilingual-cased-ner-hrl');\n */\nexport class AutoModelForTokenClassification extends PretrainedMixin {\n  static MODEL_CLASS_MAPPINGS = [MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING_NAMES];\n}\n\n/**\n * Helper class which is used to instantiate pretrained sequence-to-sequence models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModelForSeq2SeqLM.from_pretrained('t5-small');\n */\nexport class AutoModelForSeq2SeqLM extends PretrainedMixin {\n  static MODEL_CLASS_MAPPINGS = [MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES];\n}\n\n/**\n * Helper class which is used to instantiate pretrained sequence-to-sequence speech-to-text models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModelForSpeechSeq2Seq.from_pretrained('openai/whisper-tiny.en');\n */\nexport class AutoModelForSpeechSeq2Seq extends PretrainedMixin {\n  static MODEL_CLASS_MAPPINGS = [MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING_NAMES];\n}\n\n/**\n * Helper class which is used to instantiate pretrained sequence-to-sequence text-to-spectrogram models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModelForTextToSpectrogram.from_pretrained('microsoft/speecht5_tts');\n */\nexport class AutoModelForTextToSpectrogram extends PretrainedMixin {\n  static MODEL_CLASS_MAPPINGS = [MODEL_FOR_TEXT_TO_SPECTROGRAM_MAPPING_NAMES];\n}\n\n/**\n * Helper class which is used to instantiate pretrained text-to-waveform models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModelForTextToSpectrogram.from_pretrained('facebook/mms-tts-eng');\n */\nexport class AutoModelForTextToWaveform extends PretrainedMixin {\n  static MODEL_CLASS_MAPPINGS = [MODEL_FOR_TEXT_TO_WAVEFORM_MAPPING_NAMES];\n}\n\n/**\n * Helper class which is used to instantiate pretrained causal language models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModelForCausalLM.from_pretrained('gpt2');\n */\nexport class AutoModelForCausalLM extends PretrainedMixin {\n  static MODEL_CLASS_MAPPINGS = [MODEL_WITH_LM_HEAD_MAPPING_NAMES];\n}\n\n/**\n * Helper class which is used to instantiate pretrained masked language models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModelForMaskedLM.from_pretrained('bert-base-uncased');\n */\nexport class AutoModelForMaskedLM extends PretrainedMixin {\n  static MODEL_CLASS_MAPPINGS = [MODEL_FOR_MASKED_LM_MAPPING_NAMES];\n}\n\n/**\n * Helper class which is used to instantiate pretrained question answering models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModelForQuestionAnswering.from_pretrained('distilbert-base-cased-distilled-squad');\n */\nexport class AutoModelForQuestionAnswering extends PretrainedMixin {\n  static MODEL_CLASS_MAPPINGS = [MODEL_FOR_QUESTION_ANSWERING_MAPPING_NAMES];\n}\n\n/**\n * Helper class which is used to instantiate pretrained vision-to-sequence models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModelForVision2Seq.from_pretrained('nlpconnect/vit-gpt2-image-captioning');\n */\nexport class AutoModelForVision2Seq extends PretrainedMixin {\n  static MODEL_CLASS_MAPPINGS = [MODEL_FOR_VISION_2_SEQ_MAPPING_NAMES];\n}\n\n/**\n * Helper class which is used to instantiate pretrained image classification models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModelForImageClassification.from_pretrained('google/vit-base-patch16-224');\n */\nexport class AutoModelForImageClassification extends PretrainedMixin {\n  static MODEL_CLASS_MAPPINGS = [MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING_NAMES];\n}\n\n/**\n * Helper class which is used to instantiate pretrained image segmentation models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModelForImageSegmentation.from_pretrained('facebook/detr-resnet-50-panoptic');\n */\nexport class AutoModelForImageSegmentation extends PretrainedMixin {\n  static MODEL_CLASS_MAPPINGS = [MODEL_FOR_IMAGE_SEGMENTATION_MAPPING_NAMES];\n}\n\n/**\n * Helper class which is used to instantiate pretrained image segmentation models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModelForSemanticSegmentation.from_pretrained('nvidia/segformer-b3-finetuned-cityscapes-1024-1024');\n */\nexport class AutoModelForSemanticSegmentation extends PretrainedMixin {\n  static MODEL_CLASS_MAPPINGS = [MODEL_FOR_SEMANTIC_SEGMENTATION_MAPPING_NAMES];\n}\n\n/**\n * Helper class which is used to instantiate pretrained object detection models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModelForObjectDetection.from_pretrained('facebook/detr-resnet-50');\n */\nexport class AutoModelForObjectDetection extends PretrainedMixin {\n  static MODEL_CLASS_MAPPINGS = [MODEL_FOR_OBJECT_DETECTION_MAPPING_NAMES];\n}\nexport class AutoModelForZeroShotObjectDetection extends PretrainedMixin {\n  static MODEL_CLASS_MAPPINGS = [MODEL_FOR_ZERO_SHOT_OBJECT_DETECTION_MAPPING_NAMES];\n}\n\n/**\n * Helper class which is used to instantiate pretrained mask generation models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModelForMaskGeneration.from_pretrained('Xenova/sam-vit-base');\n */\nexport class AutoModelForMaskGeneration extends PretrainedMixin {\n  static MODEL_CLASS_MAPPINGS = [MODEL_FOR_MASK_GENERATION_MAPPING_NAMES];\n}\nexport class AutoModelForCTC extends PretrainedMixin {\n  static MODEL_CLASS_MAPPINGS = [MODEL_FOR_CTC_MAPPING_NAMES];\n}\nexport class AutoModelForAudioClassification extends PretrainedMixin {\n  static MODEL_CLASS_MAPPINGS = [MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING_NAMES];\n}\nexport class AutoModelForXVector extends PretrainedMixin {\n  static MODEL_CLASS_MAPPINGS = [MODEL_FOR_AUDIO_XVECTOR_MAPPING_NAMES];\n}\nexport class AutoModelForAudioFrameClassification extends PretrainedMixin {\n  static MODEL_CLASS_MAPPINGS = [MODEL_FOR_AUDIO_FRAME_CLASSIFICATION_MAPPING_NAMES];\n}\nexport class AutoModelForDocumentQuestionAnswering extends PretrainedMixin {\n  static MODEL_CLASS_MAPPINGS = [MODEL_FOR_DOCUMENT_QUESTION_ANSWERING_MAPPING_NAMES];\n}\nexport class AutoModelForImageMatting extends PretrainedMixin {\n  static MODEL_CLASS_MAPPINGS = [MODEL_FOR_IMAGE_MATTING_MAPPING_NAMES];\n}\nexport class AutoModelForImageToImage extends PretrainedMixin {\n  static MODEL_CLASS_MAPPINGS = [MODEL_FOR_IMAGE_TO_IMAGE_MAPPING_NAMES];\n}\nexport class AutoModelForDepthEstimation extends PretrainedMixin {\n  static MODEL_CLASS_MAPPINGS = [MODEL_FOR_DEPTH_ESTIMATION_MAPPING_NAMES];\n}\nexport class AutoModelForImageFeatureExtraction extends PretrainedMixin {\n  static MODEL_CLASS_MAPPINGS = [MODEL_FOR_IMAGE_FEATURE_EXTRACTION_MAPPING_NAMES];\n}\n\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\nexport class Seq2SeqLMOutput extends ModelOutput {\n  /**\n   * @param {Object} output The output of the model.\n   * @param {Tensor} output.logits The output logits of the model.\n   * @param {Tensor} output.past_key_values An tensor of key/value pairs that represent the previous state of the model.\n   * @param {Tensor} output.encoder_outputs The output of the encoder in a sequence-to-sequence model.\n   * @param {Tensor} [output.decoder_attentions] Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the self-attention heads.\n   * @param {Tensor} [output.cross_attentions] Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the weighted average in the cross-attention heads.\n   */\n  constructor({\n    logits,\n    past_key_values,\n    encoder_outputs,\n    decoder_attentions = null,\n    cross_attentions = null\n  }) {\n    super();\n    this.logits = logits;\n    this.past_key_values = past_key_values;\n    this.encoder_outputs = encoder_outputs;\n    this.decoder_attentions = decoder_attentions;\n    this.cross_attentions = cross_attentions;\n  }\n}\n\n/**\n * Base class for outputs of sentence classification models.\n */\nexport class SequenceClassifierOutput extends ModelOutput {\n  /**\n   * @param {Object} output The output of the model.\n   * @param {Tensor} output.logits classification (or regression if config.num_labels==1) scores (before SoftMax).\n   */\n  constructor({\n    logits\n  }) {\n    super();\n    this.logits = logits;\n  }\n}\n\n/**\n * Base class for outputs of XVector models.\n */\nexport class XVectorOutput extends ModelOutput {\n  /**\n   * @param {Object} output The output of the model.\n   * @param {Tensor} output.logits Classification hidden states before AMSoftmax, of shape `(batch_size, config.xvector_output_dim)`.\n   * @param {Tensor} output.embeddings Utterance embeddings used for vector similarity-based retrieval, of shape `(batch_size, config.xvector_output_dim)`.\n   */\n  constructor({\n    logits,\n    embeddings\n  }) {\n    super();\n    this.logits = logits;\n    this.embeddings = embeddings;\n  }\n}\n\n/**\n * Base class for outputs of token classification models.\n */\nexport class TokenClassifierOutput extends ModelOutput {\n  /**\n   * @param {Object} output The output of the model.\n   * @param {Tensor} output.logits Classification scores (before SoftMax).\n   */\n  constructor({\n    logits\n  }) {\n    super();\n    this.logits = logits;\n  }\n}\n\n/**\n * Base class for masked language models outputs.\n */\nexport class MaskedLMOutput extends ModelOutput {\n  /**\n   * @param {Object} output The output of the model.\n   * @param {Tensor} output.logits Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n   */\n  constructor({\n    logits\n  }) {\n    super();\n    this.logits = logits;\n  }\n}\n\n/**\n * Base class for outputs of question answering models.\n */\nexport class QuestionAnsweringModelOutput extends ModelOutput {\n  /**\n   * @param {Object} output The output of the model.\n   * @param {Tensor} output.start_logits Span-start scores (before SoftMax).\n   * @param {Tensor} output.end_logits Span-end scores (before SoftMax).\n   */\n  constructor({\n    start_logits,\n    end_logits\n  }) {\n    super();\n    this.start_logits = start_logits;\n    this.end_logits = end_logits;\n  }\n}\n\n/**\n * Base class for causal language model (or autoregressive) outputs.\n */\nexport class CausalLMOutput extends ModelOutput {\n  /**\n   * @param {Object} output The output of the model.\n   * @param {Tensor} output.logits Prediction scores of the language modeling head (scores for each vocabulary token before softmax).\n   */\n  constructor({\n    logits\n  }) {\n    super();\n    this.logits = logits;\n  }\n}\n\n/**\n * Base class for causal language model (or autoregressive) outputs.\n */\nexport class CausalLMOutputWithPast extends ModelOutput {\n  /**\n   * @param {Object} output The output of the model.\n   * @param {Tensor} output.logits Prediction scores of the language modeling head (scores for each vocabulary token before softmax).\n   * @param {Tensor} output.past_key_values Contains pre-computed hidden-states (key and values in the self-attention blocks)\n   * that can be used (see `past_key_values` input) to speed up sequential decoding.\n   */\n  constructor({\n    logits,\n    past_key_values\n  }) {\n    super();\n    this.logits = logits;\n    this.past_key_values = past_key_values;\n  }\n}\nexport class ImageMattingOutput extends ModelOutput {\n  /**\n   * @param {Object} output The output of the model.\n   * @param {Tensor} output.alphas Estimated alpha values, of shape `(batch_size, num_channels, height, width)`.\n   */\n  constructor({\n    alphas\n  }) {\n    super();\n    this.alphas = alphas;\n  }\n}\n\n/**\n * Describes the outputs for the VITS model.\n */\nexport class VitsModelOutput extends ModelOutput {\n  /**\n   * @param {Object} output The output of the model.\n   * @param {Tensor} output.waveform The final audio waveform predicted by the model, of shape `(batch_size, sequence_length)`.\n   * @param {Tensor} output.spectrogram The log-mel spectrogram predicted at the output of the flow model.\n   * This spectrogram is passed to the Hi-Fi GAN decoder model to obtain the final audio waveform.\n   */\n  constructor({\n    waveform,\n    spectrogram\n  }) {\n    super();\n    this.waveform = waveform;\n    this.spectrogram = spectrogram;\n  }\n}","map":{"version":3,"names":["AutoConfig","Callable","isIntegralNumber","isTypedArray","mergeArrays","getModelFile","getModelJSON","LogitsProcessorList","GenerationConfig","ForceTokensLogitsProcessor","ForcedBOSTokenLogitsProcessor","ForcedEOSTokenLogitsProcessor","SuppressTokensAtBeginLogitsProcessor","WhisperTimeStampLogitsProcessor","NoRepeatNGramLogitsProcessor","RepetitionPenaltyLogitsProcessor","NoBadWordsLogitsProcessor","MinLengthLogitsProcessor","MinNewTokensLengthLogitsProcessor","Sampler","cat","dynamicTimeWarping","mean","ones_like","stack","std_mean","Tensor","executionProviders","ONNX","medianFilter","InferenceSession","ONNXTensor","env","MODEL_TYPES","EncoderOnly","EncoderDecoder","Seq2Seq","Vision2Seq","DecoderOnly","MaskGeneration","MODEL_TYPE_MAPPING","Map","MODEL_NAME_TO_CLASS_MAPPING","MODEL_CLASS_TO_NAME_MAPPING","constructSession","pretrained_model_name_or_path","fileName","options","modelFileName","quantized","buffer","create","err","length","console","warn","validateInputs","session","inputs","checkedInputs","Object","missingInputs","inputName","inputNames","tensor","push","wasm","proxy","clone","Error","join","numInputsProvided","keys","numInputsNeeded","ignored","filter","includes","sessionRun","output","run","replaceTensors","e","error","obj","prop","toI64Tensor","items","Array","isArray","some","x","BigInt64Array","from","flat","map","BigInt","prepareAttentionMask","self","tokens","pad_token_id","config","eos_token_id","is_pad_token_in_inputs","indexOf","is_pad_token_not_equal_to_eos_token_id","data","dims","preparePositionIds","feeds","use_cache_branch","attention_mask","i","start","sum","j","index","position_ids","slice","unsqueeze_","boolTensor","value","seq2seqForward","model_inputs","encoder_outputs","past_key_values","encoderForward","last_hidden_state","decoderFeeds","input_ids","decoder_input_ids","encoder_hidden_states","decoder_merged_session","encoder_attention_mask","addPastKeyValues","decoderResults","logits","getPastKeyValues","attns","getAttentions","Seq2SeqLMOutput","seq2seqStartBeams","inputTokenIds","generation_config","numOutputTokens","beams","beamId","requires_attention_mask","decoder_start_token_id","bos_token_id","tolist","prev_model_outputs","output_token_ids","done","score","id","seq2seqRunBeam","beam","input_name","main_input_name","forward","seq2seqUpdatebeam","newTokenId","encoderFeeds","key","token_type_ids","decoderForward","decoderStartBeams","inputs_attention_mask","Number","attn_mask","input","model_input_ids","num_output_tokens","decoderRunBeam","attnMaskData","fill","decoderUpdatebeam","PreTrainedModel","constructor","modelName","get","modelType","can_generate","_runBeam","_getStartBeams","_updateBeam","_forward","dispose","promises","item","handler","Promise","all","from_pretrained","progress_callback","cache_dir","local_files_only","revision","model_file_name","info","model_type","_call","_get_logits_processor","input_ids_seq_length","logits_processor","processors","repetition_penalty","no_repeat_ngram_size","bad_words_ids","min_length","min_new_tokens","forced_bos_token_id","forced_eos_token_id","max_length","begin_suppress_tokens","begin_index","forced_decoder_ids","extend","_get_generation_config","gen_config","assign","generate","errorMessage","possibleInfo","MODEL_WITH_LM_HEAD_MAPPING_NAMES","MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES","MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING_NAMES","MODEL_FOR_VISION_2_SEQ_MAPPING_NAMES","name","is_encoder_decoder","at","eos_token_ids","maxOutputTokens","max_new_tokens","Infinity","useMaxLength","isInteger","sampler","getSampler","getStartBeams","newest_beams","runBeam","output_attentions","addAttentionsToBeam","output_scores","sampledTokens","logProb","newBeam","updateBeam","groupBeams","group","sort","a","b","num_beams","callback_function","groupedBeams","getFlattened","batch","num_return_sequences","sequences","return_dict_in_generate","decoder_attentions","cross_attentions","groups","undefined","values","pastKeyValues","pkvs","startsWith","newName","replace","attnName","result","split","pop","batch_size","add_encoder_pkv","encoder_dims","num_encoder_heads","encoder_dim_kv","decoder_dims","num_decoder_heads","decoder_dim_kv","num_decoder_layers","num_heads","dim_kv","num_layers","multi_query","keyDims","valueDims","ModelOutput","BaseModelOutput","hidden_states","attentions","BertPreTrainedModel","BertModel","BertForMaskedLM","MaskedLMOutput","BertForSequenceClassification","SequenceClassifierOutput","BertForTokenClassification","TokenClassifierOutput","BertForQuestionAnswering","QuestionAnsweringModelOutput","NomicBertPreTrainedModel","NomicBertModel","RoFormerPreTrainedModel","RoFormerModel","RoFormerForMaskedLM","RoFormerForSequenceClassification","RoFormerForTokenClassification","RoFormerForQuestionAnswering","ConvBertPreTrainedModel","ConvBertModel","ConvBertForMaskedLM","ConvBertForSequenceClassification","ConvBertForTokenClassification","ConvBertForQuestionAnswering","ElectraPreTrainedModel","ElectraModel","ElectraForMaskedLM","ElectraForSequenceClassification","ElectraForTokenClassification","ElectraForQuestionAnswering","CamembertPreTrainedModel","CamembertModel","CamembertForMaskedLM","CamembertForSequenceClassification","CamembertForTokenClassification","CamembertForQuestionAnswering","DebertaPreTrainedModel","DebertaModel","DebertaForMaskedLM","DebertaForSequenceClassification","DebertaForTokenClassification","DebertaForQuestionAnswering","DebertaV2PreTrainedModel","DebertaV2Model","DebertaV2ForMaskedLM","DebertaV2ForSequenceClassification","DebertaV2ForTokenClassification","DebertaV2ForQuestionAnswering","DistilBertPreTrainedModel","DistilBertModel","DistilBertForSequenceClassification","DistilBertForTokenClassification","DistilBertForQuestionAnswering","DistilBertForMaskedLM","EsmPreTrainedModel","EsmModel","EsmForMaskedLM","EsmForSequenceClassification","EsmForTokenClassification","MobileBertPreTrainedModel","MobileBertModel","MobileBertForMaskedLM","MobileBertForSequenceClassification","MobileBertForQuestionAnswering","MPNetPreTrainedModel","MPNetModel","MPNetForMaskedLM","MPNetForSequenceClassification","MPNetForTokenClassification","MPNetForQuestionAnswering","SqueezeBertPreTrainedModel","SqueezeBertModel","SqueezeBertForMaskedLM","SqueezeBertForSequenceClassification","SqueezeBertForQuestionAnswering","AlbertPreTrainedModel","AlbertModel","AlbertForSequenceClassification","AlbertForQuestionAnswering","AlbertForMaskedLM","T5PreTrainedModel","T5Model","T5ForConditionalGeneration","d_kv","num_encoder_layers","LongT5PreTrainedModel","LongT5Model","LongT5ForConditionalGeneration","MT5PreTrainedModel","MT5Model","MT5ForConditionalGeneration","BartPretrainedModel","BartModel","BartForConditionalGeneration","decoder_layers","decoder_attention_heads","d_model","encoder_layers","encoder_attention_heads","BartForSequenceClassification","MBartPreTrainedModel","MBartModel","MBartForConditionalGeneration","MBartForSequenceClassification","MBartForCausalLM","BlenderbotPreTrainedModel","BlenderbotModel","BlenderbotForConditionalGeneration","BlenderbotSmallPreTrainedModel","BlenderbotSmallModel","BlenderbotSmallForConditionalGeneration","RobertaPreTrainedModel","RobertaModel","RobertaForMaskedLM","RobertaForSequenceClassification","RobertaForTokenClassification","RobertaForQuestionAnswering","XLMPreTrainedModel","XLMModel","XLMWithLMHeadModel","XLMForSequenceClassification","XLMForTokenClassification","XLMForQuestionAnswering","XLMRobertaPreTrainedModel","XLMRobertaModel","XLMRobertaForMaskedLM","XLMRobertaForSequenceClassification","XLMRobertaForTokenClassification","XLMRobertaForQuestionAnswering","ASTPreTrainedModel","ASTModel","ASTForAudioClassification","WhisperPreTrainedModel","WhisperModel","WhisperForConditionalGeneration","return_timestamps","return_token_timestamps","task","alignment_heads","outputs","_extract_token_timestamps","num_frames","generate_outputs","time_precision","median_filter_width","batchedMatrices","_","weights","l","h","transpose","std","calculatedMean","smoothedWeights","aTensor","bTensor","stdTensor","meanTensor","c","cTensor","d","set","matrix","timestampsShape","timestamps","Float32Array","batch_idx","neg","squeeze_","text_indices","time_indices","diffs","v","jumps","jump_times","VisionEncoderDecoderModel","encoderConfig","encoder","decoderConfig","decoder","encoderModelType","encoderModel","MODEL_MAPPING_NAMES_ENCODER_ONLY","MODEL_MAPPING_NAMES_ENCODER_DECODER","decoderModel","decoderModelClass","CLIPPreTrainedModel","CLIPModel","CLIPTextModelWithProjection","CLIPVisionModelWithProjection","SiglipPreTrainedModel","SiglipModel","SiglipTextModel","SiglipVisionModel","ChineseCLIPPreTrainedModel","ChineseCLIPModel","CLIPSegPreTrainedModel","CLIPSegModel","CLIPSegForImageSegmentation","GPT2PreTrainedModel","n_head","n_layer","n_embd","GPT2Model","GPT2LMHeadModel","GPTNeoPreTrainedModel","hidden_size","GPTNeoModel","GPTNeoForCausalLM","GPTNeoXPreTrainedModel","num_attention_heads","num_hidden_layers","GPTNeoXModel","GPTNeoXForCausalLM","GPTJPreTrainedModel","GPTJModel","GPTJForCausalLM","GPTBigCodePreTrainedModel","GPTBigCodeModel","GPTBigCodeForCausalLM","CodeGenPreTrainedModel","CodeGenModel","CodeGenForCausalLM","LlamaPreTrainedModel","num_key_value_heads","LlamaModel","LlamaForCausalLM","Qwen2PreTrainedModel","Qwen2Model","Qwen2ForCausalLM","PhiPreTrainedModel","PhiModel","PhiForCausalLM","BloomPreTrainedModel","BloomModel","BloomForCausalLM","MptPreTrainedModel","n_heads","n_layers","MptModel","MptForCausalLM","OPTPreTrainedModel","OPTModel","OPTForCausalLM","ViTPreTrainedModel","ViTModel","ViTForImageClassification","FastViTPreTrainedModel","FastViTModel","FastViTForImageClassification","VitMattePreTrainedModel","VitMatteForImageMatting","ImageMattingOutput","MobileViTPreTrainedModel","MobileViTModel","MobileViTForImageClassification","MobileViTV2PreTrainedModel","MobileViTV2Model","MobileViTV2ForImageClassification","OwlViTPreTrainedModel","OwlViTModel","OwlViTForObjectDetection","Owlv2PreTrainedModel","Owlv2Model","Owlv2ForObjectDetection","BeitPreTrainedModel","BeitModel","BeitForImageClassification","DetrPreTrainedModel","DetrModel","DetrForObjectDetection","DetrObjectDetectionOutput","DetrForSegmentation","DetrSegmentationOutput","pred_boxes","pred_masks","TableTransformerPreTrainedModel","TableTransformerModel","TableTransformerForObjectDetection","TableTransformerObjectDetectionOutput","DeiTPreTrainedModel","DeiTModel","DeiTForImageClassification","ResNetPreTrainedModel","ResNetModel","ResNetForImageClassification","SwinPreTrainedModel","SwinModel","SwinForImageClassification","Swin2SRPreTrainedModel","Swin2SRModel","Swin2SRForImageSuperResolution","DPTPreTrainedModel","DPTModel","DPTForDepthEstimation","DepthAnythingPreTrainedModel","DepthAnythingForDepthEstimation","GLPNPreTrainedModel","GLPNModel","GLPNForDepthEstimation","DonutSwinPreTrainedModel","DonutSwinModel","ConvNextPreTrainedModel","ConvNextModel","ConvNextForImageClassification","ConvNextV2PreTrainedModel","ConvNextV2Model","ConvNextV2ForImageClassification","Dinov2PreTrainedModel","Dinov2Model","Dinov2ForImageClassification","YolosPreTrainedModel","YolosModel","YolosForObjectDetection","YolosObjectDetectionOutput","SamPreTrainedModel","SamModel","vision_encoder","prompt_encoder_mask_decoder","get_image_embeddings","pixel_values","image_embeddings","image_positional_embeddings","input_labels","shape","input_points","numElements","reduce","SamImageSegmentationOutput","iou_scores","MarianPreTrainedModel","MarianModel","MarianMTModel","M2M100PreTrainedModel","M2M100Model","M2M100ForConditionalGeneration","Wav2Vec2PreTrainedModel","Wav2Vec2Model","Wav2Vec2ForCTC","CausalLMOutput","Wav2Vec2ForSequenceClassification","Wav2Vec2ForAudioFrameClassification","UniSpeechPreTrainedModel","UniSpeechModel","UniSpeechForCTC","UniSpeechForSequenceClassification","UniSpeechSatPreTrainedModel","UniSpeechSatModel","UniSpeechSatForCTC","UniSpeechSatForSequenceClassification","UniSpeechSatForAudioFrameClassification","Wav2Vec2BertPreTrainedModel","Wav2Vec2BertModel","Wav2Vec2BertForCTC","Wav2Vec2BertForSequenceClassification","HubertPreTrainedModel","HubertModel","HubertForCTC","HubertForSequenceClassification","WavLMPreTrainedModel","WavLMModel","WavLMForCTC","WavLMForSequenceClassification","WavLMForXVector","XVectorOutput","WavLMForAudioFrameClassification","SpeechT5PreTrainedModel","SpeechT5Model","SpeechT5ForSpeechToText","SpeechT5ForTextToSpeech","generate_speech","input_values","speaker_embeddings","threshold","minlenratio","maxlenratio","vocoder","r","reduction_factor","maxlen","Math","floor","minlen","num_mel_bins","spectrogramParts","decoder_outputs","idx","output_sequence","output_sequence_out","prob","spectrum","p","spectrogram","waveform","SpeechT5HifiGan","TrOCRPreTrainedModel","TrOCRForCausalLM","MistralPreTrainedModel","MistralModel","MistralForCausalLM","Starcoder2PreTrainedModel","Starcoder2Model","Starcoder2ForCausalLM","FalconPreTrainedModel","FalconModel","FalconForCausalLM","ClapPreTrainedModel","ClapModel","ClapTextModelWithProjection","ClapAudioModelWithProjection","VitsPreTrainedModel","VitsModel","VitsModelOutput","SegformerPreTrainedModel","SegformerModel","SegformerForImageClassification","SegformerForSemanticSegmentation","StableLmPreTrainedModel","StableLmModel","StableLmForCausalLM","EfficientNetPreTrainedModel","EfficientNetModel","EfficientNetForImageClassification","PretrainedMixin","MODEL_CLASS_MAPPINGS","BASE_IF_FAIL","MODEL_CLASS_MAPPING","modelInfo","MODEL_MAPPING_NAMES_DECODER_ONLY","MODEL_FOR_TEXT_TO_SPECTROGRAM_MAPPING_NAMES","MODEL_FOR_TEXT_TO_WAVEFORM_MAPPING_NAMES","MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES","MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING_NAMES","MODEL_FOR_MASKED_LM_MAPPING_NAMES","MODEL_FOR_QUESTION_ANSWERING_MAPPING_NAMES","MODEL_FOR_DOCUMENT_QUESTION_ANSWERING_MAPPING_NAMES","MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING_NAMES","MODEL_FOR_OBJECT_DETECTION_MAPPING_NAMES","MODEL_FOR_ZERO_SHOT_OBJECT_DETECTION_MAPPING_NAMES","MODEL_FOR_IMAGE_SEGMENTATION_MAPPING_NAMES","MODEL_FOR_SEMANTIC_SEGMENTATION_MAPPING_NAMES","MODEL_FOR_MASK_GENERATION_MAPPING_NAMES","MODEL_FOR_CTC_MAPPING_NAMES","MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING_NAMES","MODEL_FOR_AUDIO_XVECTOR_MAPPING_NAMES","MODEL_FOR_AUDIO_FRAME_CLASSIFICATION_MAPPING_NAMES","MODEL_FOR_IMAGE_MATTING_MAPPING_NAMES","MODEL_FOR_IMAGE_TO_IMAGE_MAPPING_NAMES","MODEL_FOR_DEPTH_ESTIMATION_MAPPING_NAMES","MODEL_FOR_IMAGE_FEATURE_EXTRACTION_MAPPING_NAMES","MODEL_CLASS_TYPE_MAPPING","mappings","type","model","CUSTOM_MAPPING","AutoModel","AutoModelForSequenceClassification","AutoModelForTokenClassification","AutoModelForSeq2SeqLM","AutoModelForSpeechSeq2Seq","AutoModelForTextToSpectrogram","AutoModelForTextToWaveform","AutoModelForCausalLM","AutoModelForMaskedLM","AutoModelForQuestionAnswering","AutoModelForVision2Seq","AutoModelForImageClassification","AutoModelForImageSegmentation","AutoModelForSemanticSegmentation","AutoModelForObjectDetection","AutoModelForZeroShotObjectDetection","AutoModelForMaskGeneration","AutoModelForCTC","AutoModelForAudioClassification","AutoModelForXVector","AutoModelForAudioFrameClassification","AutoModelForDocumentQuestionAnswering","AutoModelForImageMatting","AutoModelForImageToImage","AutoModelForDepthEstimation","AutoModelForImageFeatureExtraction","embeddings","start_logits","end_logits","CausalLMOutputWithPast","alphas"],"sources":["/Users/lorryrio/Project/calico/node_modules/@xenova/transformers/src/models.js"],"sourcesContent":["\n/**\n * @file Definitions of all models available in Transformers.js.\n * \n * **Example:** Load and run an `AutoModel`.\n * \n * ```javascript\n * import { AutoModel, AutoTokenizer } from '@xenova/transformers';\n *\n * let tokenizer = await AutoTokenizer.from_pretrained('Xenova/bert-base-uncased');\n * let model = await AutoModel.from_pretrained('Xenova/bert-base-uncased');\n *\n * let inputs = await tokenizer('I love transformers!');\n * let { logits } = await model(inputs);\n * // Tensor {\n * //     data: Float32Array(183132) [-7.117443084716797, -7.107812881469727, -7.092104911804199, ...]\n * //     dims: (3) [1, 6, 30522],\n * //     type: \"float32\",\n * //     size: 183132,\n * // }\n * ```\n * \n * We also provide other `AutoModel`s (listed below), which you can use in the same way as the Python library. For example:\n * \n * **Example:** Load and run an `AutoModelForSeq2SeqLM`.\n * ```javascript\n * import { AutoModelForSeq2SeqLM, AutoTokenizer } from '@xenova/transformers';\n * \n * let tokenizer = await AutoTokenizer.from_pretrained('Xenova/t5-small');\n * let model = await AutoModelForSeq2SeqLM.from_pretrained('Xenova/t5-small');\n *\n * let { input_ids } = await tokenizer('translate English to German: I love transformers!');\n * let outputs = await model.generate(input_ids);\n * let decoded = tokenizer.decode(outputs[0], { skip_special_tokens: true });\n * // 'Ich liebe Transformatoren!'\n * ```\n * \n * @module models\n */\n\nimport {\n    AutoConfig,\n} from './configs.js';\n\nimport {\n    Callable,\n    isIntegralNumber,\n    isTypedArray,\n    mergeArrays,\n} from './utils/core.js';\n\nimport {\n    getModelFile,\n    getModelJSON,\n} from './utils/hub.js';\n\nimport {\n    LogitsProcessorList,\n    GenerationConfig,\n    ForceTokensLogitsProcessor,\n    ForcedBOSTokenLogitsProcessor,\n    ForcedEOSTokenLogitsProcessor,\n    SuppressTokensAtBeginLogitsProcessor,\n    WhisperTimeStampLogitsProcessor,\n    NoRepeatNGramLogitsProcessor,\n    RepetitionPenaltyLogitsProcessor,\n    NoBadWordsLogitsProcessor,\n    MinLengthLogitsProcessor,\n    MinNewTokensLengthLogitsProcessor,\n\n    Sampler,\n} from './utils/generation.js';\n\nimport {\n    cat,\n    dynamicTimeWarping,\n    mean,\n    ones_like,\n    stack,\n    std_mean,\n    Tensor,\n} from './utils/tensor.js';\n\nimport { executionProviders, ONNX } from './backends/onnx.js';\nimport { medianFilter } from './transformers.js';\nconst { InferenceSession, Tensor: ONNXTensor, env } = ONNX;\n\n/** @typedef {import('onnxruntime-web').InferenceSession} InferenceSession */\n\n//////////////////////////////////////////////////\n// Model types: used internally\nconst MODEL_TYPES = {\n    EncoderOnly: 0,\n    EncoderDecoder: 1,\n    Seq2Seq: 2,\n    Vision2Seq: 3,\n    DecoderOnly: 4,\n    MaskGeneration: 5,\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// Helper functions\n\n// NOTE: These will be populated fully later\nconst MODEL_TYPE_MAPPING = new Map();\nconst MODEL_NAME_TO_CLASS_MAPPING = new Map();\nconst MODEL_CLASS_TO_NAME_MAPPING = new Map();\n\n\n/**\n * Constructs an InferenceSession using a model file located at the specified path.\n * @param {string} pretrained_model_name_or_path The path to the directory containing the model file.\n * @param {string} fileName The name of the model file.\n * @param {import('./utils/hub.js').PretrainedOptions} options Additional options for loading the model.\n * @returns {Promise<InferenceSession>} A Promise that resolves to an InferenceSession object.\n * @private\n */\nasync function constructSession(pretrained_model_name_or_path, fileName, options) {\n    // TODO add option for user to force specify their desired execution provider\n    let modelFileName = `onnx/${fileName}${options.quantized ? '_quantized' : ''}.onnx`;\n    let buffer = await getModelFile(pretrained_model_name_or_path, modelFileName, true, options);\n\n    try {\n        return await InferenceSession.create(buffer, {\n            executionProviders,\n        });\n    } catch (err) {\n        // If the execution provided was only wasm, throw the error\n        if (executionProviders.length === 1 && executionProviders[0] === 'wasm') {\n            throw err;\n        }\n\n        console.warn(err);\n        console.warn(\n            'Something went wrong during model construction (most likely a missing operation). ' +\n            'Using `wasm` as a fallback. '\n        )\n        return await InferenceSession.create(buffer, {\n            executionProviders: ['wasm']\n        });\n    }\n}\n\n/**\n * Validate model inputs\n * @param {InferenceSession} session The InferenceSession object that will be run.\n * @param {Record<string, Tensor>} inputs The inputs to check.\n * @returns {Record<string, Tensor>} The checked inputs.\n * @throws {Error} If any inputs are missing.\n * @private\n */\nfunction validateInputs(session, inputs) {\n    /**\n     * NOTE: Create either a shallow or deep copy based on `onnx.wasm.proxy`\n     * @type {Record<string, Tensor>}\n     */\n    const checkedInputs = Object.create(null);\n    const missingInputs = [];\n    for (const inputName of session.inputNames) {\n        const tensor = inputs[inputName];\n        // Rare case where one of the model's input names corresponds to a built-in\n        // object name (e.g., toString), which would cause a simple (!tensor) check to fail,\n        // because it's not undefined but a function.\n        if (!(tensor instanceof Tensor)) {\n            missingInputs.push(inputName);\n            continue;\n        }\n        // NOTE: When `env.wasm.proxy is true` the tensor is moved across the Worker\n        // boundary, transferring ownership to the worker and invalidating the tensor.\n        // So, in this case, we simply sacrifice a clone for it.\n        checkedInputs[inputName] = env.wasm.proxy ? tensor.clone() : tensor;\n    }\n    if (missingInputs.length > 0) {\n        throw new Error(\n            `An error occurred during model execution: \"Missing the following inputs: ${missingInputs.join(', ')}.`);\n    }\n\n    const numInputsProvided = Object.keys(inputs).length;\n    const numInputsNeeded = session.inputNames.length;\n    if (numInputsProvided > numInputsNeeded) {\n        // No missing inputs, but too many inputs were provided.\n        // Warn the user and ignore the extra inputs.\n        let ignored = Object.keys(inputs).filter(inputName => !session.inputNames.includes(inputName));\n        console.warn(`WARNING: Too many inputs were provided (${numInputsProvided} > ${numInputsNeeded}). The following inputs will be ignored: \"${ignored.join(', ')}\".`);\n    }\n\n    return checkedInputs;\n}\n\n/**\n * Executes an InferenceSession using the specified inputs.\n * NOTE: `inputs` must contain at least the input names of the model.\n *  - If additional inputs are passed, they will be ignored.\n *  - If inputs are missing, an error will be thrown.\n * \n * @param {InferenceSession} session The InferenceSession object to run.\n * @param {Object} inputs An object that maps input names to input tensors.\n * @returns {Promise<Object>} A Promise that resolves to an object that maps output names to output tensors.\n * @private\n */\nasync function sessionRun(session, inputs) {\n    const checkedInputs = validateInputs(session, inputs);\n    try {\n        // @ts-ignore\n        let output = await session.run(checkedInputs);\n        output = replaceTensors(output);\n        return output;\n    } catch (e) {\n        // This usually occurs when the inputs are of the wrong type.\n        console.error(`An error occurred during model execution: \"${e}\".`);\n        console.error('Inputs given to model:', checkedInputs);\n        throw e;\n    }\n}\n\n/**\n * Replaces ONNX Tensor objects with custom Tensor objects to support additional functions.\n * @param {Object} obj The object to replace tensor objects in.\n * @returns {Object} The object with tensor objects replaced by custom Tensor objects.\n * @private\n */\nfunction replaceTensors(obj) {\n    for (let prop in obj) {\n        if (obj[prop] instanceof ONNXTensor) {\n            obj[prop] = new Tensor(obj[prop]);\n        } else if (typeof obj[prop] === 'object') {\n            replaceTensors(obj[prop]);\n        }\n    }\n    return obj;\n}\n\n\n/**\n * Converts an array or Tensor of integers to an int64 Tensor.\n * @param {Array|Tensor} items The input integers to be converted.\n * @returns {Tensor} The int64 Tensor with the converted values.\n * @throws {Error} If the input array is empty or the input is a batched Tensor and not all sequences have the same length.\n * @private\n */\nfunction toI64Tensor(items) {\n    if (items instanceof Tensor) {\n        return items;\n    }\n    // items is an array\n    if (items.length === 0) {\n        throw Error(\"items must be non-empty\");\n    }\n\n    if (Array.isArray(items[0])) {\n        // batched\n        if (items.some(x => x.length !== items[0].length)) {\n            throw Error(\"Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' and/or 'truncation=True' to have batched tensors with the same length.\")\n        }\n\n        return new Tensor('int64',\n            BigInt64Array.from(items.flat().map(x => BigInt(x))),\n            [items.length, items[0].length]\n        );\n    } else {\n        //flat\n        return new Tensor('int64',\n            BigInt64Array.from(items.map(x => BigInt(x))),\n            [1, items.length]\n        );\n    }\n}\n\n/**\n * Prepares an attention mask for a sequence of tokens based on configuration options.\n * @param {Object} self The calling object instance.\n * @param {Tensor} tokens The input tokens.\n * @returns {Tensor} The attention mask tensor.\n * @private\n */\nfunction prepareAttentionMask(self, tokens) {\n\n    // Prepare attention mask\n    let pad_token_id = self.config.pad_token_id ?? null;\n    let eos_token_id = self.config.eos_token_id ?? null;\n    if (isIntegralNumber(eos_token_id)) {\n        eos_token_id = [eos_token_id];\n    }\n\n    let is_pad_token_in_inputs = tokens.indexOf(pad_token_id) !== -1;\n    let is_pad_token_not_equal_to_eos_token_id = (eos_token_id === null) || !eos_token_id.includes(pad_token_id)\n\n    if (is_pad_token_in_inputs && is_pad_token_not_equal_to_eos_token_id) {\n        let data = BigInt64Array.from(\n            // Note: != so that int matches bigint\n            // @ts-ignore\n            tokens.data.map(x => x != pad_token_id)\n        )\n        return new Tensor('int64', data, tokens.dims)\n    } else {\n        return ones_like(tokens);\n    }\n}\n\n/**\n * Add position IDs to the feeds object.\n * @param {Object} session The inference session.\n * @param {Object} feeds The input to the model.\n * @param {boolean} use_cache_branch Whether to use the cache branch of the model.\n * @returns {void}\n * @private\n */\nfunction preparePositionIds(session, feeds, use_cache_branch) {\n    if (!session.inputNames.includes('position_ids')) return;\n\n    const data = new BigInt64Array(feeds.attention_mask.data.length);\n\n    // Compute cumulative sum of the attention mask along the sequence length dimension\n    for (let i = 0; i < feeds.attention_mask.dims[0]; ++i) {\n        let start = i * feeds.attention_mask.dims[1];\n        let sum = BigInt(0);\n        for (let j = 0; j < feeds.attention_mask.dims[1]; ++j) {\n            const index = start + j;\n            if (feeds.attention_mask.data[index] === 0n) {\n                data[index] = BigInt(1);\n            } else { // === 1n\n                data[index] = sum;\n                sum += feeds.attention_mask.data[index];\n            }\n        }\n    }\n\n    feeds.position_ids = new Tensor('int64', data, feeds.attention_mask.dims);\n\n    if (use_cache_branch) {\n        feeds.position_ids = feeds.position_ids.slice(null, -1).unsqueeze_(-1);\n    }\n}\n\n/**\n * Creates a boolean tensor with a single value.\n * @param {boolean} value The value of the tensor.\n * @returns {Tensor} The boolean tensor.\n * @private\n */\nfunction boolTensor(value) {\n    return new Tensor('bool', [value], [1]);\n}\n\n// JS doesn't support mixins, so we define some reused functions here, and allow \"this\" to be passed in\n/**\n * Perform forward pass on the seq2seq model (both encoder and decoder).\n * @param {Object} self The seq2seq model object.\n * @param {Object} model_inputs The input object for the model containing encoder and decoder inputs.\n * @returns {Promise<Seq2SeqLMOutput>} Promise that resolves with the output of the seq2seq model.\n * @private\n */\nasync function seq2seqForward(self, model_inputs) {\n\n    let { encoder_outputs, past_key_values } = model_inputs;\n\n    if (!encoder_outputs) {\n        // Encoder outputs are not given, so we must compute them.\n        encoder_outputs = (await encoderForward(self, model_inputs)).last_hidden_state;\n    }\n    let decoderFeeds = {\n        input_ids: model_inputs.decoder_input_ids,\n        encoder_hidden_states: encoder_outputs,\n    };\n    const use_cache_branch = !!past_key_values;\n\n    if (self.decoder_merged_session.inputNames.includes('use_cache_branch')) {\n        decoderFeeds.use_cache_branch = boolTensor(use_cache_branch);\n    }\n\n    if (self.decoder_merged_session.inputNames.includes('encoder_attention_mask')) {\n        decoderFeeds.encoder_attention_mask = model_inputs.attention_mask\n    }\n\n    preparePositionIds(self.decoder_merged_session, decoderFeeds, use_cache_branch);\n    self.addPastKeyValues(decoderFeeds, past_key_values);\n\n    const decoderResults = await sessionRun(self.decoder_merged_session, decoderFeeds);\n    let logits = decoderResults.logits;\n    past_key_values = self.getPastKeyValues(decoderResults, past_key_values);\n\n    // Get cross attention and/or decoder attentions if they are present\n    const attns = self.getAttentions(decoderResults);\n\n    return new Seq2SeqLMOutput({ logits, past_key_values, encoder_outputs, ...attns });\n}\n\n/**\n * Start the beam search process for the seq2seq model.\n * @param {PreTrainedModel} self The seq2seq model object.\n * @param {Tensor} inputTokenIds Array of input token ids for each input sequence.\n * @param {Object} generation_config The generation config.\n * @param {number} numOutputTokens The maximum number of output tokens for the model.\n * @returns {Object[]} Array of beam search objects.\n * @private\n */\nfunction seq2seqStartBeams(self, inputTokenIds, generation_config, numOutputTokens) {\n    let beams = [];\n    let beamId = 0;\n\n    // @ts-ignore\n    const requires_attention_mask = self.requires_attention_mask ?? true;\n\n    // decoder_input_ids == output_token_ids\n    let decoder_input_ids =\n        generation_config.decoder_input_ids\n        ?? generation_config.decoder_start_token_id\n        ?? generation_config.bos_token_id\n        ?? generation_config.eos_token_id;\n\n    // Support input as tensor or list\n    // TODO support batched decoder_input_ids\n    if (decoder_input_ids instanceof Tensor) {\n        decoder_input_ids = decoder_input_ids.tolist().flat();\n    } else if (!Array.isArray(decoder_input_ids)) {\n        decoder_input_ids = [decoder_input_ids];\n    }\n\n    for (let tokens of inputTokenIds) {\n        // TODO: Improve\n        // Currently, just add back batch dimension.\n        // In future, allow for true parallel execution\n        tokens.dims = [1, ...tokens.dims]\n\n        // Create beam\n        let start = {\n            inputs: tokens,\n            encoder_outputs: null,\n            prev_model_outputs: null,\n\n            output_token_ids: decoder_input_ids,\n            done: false,\n            score: 0,\n            id: beamId++ // assign unique id to beams\n        }\n\n        if (requires_attention_mask) {\n            start.attention_mask = prepareAttentionMask(self, tokens);\n        }\n\n        beams.push(start);\n    }\n\n    return beams;\n}\n\n/**\n * Run beam search on the seq2seq model for a single beam.\n * @param {PreTrainedModel} self The seq2seq model object.\n * @param {Object} beam The beam search object for which to run the model.\n * @param {Object} options options\n * @param {string} [options.input_name='input_ids'] The name of the input tensor for the encoder.\n * @returns {Promise<Object>} Promise that resolves with the output of the seq2seq model for the given beam.\n * @private\n */\nasync function seq2seqRunBeam(self, beam) {\n    const input_name = self.main_input_name;\n\n    let decoder_input_ids = beam.output_token_ids;\n    if (beam.prev_model_outputs) {\n        // After the first step, `prev_model_outputs` won't be null.\n        // So, we cut decoder_input_ids if past is used\n        decoder_input_ids = decoder_input_ids.slice(-1);\n    }\n\n    // 1. Prepare\n    let model_inputs = {\n        [input_name]: beam.inputs,\n        decoder_input_ids: toI64Tensor(decoder_input_ids),\n        encoder_outputs: beam.encoder_outputs,\n        past_key_values: beam.prev_model_outputs?.past_key_values,\n    }\n    if (beam.attention_mask) {\n        model_inputs.attention_mask = beam.attention_mask\n    }\n\n    // 2. Run\n    let output = await self.forward(model_inputs);\n\n    // 3. Update\n    beam.prev_model_outputs = output;\n    beam.encoder_outputs = output.encoder_outputs;\n\n    return output;\n}\n\n/**\n * Update a beam with a new token ID.\n * @param {Object} beam The beam to update.\n * @param {number} newTokenId The new token ID to add to the beam's output.\n * @private\n */\nfunction seq2seqUpdatebeam(beam, newTokenId) {\n    beam.output_token_ids = [...beam.output_token_ids, newTokenId];\n}\n\n/**\n * Forward pass of an encoder model.\n * @param {Object} self The encoder model.\n * @param {Object} model_inputs The input data to be used for the forward pass.\n * @returns {Promise<Object>} Promise that resolves with an object containing the model's outputs.\n * @private\n */\nasync function encoderForward(self, model_inputs) {\n    const encoderFeeds = Object.create(null);\n    for (const key of self.session.inputNames) {\n        encoderFeeds[key] = model_inputs[key];\n    }\n    if (self.session.inputNames.includes('token_type_ids') && !encoderFeeds.token_type_ids) {\n        // Assign default `token_type_ids` (all zeroes) to the `encoderFeeds` if the model expects it,\n        // but they weren't created by the tokenizer.\n        encoderFeeds.token_type_ids = new Tensor(\n            'int64',\n            new BigInt64Array(encoderFeeds.input_ids.data.length),\n            encoderFeeds.input_ids.dims\n        )\n    }\n    return await sessionRun(self.session, encoderFeeds);\n}\n\n\n/**\n * Forward pass of a decoder model.\n * @param {Object} self The decoder model.\n * @param {Object} model_inputs The input data to be used for the forward pass.\n * @returns {Promise<Object>} Promise that resolves with an object containing the logits and past key values.\n * @private\n */\nasync function decoderForward(self, model_inputs) {\n    let { input_ids, past_key_values, attention_mask } = model_inputs;\n    let decoderFeeds = {\n        input_ids: input_ids,\n        attention_mask: attention_mask ?? prepareAttentionMask(self, input_ids),\n    }\n    const use_cache_branch = !!past_key_values;\n\n    if (self.session.inputNames.includes('use_cache_branch')) {\n        decoderFeeds.use_cache_branch = boolTensor(use_cache_branch);\n    }\n\n    preparePositionIds(self.session, decoderFeeds, use_cache_branch);\n\n    self.addPastKeyValues(decoderFeeds, past_key_values);\n\n    let decoderResults = await sessionRun(self.session, decoderFeeds);\n\n    let logits = decoderResults.logits;\n\n    past_key_values = self.getPastKeyValues(decoderResults, past_key_values);\n    return { logits, past_key_values };\n}\n\n/**\n * Starts the generation of text by initializing the beams for the given input token IDs.\n * @param {Object} self The text generation model object.\n * @param {Tensor} inputTokenIds An tensor of input token IDs to generate text from.\n * @param {Object} generation_config The generation config.\n * @param {number} numOutputTokens The maximum number of tokens to generate for each beam.\n * @param {Tensor} [inputs_attention_mask] The attention mask tensor for the input token IDs.\n * @returns {Object[]} An array of beams initialized with the given inputs and parameters.\n * @private\n */\nfunction decoderStartBeams(self, inputTokenIds, generation_config, numOutputTokens, inputs_attention_mask) {\n    let beams = [];\n\n    let beamId = 0;\n    for (let tokens of inputTokenIds) {\n        let output_token_ids = tokens.tolist().map(Number);\n\n        // TODO: Improve\n        // Currently, just add back batch dimension.\n        // In future, allow for true parallel execution\n        tokens.dims = [1, ...tokens.dims]\n\n        let attn_mask;\n        if (inputs_attention_mask) {\n            attn_mask = inputs_attention_mask[beamId];\n            attn_mask.dims = [1, ...attn_mask.dims]\n\n        } else {\n            attn_mask = prepareAttentionMask(self, tokens)\n        }\n\n        let start = {\n            input: tokens,\n            model_input_ids: tokens,\n            attention_mask: attn_mask,\n            prev_model_outputs: null,\n\n            output_token_ids: output_token_ids,\n            num_output_tokens: numOutputTokens,\n\n            done: false,\n            score: 0,\n            id: beamId++ // assign unique id to beams\n        }\n\n        beams.push(start);\n    }\n    return beams;\n}\n\n/**\n * Runs a single step of the text generation process for a given beam.\n *\n * @param {Object} self The decoder object.\n * @param {Object} beam The beam to run.\n * @param {Tensor} beam.input The input tensor.\n * @param {Tensor} beam.model_input_ids The input ids to the model.\n * @param {Tensor} beam.attention_mask The attention mask.\n * @param {Object} beam.prev_model_outputs The past key values.\n * @param {number[]} beam.output_token_ids The output token ids.\n * @returns {Promise<Object>} The output of the generation step.\n * @private\n */\nasync function decoderRunBeam(self, beam) {\n    let attnMaskData = new BigInt64Array(beam.output_token_ids.length).fill(1n)\n\n    // 1. Prepare\n    let model_inputs = {\n        input_ids: beam.model_input_ids,\n        attention_mask: new Tensor(\n            'int64',\n            attnMaskData,\n            [1, attnMaskData.length]\n        ),\n        past_key_values: beam.prev_model_outputs?.past_key_values,\n    }\n\n    // 2. Run\n    let output = await self.forward(model_inputs);\n\n    // 3. Update\n    beam.prev_model_outputs = output;\n\n    return output;\n}\n\n/**\n * Update a beam with a new token ID.\n * @param {Object} beam The beam to update.\n * @param {number} newTokenId The new token ID to add to the beam's output.\n * @private\n */\nfunction decoderUpdatebeam(beam, newTokenId) {\n    beam.output_token_ids = [...beam.output_token_ids, newTokenId];\n    beam.model_input_ids = new Tensor('int64', [BigInt(newTokenId)], [1, 1]);\n}\n\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n/**\n * A base class for pre-trained models that provides the model configuration and an ONNX session.\n */\nexport class PreTrainedModel extends Callable {\n    main_input_name = 'input_ids';\n\n    /**\n     * Creates a new instance of the `PreTrainedModel` class.\n     * @param {Object} config The model configuration.\n     * @param {any} session session for the model.\n     */\n    constructor(config, session) {\n        super();\n\n        this.config = config;\n        this.session = session;\n\n        const modelName = MODEL_CLASS_TO_NAME_MAPPING.get(this.constructor);\n        const modelType = MODEL_TYPE_MAPPING.get(modelName);\n\n        this.can_generate = false;\n        this._runBeam = null;\n        this._getStartBeams = null;\n        this._updateBeam = null;\n        this._forward = null;\n        if (modelType === MODEL_TYPES.DecoderOnly) {\n            this.can_generate = true;\n\n            this._runBeam = decoderRunBeam;\n            this._getStartBeams = decoderStartBeams;\n            this._updateBeam = decoderUpdatebeam;\n            this._forward = decoderForward;\n\n        } else if (modelType === MODEL_TYPES.Seq2Seq || modelType === MODEL_TYPES.Vision2Seq) {\n            this.can_generate = true;\n\n            this._runBeam = seq2seqRunBeam;\n            this._getStartBeams = seq2seqStartBeams;\n            this._updateBeam = seq2seqUpdatebeam;\n            this._forward = seq2seqForward;\n\n        } else if (modelType === MODEL_TYPES.EncoderDecoder) {\n            this._forward = encoderForward;\n\n        } else { // should be MODEL_TYPES.EncoderOnly\n            this._forward = encoderForward;\n        }\n    }\n\n    /**\n    * Disposes of all the ONNX sessions that were created during inference.\n    * @returns {Promise<unknown[]>} An array of promises, one for each ONNX session that is being disposed.\n    * @todo Use https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/FinalizationRegistry\n    */\n    async dispose() {\n        const promises = [];\n        for (let key of Object.keys(this)) {\n            const item = this[key];\n            // @ts-ignore\n            if (item instanceof InferenceSession) {\n                promises.push(item.handler.dispose())\n            }\n        }\n        return await Promise.all(promises);\n    }\n\n    /**\n     * Instantiate one of the model classes of the library from a pretrained model.\n     * \n     * The model class to instantiate is selected based on the `model_type` property of the config object\n     * (either passed as an argument or loaded from `pretrained_model_name_or_path` if possible)\n     * \n     * @param {string} pretrained_model_name_or_path The name or path of the pretrained model. Can be either:\n     * - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.\n     *   Valid model ids can be located at the root-level, like `bert-base-uncased`, or namespaced under a\n     *   user or organization name, like `dbmdz/bert-base-german-cased`.\n     * - A path to a *directory* containing model weights, e.g., `./my_model_directory/`.\n     * @param {import('./utils/hub.js').PretrainedOptions} options Additional options for loading the model.\n     * \n     * @returns {Promise<PreTrainedModel>} A new instance of the `PreTrainedModel` class.\n     */\n    static async from_pretrained(pretrained_model_name_or_path, {\n        quantized = true,\n        progress_callback = null,\n        config = null,\n        cache_dir = null,\n        local_files_only = false,\n        revision = 'main',\n        model_file_name = null,\n    } = {}) {\n\n        let options = {\n            quantized,\n            progress_callback,\n            config,\n            cache_dir,\n            local_files_only,\n            revision,\n            model_file_name,\n        }\n\n        const modelName = MODEL_CLASS_TO_NAME_MAPPING.get(this);\n        const modelType = MODEL_TYPE_MAPPING.get(modelName);\n\n        let info;\n        if (modelType === MODEL_TYPES.DecoderOnly) {\n            info = await Promise.all([\n                AutoConfig.from_pretrained(pretrained_model_name_or_path, options),\n                constructSession(pretrained_model_name_or_path, options.model_file_name ?? 'decoder_model_merged', options),\n                getModelJSON(pretrained_model_name_or_path, 'generation_config.json', false, options),\n            ]);\n\n        } else if (modelType === MODEL_TYPES.Seq2Seq || modelType === MODEL_TYPES.Vision2Seq) {\n            info = await Promise.all([\n                AutoConfig.from_pretrained(pretrained_model_name_or_path, options),\n                constructSession(pretrained_model_name_or_path, 'encoder_model', options),\n                constructSession(pretrained_model_name_or_path, 'decoder_model_merged', options),\n                getModelJSON(pretrained_model_name_or_path, 'generation_config.json', false, options),\n            ]);\n\n        } else if (modelType === MODEL_TYPES.MaskGeneration) {\n            info = await Promise.all([\n                AutoConfig.from_pretrained(pretrained_model_name_or_path, options),\n                constructSession(pretrained_model_name_or_path, 'vision_encoder', options),\n                constructSession(pretrained_model_name_or_path, 'prompt_encoder_mask_decoder', options),\n            ]);\n\n        } else if (modelType === MODEL_TYPES.EncoderDecoder) {\n            info = await Promise.all([\n                AutoConfig.from_pretrained(pretrained_model_name_or_path, options),\n                constructSession(pretrained_model_name_or_path, 'encoder_model', options),\n                constructSession(pretrained_model_name_or_path, 'decoder_model_merged', options),\n            ]);\n\n        } else { // should be MODEL_TYPES.EncoderOnly\n            if (modelType !== MODEL_TYPES.EncoderOnly) {\n                console.warn(`Model type for '${modelName ?? config?.model_type}' not found, assuming encoder-only architecture. Please report this at https://github.com/xenova/transformers.js/issues/new/choose.`)\n            }\n            info = await Promise.all([\n                AutoConfig.from_pretrained(pretrained_model_name_or_path, options),\n                constructSession(pretrained_model_name_or_path, options.model_file_name ?? 'model', options)\n            ]);\n        }\n\n        // @ts-ignore\n        return new this(...info);\n    }\n\n    /**\n     * Runs the model with the provided inputs\n     * @param {Object} model_inputs Object containing input tensors\n     * @returns {Promise<Object>} Object containing output tensors\n     */\n    async _call(model_inputs) {\n        return await this.forward(model_inputs);\n    }\n\n    /**\n     * Forward method for a pretrained model. If not overridden by a subclass, the correct forward method\n     * will be chosen based on the model type.\n     * @param {Object} model_inputs The input data to the model in the format specified in the ONNX model.\n     * @returns {Promise<Object>} The output data from the model in the format specified in the ONNX model.\n     * @throws {Error} This method must be implemented in subclasses.\n     */\n    async forward(model_inputs) {\n        return await this._forward(this, model_inputs);\n    }\n\n    /**\n     * @param {import('./utils/generation.js').GenerationConfigType} generation_config \n     * @param {number} input_ids_seq_length The starting sequence length for the input ids.\n     * @returns {LogitsProcessorList}\n     * @private\n     */\n    _get_logits_processor(\n        generation_config,\n        input_ids_seq_length,\n        // encoder_input_ids, TODO\n        // prefix_allowed_tokens_fn, TODO\n        logits_processor = null\n    ) {\n        const processors = new LogitsProcessorList();\n\n        // if (generation_config.diversity_penalty !== null && generation_config.diversity_penalty > 0.0) {\n        //     processors.push(new HammingDiversityLogitsProcessor(\n        //         generation_config.diversity_penalty,\n        //         generation_config.num_beams,\n        //         generation_config.num_beam_groups\n        //     ));\n        // }\n\n        // if (generation_config.encoder_repetition_penalty !== null && generation_config.encoder_repetition_penalty !== 1.0) {\n        //     processors.push(new EncoderRepetitionPenaltyLogitsProcessor(\n        //         generation_config.encoder_repetition_penalty,\n        //         encoder_input_ids\n        //     ));\n        // }\n\n        if (generation_config.repetition_penalty !== null && generation_config.repetition_penalty !== 1.0) {\n            processors.push(new RepetitionPenaltyLogitsProcessor(generation_config.repetition_penalty));\n        }\n\n        if (generation_config.no_repeat_ngram_size !== null && generation_config.no_repeat_ngram_size > 0) {\n            processors.push(new NoRepeatNGramLogitsProcessor(generation_config.no_repeat_ngram_size));\n        }\n\n        // if (generation_config.encoder_no_repeat_ngram_size !== null && generation_config.encoder_no_repeat_ngram_size > 0) {\n        //     if (this.config.is_encoder_decoder) {\n        //         processors.push(new EncoderNoRepeatNGramLogitsProcessor(\n        //             generation_config.encoder_no_repeat_ngram_size,\n        //             encoder_input_ids\n        //         ));\n        //     } else {\n        //         throw new Error(\"It's impossible to use `encoder_no_repeat_ngram_size` with decoder-only architecture\");\n        //     }\n        // }\n\n        if (generation_config.bad_words_ids !== null) {\n            processors.push(new NoBadWordsLogitsProcessor(generation_config.bad_words_ids, generation_config.eos_token_id));\n        }\n\n        if (generation_config.min_length !== null && generation_config.eos_token_id !== null && generation_config.min_length > 0) {\n            processors.push(new MinLengthLogitsProcessor(generation_config.min_length, generation_config.eos_token_id));\n        }\n\n        if (generation_config.min_new_tokens !== null && generation_config.eos_token_id !== null && generation_config.min_new_tokens > 0) {\n            processors.push(new MinNewTokensLengthLogitsProcessor(\n                input_ids_seq_length,\n                generation_config.min_new_tokens,\n                generation_config.eos_token_id\n            ));\n        }\n\n        // if (prefix_allowed_tokens_fn !== null) {\n        //     processors.push(new PrefixConstrainedLogitsProcessor(\n        //         prefix_allowed_tokens_fn,\n        //         generation_config.num_beams / generation_config.num_beam_groups\n        //     ));\n        // }\n\n\n        if (generation_config.forced_bos_token_id !== null) {\n            processors.push(new ForcedBOSTokenLogitsProcessor(generation_config.forced_bos_token_id));\n        }\n\n        if (generation_config.forced_eos_token_id !== null) {\n            processors.push(new ForcedEOSTokenLogitsProcessor(\n                generation_config.max_length,\n                generation_config.forced_eos_token_id\n            ));\n        }\n\n        // if (generation_config.remove_invalid_values === true) {\n        //     processors.push(new InfNanRemoveLogitsProcessor());\n        // }\n\n        // if (generation_config.exponential_decay_length_penalty !== null) {\n        //     processors.push(new ExponentialDecayLengthPenalty(\n        //         generation_config.exponential_decay_length_penalty,\n        //         generation_config.eos_token_id,\n        //         input_ids_seq_length\n        //     ));\n        // }\n\n        // if (generation_config.suppress_tokens !== null) {\n        //     processors.push(new SuppressTokensLogitsProcessor(generation_config.suppress_tokens));\n        // }\n\n        if (generation_config.begin_suppress_tokens !== null) {\n            let begin_index = (input_ids_seq_length > 1 || generation_config.forced_bos_token_id === null)\n                ? input_ids_seq_length\n                : input_ids_seq_length + 1;\n\n            if (generation_config.forced_decoder_ids !== null) {\n                // generation starts after the last token that is forced\n                begin_index += generation_config.forced_decoder_ids[generation_config.forced_decoder_ids.length - 1][0];\n            }\n            processors.push(new SuppressTokensAtBeginLogitsProcessor(generation_config.begin_suppress_tokens, begin_index));\n        }\n\n        if (generation_config.forced_decoder_ids !== null) {\n            processors.push(new ForceTokensLogitsProcessor(generation_config.forced_decoder_ids));\n        }\n\n        if (logits_processor !== null) {\n            processors.extend(logits_processor)\n        }\n\n        // `LogitNormalization` should always be the last logit processor, when present\n        // if (generation_config.renormalize_logits === true) {\n        //     processors.push(new LogitNormalization());\n        // }\n\n        return processors;\n    }\n\n    /**\n     * This function merges multiple generation configs together to form a final generation config to be used by the model for text generation.\n     * It first creates an empty `GenerationConfig` object, then it applies the model's own `generation_config` property to it. Finally, if a `generation_config` object was passed in the arguments, it overwrites the corresponding properties in the final config with those of the passed config object.\n     * @param {import('./utils/generation.js').GenerationConfigType} generation_config A `GenerationConfig` object containing generation parameters.\n     * @returns {import('./utils/generation.js').GenerationConfigType} The final generation config object to be used by the model for text generation.\n     */\n    _get_generation_config(generation_config) {\n        // Create empty generation config (contains defaults)\n        // We pass `this.config` so that if `eos_token_id` or `bos_token_id` exist in the model's config, we will use them\n        let gen_config = new GenerationConfig(this.config);\n\n        // Apply model's generation config, if it exists\n        if ('generation_config' in this) {\n            Object.assign(gen_config, this.generation_config);\n        }\n\n        // Finally, use any generation config specified by the user\n        // when calling `generate`\n        if (generation_config !== null) {\n            Object.assign(gen_config, generation_config);\n        }\n        return gen_config;\n    }\n\n    /**\n     * @typedef {import('./utils/maths.js').TypedArray} TypedArray\n     */\n\n    /**\n     * @typedef {{ sequences: Tensor, decoder_attentions: Tensor, cross_attentions: Tensor }} EncoderDecoderOutput\n     * @typedef {Object} DecoderOutput\n     * \n     * Generates text based on the given inputs and generation configuration using the model.\n     * @param {Tensor|Array|TypedArray} inputs An array of input token IDs.\n     * @param {Object|GenerationConfig|null} generation_config The generation configuration to use. If null, default configuration will be used.\n     * @param {Object|null} logits_processor An optional logits processor to use. If null, a new LogitsProcessorList instance will be created.\n     * @param {Object} options options\n     * @param {Object} [options.inputs_attention_mask=null] An optional attention mask for the inputs.\n     * @returns {Promise<number[][]|EncoderDecoderOutput|DecoderOutput>} An array of generated output sequences, where each sequence is an array of token IDs.\n     * @throws {Error} Throws an error if the inputs array is empty.\n     */\n    async generate(\n        inputs,\n        generation_config = null,\n        logits_processor = null,\n        {\n            inputs_attention_mask = null\n        } = {},\n    ) {\n        if (!this.can_generate) {\n            const modelName = MODEL_CLASS_TO_NAME_MAPPING.get(this.constructor);\n            let errorMessage = `The current model class (${modelName}) is not compatible with \\`.generate()\\`, as it doesn't have a language model head.`\n\n            const modelType = this.config.model_type;\n            const possibleInfo =\n                MODEL_WITH_LM_HEAD_MAPPING_NAMES.get(modelType)\n                ?? MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES.get(modelType)\n                ?? MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING_NAMES.get(modelType)\n                // ?? MODEL_FOR_TEXT_TO_SPECTROGRAM_MAPPING_NAMES.get(modelType) // TODO\n                ?? MODEL_FOR_VISION_2_SEQ_MAPPING_NAMES.get(modelType);\n\n            if (possibleInfo) {\n                // TODO: support multiple possible classes\n                errorMessage += ` Please use the following class instead: '${possibleInfo[0]}'`;\n            }\n            throw Error(errorMessage);\n        }\n\n        if (!(inputs instanceof Tensor) && !isTypedArray(inputs) && !Array.isArray(inputs)) {\n            throw Error(`\\`inputs\\` must be a Tensor, TypedArray, or Array, but is \"${inputs.constructor.name}\".`);\n        }\n\n        let input_ids_seq_length;\n\n        // Prepare `input_ids` which will be used for auto-regressive generation\n        // TODO: Update to align with HF transformers' implementation\n        if (this.config.is_encoder_decoder) {\n            // Generating from the encoder outputs\n            input_ids_seq_length = 0;\n\n        } else {\n            input_ids_seq_length = inputs instanceof Tensor ? inputs.dims.at(-1) : inputs.length;\n\n            // decoder-only\n            if (input_ids_seq_length === 0) {\n                throw Error(\"Must supply a non-empty array of input token ids.\")\n            }\n        }\n\n        // Update generation config with defaults\n        generation_config = this._get_generation_config(generation_config);\n\n        logits_processor = logits_processor ?? new LogitsProcessorList()\n\n        // Update logits processor\n        logits_processor = this._get_logits_processor(\n            generation_config,\n            input_ids_seq_length,\n            logits_processor\n        )\n\n        /** @type {number[]} */\n        let eos_token_ids = generation_config.eos_token_id;\n        if (eos_token_ids !== null && !Array.isArray(eos_token_ids)) {\n            eos_token_ids = [eos_token_ids];\n        }\n\n        // TODO implement early_stopping\n        // https://huggingface.co/blog/how-to-generate\n\n        let numOutputTokens = 1;\n        const maxOutputTokens = numOutputTokens + (generation_config.max_new_tokens ?? Infinity);\n\n        // Only use max length if max_new_tokens is not provided\n        const useMaxLength = Number.isInteger(generation_config.max_length) && (generation_config.max_new_tokens ?? null) === null;\n        let sampler = Sampler.getSampler(generation_config);\n\n        // @ts-ignore\n        let beams = this.getStartBeams(inputs, generation_config, numOutputTokens, inputs_attention_mask);\n\n        while (beams.some(x => !x.done) && numOutputTokens < maxOutputTokens) {\n            let newest_beams = [];\n            for (let beam of beams) {\n                if (beam.done) {\n                    // Add this beam back into the pool\n                    newest_beams.push(beam);\n                    continue\n                }\n                if (useMaxLength && beam.output_token_ids.length >= generation_config.max_length) {\n                    // Set this beam to done and add it back into the pool\n                    beam.done = true;\n                    newest_beams.push(beam);\n                    continue\n                }\n\n                // @ts-ignore\n                let output = await this.runBeam(beam);\n\n                // add attentions/scores to beam only if user requested\n                if (generation_config.output_attentions) {\n                    this.addAttentionsToBeam(beam, output);\n                }\n                if (generation_config.output_scores) {\n                    // TODO add\n                }\n\n                // Logits are of the form [batch_size, out_seq_length, vocab_size]\n                // In most cases, this will be [batch_size, 1, vocab_size]\n                // So, we select the last token's logits:\n                // (equivalent to `logits = outputs.logits[:, -1, :]`)\n                let logits = output.logits.slice(null, -1, null);\n\n                // Apply logits processor\n                logits_processor(beam.output_token_ids, logits);\n\n                let sampledTokens = sampler(logits);\n                for (let [newTokenId, logProb] of sampledTokens) {\n                    // use previous beam as a starting point\n                    let newBeam = { ...beam };\n\n                    // update new beam\n                    // @ts-ignore\n                    this.updateBeam(newBeam, newTokenId);\n\n                    newBeam.score += logProb;\n\n                    if (eos_token_ids && eos_token_ids.includes(newTokenId)) {\n                        newBeam.done = true;\n                    }\n\n                    newest_beams.push(newBeam);\n                }\n            }\n            ++numOutputTokens;\n\n            // Next, we get the best beams, per ID\n            newest_beams = this.groupBeams(newest_beams).map(\n                group => group\n                    .sort((a, b) => b.score - a.score)      // sort by score\n                    .slice(0, generation_config.num_beams)  // remove outside beam width\n            );\n\n            // Flatten beams\n            beams = newest_beams.flat();\n\n            // Run callback\n            if (generation_config.callback_function) {\n                generation_config.callback_function(beams);\n            }\n        }\n\n        // TODO: Ensure that we can return non-batched outputs\n\n        const groupedBeams = this.groupBeams(beams);\n\n        const getFlattened = (key) => groupedBeams.map(\n            batch => {\n                if (generation_config.num_return_sequences > 1) {\n                    return batch.slice(0, generation_config.num_return_sequences).map(x => x[key]);\n                } else {\n                    return [batch[0][key]];\n                }\n            }\n        ).flat(); // Flatten across batches (depth=1)\n\n        const sequences = getFlattened('output_token_ids'); // [1, seqLength]\n\n        if (generation_config.return_dict_in_generate) {\n            // NOTE: `decoder_attentions` and `cross_attentions` should be:\n            //    list (one element for each generated token)\n            //    of list (one element for each layer of the decoder)\n            //    of torch.FloatTensor of shape (batch_size, num_heads, generated_length, sequence_length)\n            // However, since we are only generating one batch at a time, they are of the form:\n            //   list (batches)\n            //   of list (one element for each generated token)\n            //   of list (one element for each layer of the decoder)\n            //   of torch.FloatTensor of shape (1, num_heads, generated_length, sequence_length)\n            // \n            // TODO: In future (when true parallelism, we should be able to return the correct shape)\n\n            const decoder_attentions = getFlattened('decoder_attentions');\n            const cross_attentions = getFlattened('cross_attentions');\n\n            return {\n                sequences,\n\n                decoder_attentions,\n                cross_attentions,\n            }\n        } else {\n            return sequences;\n        }\n    }\n\n    /**\n     * Helper function to add attentions to beam\n     * @param {Object} beam \n     * @param {Object} output\n     * @private \n     */\n    addAttentionsToBeam(beam, output) {\n        if (this.config.is_encoder_decoder) {\n            if (!output.cross_attentions || output.cross_attentions.length === 0) {\n                throw Error(\n                    \"`output_attentions` is true, but the model did not produce cross-attentions. \" +\n                    \"This is most likely because the model was not exported with `output_attentions=True`.\"\n                )\n            }\n            if (!beam.cross_attentions) {\n                beam.cross_attentions = [];\n            }\n            beam.cross_attentions.push(output.cross_attentions);\n        }\n\n        if (!output.decoder_attentions || output.decoder_attentions.length === 0) {\n            throw Error(\n                \"`output_attentions` is true, but the model did not produce decoder-attentions. \" +\n                \"This is most likely because the model was not exported with `output_attentions=True`.\"\n            )\n        }\n        if (!beam.decoder_attentions) {\n            beam.decoder_attentions = [];\n        }\n        beam.decoder_attentions.push(output.decoder_attentions);\n    }\n\n    /**\n     * Groups an array of beam objects by their ids.\n     *\n     * @param {Array} beams The array of beam objects to group.\n     * @returns {Array} An array of arrays, where each inner array contains beam objects with the same id.\n     */\n    groupBeams(beams) {\n        // Group beams by their ids\n        const groups = Object.create(null);\n        for (const obj of beams) {\n            if (groups[obj.id] === undefined) {\n                groups[obj.id] = [obj];\n            } else {\n                groups[obj.id].push(obj);\n            }\n        }\n\n        return Object.values(groups);\n    }\n\n    /**\n     * Returns an object containing past key values from the given decoder results object.\n     *\n     * @param {Object} decoderResults The decoder results object.\n     * @param {Object} pastKeyValues The previous past key values.\n     * @returns {Object} An object containing past key values.\n     */\n    getPastKeyValues(decoderResults, pastKeyValues) {\n\n        const pkvs = Object.create(null);\n\n        for (const name in decoderResults) {\n            if (name.startsWith('present')) {\n                let newName = name.replace('present', 'past_key_values');\n\n                if (pastKeyValues && name.includes('encoder')) {\n                    // Optimization introduced by optimum to reuse past key values. So, we just replace the constant\n                    // outputs with the previous past key values.\n                    // https://github.com/huggingface/optimum/blob/0bf2c05fb7e1182b52d21b703cfc95fd9e4ea3dc/optimum/onnxruntime/base.py#L677-L704\n                    pkvs[newName] = pastKeyValues[newName];\n                } else {\n                    pkvs[newName] = decoderResults[name];\n                }\n            }\n        }\n        return pkvs;\n    }\n\n    /**\n     * Returns an object containing attentions from the given decoder results object.\n     *\n     * @param {Object} decoderResults The decoder results object.\n     * @returns {Object} An object containing attentions.\n     */\n    getAttentions(decoderResults) {\n        const attns = Object.create(null);\n\n        for (const attnName of ['cross_attentions', 'decoder_attentions']) {\n            const result = [];\n            for (const name in decoderResults) {\n                if (name.startsWith(attnName)) {\n                    const index = name.split('.').pop()\n                    result[index] = decoderResults[name];\n                }\n            }\n            attns[attnName] = result;\n        }\n        return attns;\n    }\n\n    /**\n     * Adds past key values to the decoder feeds object. If pastKeyValues is null, creates new tensors for past key values.\n     *\n     * @param {Object} decoderFeeds The decoder feeds object to add past key values to.\n     * @param {Object} pastKeyValues An object containing past key values.\n     */\n    addPastKeyValues(decoderFeeds, pastKeyValues) {\n        if (pastKeyValues) {\n            Object.assign(decoderFeeds, pastKeyValues)\n        } else {\n            // TODO support batches (i.e., batch_size > 1)\n            const batch_size = 1;\n\n            // @ts-ignore\n            if (this.config.is_encoder_decoder && (this.add_encoder_pkv ?? true)) {\n                // @ts-ignore\n                let encoder_dims = [batch_size, this.num_encoder_heads, 0, this.encoder_dim_kv];\n                // @ts-ignore\n                let decoder_dims = [batch_size, this.num_decoder_heads, 0, this.decoder_dim_kv];\n                // @ts-ignore\n                for (let i = 0; i < this.num_decoder_layers; ++i) {\n                    decoderFeeds[`past_key_values.${i}.encoder.key`] = new Tensor('float32', [], encoder_dims)\n                    decoderFeeds[`past_key_values.${i}.encoder.value`] = new Tensor('float32', [], encoder_dims)\n                    decoderFeeds[`past_key_values.${i}.decoder.key`] = new Tensor('float32', [], decoder_dims)\n                    decoderFeeds[`past_key_values.${i}.decoder.value`] = new Tensor('float32', [], decoder_dims)\n                }\n            } else if (this.config.model_type === 'falcon') {\n                // NOTE: Custom implementation for Falcon\n                // @ts-ignore\n                let dims = [batch_size * this.num_heads, 0, this.dim_kv]\n                // @ts-ignore\n                for (let i = 0; i < this.num_layers; ++i) {\n                    decoderFeeds[`past_key_values.${i}.key`] = new Tensor('float32', [], dims)\n                    decoderFeeds[`past_key_values.${i}.value`] = new Tensor('float32', [], dims)\n                }\n            } else if (this.config.multi_query) { // e.g., for `gpt_bigcode`\n                // @ts-ignore\n                let dims = [batch_size * this.num_heads, 0, 2 * this.dim_kv]\n                // @ts-ignore\n                for (let i = 0; i < this.num_layers; ++i) {\n                    decoderFeeds[`past_key_values.${i}.key_value`] = new Tensor('float32', [], dims)\n                }\n            } else if (this.config.model_type === 'bloom') {\n                // NOTE: Custom implementation for Bloom\n\n                // @ts-ignore\n                let keyDims = [batch_size * this.num_heads, this.dim_kv, 0] // [batch_size x num_heads,64,past_sequence_length]\n                // @ts-ignore\n                let valueDims = [batch_size * this.num_heads, 0, this.dim_kv] // [batch_size x num_heads,past_sequence_length,64]\n                // @ts-ignore\n                for (let i = 0; i < this.num_layers; ++i) {\n                    decoderFeeds[`past_key_values.${i}.key`] = new Tensor('float32', [], keyDims)\n                    decoderFeeds[`past_key_values.${i}.value`] = new Tensor('float32', [], valueDims)\n                }\n            } else { // Decoder-only\n                // @ts-ignore\n                let dims = [batch_size, this.num_heads, 0, this.dim_kv]\n                // @ts-ignore\n                for (let i = 0; i < this.num_layers; ++i) {\n                    decoderFeeds[`past_key_values.${i}.key`] = new Tensor('float32', [], dims)\n                    decoderFeeds[`past_key_values.${i}.value`] = new Tensor('float32', [], dims)\n                }\n            }\n        }\n    }\n\n    /**\n     * Initializes and returns the beam for text generation task\n     * @param {Tensor} inputTokenIds The input token ids.\n     * @param {Object} generation_config The generation config.\n     * @param {number} numOutputTokens The number of tokens to be generated.\n     * @param {Tensor} inputs_attention_mask Optional input attention mask.\n     * @returns {any} A Beam object representing the initialized beam.\n     * @private\n     */\n    getStartBeams(inputTokenIds, generation_config, numOutputTokens, inputs_attention_mask) {\n        return this._getStartBeams(this, inputTokenIds, generation_config, numOutputTokens, inputs_attention_mask)\n    }\n\n    /**\n     * Runs a single step of the beam search generation algorithm.\n     * @param {any} beam The current beam being generated.\n     * @returns {Promise<any>} The updated beam after a single generation step.\n     * @private\n     */\n    async runBeam(beam) {\n        return await this._runBeam(this, beam);\n    }\n\n    /**\n     * Update a beam with a new token ID.\n     * @param {Object} beam The beam to update.\n     * @param {number} newTokenId The new token ID to add to the beam's output.\n     * @private\n     */\n    updateBeam(beam, newTokenId) {\n        return this._updateBeam(beam, newTokenId);\n    }\n}\n\n//////////////////////////////////////////////////\n// Base model output class\nexport class ModelOutput { }\n\n/**\n * Base class for model's outputs, with potential hidden states and attentions.\n */\nexport class BaseModelOutput extends ModelOutput {\n    /**\n     * @param {Object} output The output of the model.\n     * @param {Tensor} output.last_hidden_state Sequence of hidden-states at the output of the last layer of the model.\n     * @param {Tensor} [output.hidden_states] Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n     * @param {Tensor} [output.attentions] Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n     */\n    constructor({ last_hidden_state, hidden_states = null, attentions = null }) {\n        super();\n        this.last_hidden_state = last_hidden_state;\n        this.hidden_states = hidden_states;\n        this.attentions = attentions;\n    }\n}\n//////////////////////////////////////////////////\n// Bert models\nexport class BertPreTrainedModel extends PreTrainedModel { }\nexport class BertModel extends BertPreTrainedModel { }\n\n/**\n * BertForMaskedLM is a class representing a BERT model for masked language modeling.\n */\nexport class BertForMaskedLM extends BertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<MaskedLMOutput>} An object containing the model's output logits for masked language modeling.\n     */\n    async _call(model_inputs) {\n        return new MaskedLMOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * BertForSequenceClassification is a class representing a BERT model for sequence classification.\n */\nexport class BertForSequenceClassification extends BertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * BertForTokenClassification is a class representing a BERT model for token classification.\n */\nexport class BertForTokenClassification extends BertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.\n     */\n    async _call(model_inputs) {\n        return new TokenClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * BertForQuestionAnswering is a class representing a BERT model for question answering.\n */\nexport class BertForQuestionAnswering extends BertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<QuestionAnsweringModelOutput>} An object containing the model's output logits for question answering.\n     */\n    async _call(model_inputs) {\n        return new QuestionAnsweringModelOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// NomicBert models\nexport class NomicBertPreTrainedModel extends PreTrainedModel { }\nexport class NomicBertModel extends NomicBertPreTrainedModel { }\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// RoFormer models\nexport class RoFormerPreTrainedModel extends PreTrainedModel { }\n\n/**\n * The bare RoFormer Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class RoFormerModel extends RoFormerPreTrainedModel { }\n\n/**\n * RoFormer Model with a `language modeling` head on top.\n */\nexport class RoFormerForMaskedLM extends RoFormerPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<MaskedLMOutput>} An object containing the model's output logits for masked language modeling.\n     */\n    async _call(model_inputs) {\n        return new MaskedLMOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * RoFormer Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output)\n */\nexport class RoFormerForSequenceClassification extends RoFormerPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * RoFormer Model with a token classification head on top (a linear layer on top of the hidden-states output)\n * e.g. for Named-Entity-Recognition (NER) tasks.\n */\nexport class RoFormerForTokenClassification extends RoFormerPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.\n     */\n    async _call(model_inputs) {\n        return new TokenClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * RoFormer Model with a span classification head on top for extractive question-answering tasks like SQuAD\n * (a linear layers on top of the hidden-states output to compute `span start logits` and `span end logits`).\n */\nexport class RoFormerForQuestionAnswering extends RoFormerPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<QuestionAnsweringModelOutput>} An object containing the model's output logits for question answering.\n     */\n    async _call(model_inputs) {\n        return new QuestionAnsweringModelOutput(await super._call(model_inputs));\n    }\n}\n// TODO: Add RoFormerForCausalLM and RoFormerForMultipleChoice\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// ConvBert models\nexport class ConvBertPreTrainedModel extends PreTrainedModel { }\n\n/**\n * The bare ConvBERT Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class ConvBertModel extends ConvBertPreTrainedModel { }\n\n/**\n * ConvBERT Model with a language modeling head on top.\n */\nexport class ConvBertForMaskedLM extends ConvBertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<MaskedLMOutput>} An object containing the model's output logits for masked language modeling.\n     */\n    async _call(model_inputs) {\n        return new MaskedLMOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * ConvBERT Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output)\n */\nexport class ConvBertForSequenceClassification extends ConvBertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * ConvBERT Model with a token classification head on top (a linear layer on top of the hidden-states output)\n * e.g. for Named-Entity-Recognition (NER) tasks.\n */\nexport class ConvBertForTokenClassification extends ConvBertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.\n     */\n    async _call(model_inputs) {\n        return new TokenClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * ConvBERT Model with a span classification head on top for extractive question-answering tasks like SQuAD\n * (a linear layers on top of the hidden-states output to compute `span start logits` and `span end logits`)\n */\nexport class ConvBertForQuestionAnswering extends ConvBertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<QuestionAnsweringModelOutput>} An object containing the model's output logits for question answering.\n     */\n    async _call(model_inputs) {\n        return new QuestionAnsweringModelOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// Electra models\nexport class ElectraPreTrainedModel extends PreTrainedModel { }\n\n/**\n * The bare Electra Model transformer outputting raw hidden-states without any specific head on top.\n * Identical to the BERT model except that it uses an additional linear layer between the embedding\n * layer and the encoder if the hidden size and embedding size are different.\n */\nexport class ElectraModel extends ElectraPreTrainedModel { }\n// TODO add ElectraForPreTraining\n/**\n * Electra model with a language modeling head on top.\n */\nexport class ElectraForMaskedLM extends ElectraPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<MaskedLMOutput>} An object containing the model's output logits for masked language modeling.\n     */\n    async _call(model_inputs) {\n        return new MaskedLMOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * ELECTRA Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output)\n */\nexport class ElectraForSequenceClassification extends ElectraPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * Electra model with a token classification head on top.\n */\nexport class ElectraForTokenClassification extends ElectraPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.\n     */\n    async _call(model_inputs) {\n        return new TokenClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * LECTRA Model with a span classification head on top for extractive question-answering tasks like SQuAD\n * (a linear layers on top of the hidden-states output to compute `span start logits` and `span end logits`).\n */\nexport class ElectraForQuestionAnswering extends ElectraPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<QuestionAnsweringModelOutput>} An object containing the model's output logits for question answering.\n     */\n    async _call(model_inputs) {\n        return new QuestionAnsweringModelOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// CamemBERT models\nexport class CamembertPreTrainedModel extends PreTrainedModel { }\n\n/**\n * The bare CamemBERT Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class CamembertModel extends CamembertPreTrainedModel { }\n\n/**\n * CamemBERT Model with a `language modeling` head on top.\n */\nexport class CamembertForMaskedLM extends CamembertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<MaskedLMOutput>} An object containing the model's output logits for masked language modeling.\n     */\n    async _call(model_inputs) {\n        return new MaskedLMOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * CamemBERT Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output) e.g. for GLUE tasks.\n */\nexport class CamembertForSequenceClassification extends CamembertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * CamemBERT Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks.\n */\nexport class CamembertForTokenClassification extends CamembertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.\n     */\n    async _call(model_inputs) {\n        return new TokenClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * CamemBERT Model with a span classification head on top for extractive question-answering tasks\n */\nexport class CamembertForQuestionAnswering extends CamembertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<QuestionAnsweringModelOutput>} An object containing the model's output logits for question answering.\n     */\n    async _call(model_inputs) {\n        return new QuestionAnsweringModelOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// DeBERTa models\nexport class DebertaPreTrainedModel extends PreTrainedModel { }\n\n/**\n * The bare DeBERTa Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class DebertaModel extends DebertaPreTrainedModel { }\n\n/**\n * DeBERTa Model with a `language modeling` head on top.\n */\nexport class DebertaForMaskedLM extends DebertaPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<MaskedLMOutput>} An object containing the model's output logits for masked language modeling.\n     */\n    async _call(model_inputs) {\n        return new MaskedLMOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * DeBERTa Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output)\n */\nexport class DebertaForSequenceClassification extends DebertaPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * DeBERTa Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks.\n */\nexport class DebertaForTokenClassification extends DebertaPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.\n     */\n    async _call(model_inputs) {\n        return new TokenClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * DeBERTa Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear\n * layers on top of the hidden-states output to compute `span start logits` and `span end logits`).\n */\nexport class DebertaForQuestionAnswering extends DebertaPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<QuestionAnsweringModelOutput>} An object containing the model's output logits for question answering.\n     */\n    async _call(model_inputs) {\n        return new QuestionAnsweringModelOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// DeBERTa-v2 models\nexport class DebertaV2PreTrainedModel extends PreTrainedModel { }\n\n/**\n * The bare DeBERTa-V2 Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class DebertaV2Model extends DebertaV2PreTrainedModel { }\n\n/**\n * DeBERTa-V2 Model with a `language modeling` head on top.\n */\nexport class DebertaV2ForMaskedLM extends DebertaV2PreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<MaskedLMOutput>} An object containing the model's output logits for masked language modeling.\n     */\n    async _call(model_inputs) {\n        return new MaskedLMOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * DeBERTa-V2 Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output)\n */\nexport class DebertaV2ForSequenceClassification extends DebertaV2PreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * DeBERTa-V2 Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks.\n */\nexport class DebertaV2ForTokenClassification extends DebertaV2PreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.\n     */\n    async _call(model_inputs) {\n        return new TokenClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * DeBERTa-V2 Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear\n * layers on top of the hidden-states output to compute `span start logits` and `span end logits`).\n */\nexport class DebertaV2ForQuestionAnswering extends DebertaV2PreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<QuestionAnsweringModelOutput>} An object containing the model's output logits for question answering.\n     */\n    async _call(model_inputs) {\n        return new QuestionAnsweringModelOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// DistilBert models\nexport class DistilBertPreTrainedModel extends PreTrainedModel { }\nexport class DistilBertModel extends DistilBertPreTrainedModel { }\n\n/**\n * DistilBertForSequenceClassification is a class representing a DistilBERT model for sequence classification.\n */\nexport class DistilBertForSequenceClassification extends DistilBertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * DistilBertForTokenClassification is a class representing a DistilBERT model for token classification.\n */\nexport class DistilBertForTokenClassification extends DistilBertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.\n     */\n    async _call(model_inputs) {\n        return new TokenClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n\n/**\n * DistilBertForQuestionAnswering is a class representing a DistilBERT model for question answering.\n */\nexport class DistilBertForQuestionAnswering extends DistilBertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<QuestionAnsweringModelOutput>} An object containing the model's output logits for question answering.\n     */\n    async _call(model_inputs) {\n        return new QuestionAnsweringModelOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * DistilBertForMaskedLM is a class representing a DistilBERT model for masking task.\n */\nexport class DistilBertForMaskedLM extends DistilBertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<MaskedLMOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new MaskedLMOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// ESM models\nexport class EsmPreTrainedModel extends PreTrainedModel { }\n\n/**\n * The bare ESM Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class EsmModel extends EsmPreTrainedModel { }\n\n/**\n * ESM Model with a `language modeling` head on top.\n */\nexport class EsmForMaskedLM extends EsmPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<MaskedLMOutput>} An object containing the model's output logits for masked language modeling.\n     */\n    async _call(model_inputs) {\n        return new MaskedLMOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * ESM Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output)\n */\nexport class EsmForSequenceClassification extends EsmPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * ESM Model with a token classification head on top (a linear layer on top of the hidden-states output)\n * e.g. for Named-Entity-Recognition (NER) tasks.\n */\nexport class EsmForTokenClassification extends EsmPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.\n     */\n    async _call(model_inputs) {\n        return new TokenClassifierOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// MobileBert models\nexport class MobileBertPreTrainedModel extends PreTrainedModel { }\nexport class MobileBertModel extends MobileBertPreTrainedModel { }\n\n/**\n * MobileBertForMaskedLM is a class representing a MobileBERT model for masking task.\n */\nexport class MobileBertForMaskedLM extends MobileBertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<MaskedLMOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new MaskedLMOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * MobileBert Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output)\n */\nexport class MobileBertForSequenceClassification extends MobileBertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * MobileBert Model with a span classification head on top for extractive question-answering tasks\n */\nexport class MobileBertForQuestionAnswering extends MobileBertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<QuestionAnsweringModelOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new QuestionAnsweringModelOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// MPNet models\nexport class MPNetPreTrainedModel extends PreTrainedModel { }\n\n/**\n * The bare MPNet Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class MPNetModel extends MPNetPreTrainedModel { }\n\n/**\n * MPNetForMaskedLM is a class representing a MPNet model for masked language modeling.\n */\nexport class MPNetForMaskedLM extends MPNetPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<MaskedLMOutput>} An object containing the model's output logits for masked language modeling.\n     */\n    async _call(model_inputs) {\n        return new MaskedLMOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * MPNetForSequenceClassification is a class representing a MPNet model for sequence classification.\n */\nexport class MPNetForSequenceClassification extends MPNetPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * MPNetForTokenClassification is a class representing a MPNet model for token classification.\n */\nexport class MPNetForTokenClassification extends MPNetPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.\n     */\n    async _call(model_inputs) {\n        return new TokenClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * MPNetForQuestionAnswering is a class representing a MPNet model for question answering.\n */\nexport class MPNetForQuestionAnswering extends MPNetPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<QuestionAnsweringModelOutput>} An object containing the model's output logits for question answering.\n     */\n    async _call(model_inputs) {\n        return new QuestionAnsweringModelOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// SqueezeBert models\nexport class SqueezeBertPreTrainedModel extends PreTrainedModel { }\nexport class SqueezeBertModel extends SqueezeBertPreTrainedModel { }\nexport class SqueezeBertForMaskedLM extends SqueezeBertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<MaskedLMOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new MaskedLMOutput(await super._call(model_inputs));\n    }\n}\nexport class SqueezeBertForSequenceClassification extends SqueezeBertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\nexport class SqueezeBertForQuestionAnswering extends SqueezeBertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<QuestionAnsweringModelOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new QuestionAnsweringModelOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// Albert models\nexport class AlbertPreTrainedModel extends PreTrainedModel { }\nexport class AlbertModel extends AlbertPreTrainedModel { }\nexport class AlbertForSequenceClassification extends AlbertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\nexport class AlbertForQuestionAnswering extends AlbertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<QuestionAnsweringModelOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new QuestionAnsweringModelOutput(await super._call(model_inputs));\n    }\n}\nexport class AlbertForMaskedLM extends AlbertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<MaskedLMOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new MaskedLMOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// T5 models\nexport class T5PreTrainedModel extends PreTrainedModel { };\n\nexport class T5Model extends T5PreTrainedModel { }\n\n/**\n * T5Model is a class representing a T5 model for conditional generation.\n */\nexport class T5ForConditionalGeneration extends T5PreTrainedModel {\n\n    /**\n     * Creates a new instance of the `T5ForConditionalGeneration` class.\n     * @param {Object} config The model configuration.\n     * @param {any} session session for the model.\n     * @param {any} decoder_merged_session session for the decoder.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, decoder_merged_session, generation_config) {\n        super(config, session);\n        this.decoder_merged_session = decoder_merged_session;\n        this.generation_config = generation_config;\n\n        this.num_decoder_layers = this.config.num_decoder_layers;\n        this.num_decoder_heads = this.config.num_heads;\n        this.decoder_dim_kv = this.config.d_kv;\n\n        this.num_encoder_layers = this.config.num_layers;\n        this.num_encoder_heads = this.config.num_heads;\n        this.encoder_dim_kv = this.config.d_kv;\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// LONGT5 models\n/**\n * An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained models.\n */\nexport class LongT5PreTrainedModel extends PreTrainedModel { };\n\n/**\n * The bare LONGT5 Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class LongT5Model extends LongT5PreTrainedModel { }\n\n/**\n * LONGT5 Model with a `language modeling` head on top.\n */\nexport class LongT5ForConditionalGeneration extends LongT5PreTrainedModel {\n    /**\n     * Creates a new instance of the `LongT5ForConditionalGeneration` class.\n     * @param {Object} config The model configuration.\n     * @param {any} session session for the model.\n     * @param {any} decoder_merged_session session for the decoder.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, decoder_merged_session, generation_config) {\n        super(config, session);\n        this.decoder_merged_session = decoder_merged_session;\n        this.generation_config = generation_config;\n\n        this.num_decoder_layers = this.config.num_decoder_layers;\n        this.num_decoder_heads = this.config.num_heads;\n        this.decoder_dim_kv = this.config.d_kv;\n\n        this.num_encoder_layers = this.config.num_layers;\n        this.num_encoder_heads = this.config.num_heads;\n        this.encoder_dim_kv = this.config.d_kv;\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// MT5 models\nexport class MT5PreTrainedModel extends PreTrainedModel { };\n\nexport class MT5Model extends MT5PreTrainedModel { }\n\n/**\n * A class representing a conditional sequence-to-sequence model based on the MT5 architecture.\n */\nexport class MT5ForConditionalGeneration extends MT5PreTrainedModel {\n\n    /**\n     * Creates a new instance of the `MT5ForConditionalGeneration` class.\n     * @param {any} config The model configuration.\n     * @param {any} session The ONNX session containing the encoder weights.\n     * @param {any} decoder_merged_session The ONNX session containing the merged decoder weights.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, decoder_merged_session, generation_config) {\n        super(config, session);\n        this.decoder_merged_session = decoder_merged_session;\n        this.generation_config = generation_config;\n\n        this.num_decoder_layers = this.config.num_decoder_layers;\n        this.num_decoder_heads = this.config.num_heads;\n        this.decoder_dim_kv = this.config.d_kv;\n\n        this.num_encoder_layers = this.config.num_layers;\n        this.num_encoder_heads = this.config.num_heads;\n        this.encoder_dim_kv = this.config.d_kv;\n    }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// Bart models\nexport class BartPretrainedModel extends PreTrainedModel { };\n\n/**\n * The bare BART Model outputting raw hidden-states without any specific head on top.\n */\nexport class BartModel extends BartPretrainedModel { }\n\n/**\n * The BART Model with a language modeling head. Can be used for summarization.\n */\nexport class BartForConditionalGeneration extends BartPretrainedModel {\n\n    /**\n     * Creates a new instance of the `BartForConditionalGeneration` class.\n     * @param {Object} config The configuration object for the Bart model.\n     * @param {Object} session The ONNX session used to execute the model.\n     * @param {Object} decoder_merged_session The ONNX session used to execute the decoder.\n     * @param {Object} generation_config The generation configuration object.\n     */\n    constructor(config, session, decoder_merged_session, generation_config) {\n        super(config, session);\n        this.decoder_merged_session = decoder_merged_session;\n        this.generation_config = generation_config;\n\n        this.num_decoder_layers = this.config.decoder_layers;\n        this.num_decoder_heads = this.config.decoder_attention_heads;\n        this.decoder_dim_kv = this.config.d_model / this.num_decoder_heads;\n\n        this.num_encoder_layers = this.config.encoder_layers;\n        this.num_encoder_heads = this.config.encoder_attention_heads;\n        this.encoder_dim_kv = this.config.d_model / this.num_encoder_heads;\n    }\n\n}\n\n/**\n * Bart model with a sequence classification/head on top (a linear layer on top of the pooled output)\n */\nexport class BartForSequenceClassification extends BartPretrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// MBart models\nexport class MBartPreTrainedModel extends PreTrainedModel { };\n\n/**\n * The bare MBART Model outputting raw hidden-states without any specific head on top.\n */\nexport class MBartModel extends MBartPreTrainedModel { }\n\n/**\n * The MBART Model with a language modeling head. Can be used for summarization, after fine-tuning the pretrained models.\n */\nexport class MBartForConditionalGeneration extends MBartPreTrainedModel {\n\n    /**\n     * Creates a new instance of the `MBartForConditionalGeneration` class.\n     * @param {Object} config The configuration object for the Bart model.\n     * @param {Object} session The ONNX session used to execute the model.\n     * @param {Object} decoder_merged_session The ONNX session used to execute the decoder.\n     * @param {Object} generation_config The generation configuration object.\n     */\n    constructor(config, session, decoder_merged_session, generation_config) {\n        super(config, session);\n        this.decoder_merged_session = decoder_merged_session;\n        this.generation_config = generation_config;\n\n        this.num_decoder_layers = this.config.decoder_layers;\n        this.num_decoder_heads = this.config.decoder_attention_heads;\n        this.decoder_dim_kv = this.config.d_model / this.num_decoder_heads;\n\n        this.num_encoder_layers = this.config.encoder_layers;\n        this.num_encoder_heads = this.config.encoder_attention_heads;\n        this.encoder_dim_kv = this.config.d_model / this.num_encoder_heads;\n    }\n\n}\n\n/**\n * MBart model with a sequence classification/head on top (a linear layer on top of the pooled output).\n */\nexport class MBartForSequenceClassification extends MBartPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n\nexport class MBartForCausalLM extends MBartPreTrainedModel {\n    /**\n     * Creates a new instance of the `MBartForCausalLM` class.\n     * @param {Object} config Configuration object for the model.\n     * @param {Object} decoder_merged_session ONNX Session object for the decoder.\n     * @param {Object} generation_config Configuration object for the generation process.\n     */\n    constructor(config, decoder_merged_session, generation_config) {\n        super(config, decoder_merged_session);\n        this.generation_config = generation_config;\n\n        this.num_decoder_layers = this.config.decoder_layers;\n        this.num_decoder_heads = this.config.decoder_attention_heads;\n        this.decoder_dim_kv = this.config.d_model / this.num_decoder_heads;\n\n        this.num_encoder_layers = this.config.encoder_layers;\n        this.num_encoder_heads = this.config.encoder_attention_heads;\n        this.encoder_dim_kv = this.config.d_model / this.num_encoder_heads;\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// Blenderbot models\nexport class BlenderbotPreTrainedModel extends PreTrainedModel { };\n\n/**\n * The bare Blenderbot Model outputting raw hidden-states without any specific head on top.\n */\nexport class BlenderbotModel extends BlenderbotPreTrainedModel { }\n\n/**\n * The Blenderbot Model with a language modeling head. Can be used for summarization.\n */\nexport class BlenderbotForConditionalGeneration extends BlenderbotPreTrainedModel {\n\n    /**\n     * Creates a new instance of the `BlenderbotForConditionalGeneration` class.\n     * @param {any} config The model configuration.\n     * @param {any} session The ONNX session containing the encoder weights.\n     * @param {any} decoder_merged_session The ONNX session containing the merged decoder weights.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, decoder_merged_session, generation_config) {\n        super(config, session);\n        this.decoder_merged_session = decoder_merged_session;\n        this.generation_config = generation_config;\n\n        this.num_decoder_layers = this.config.decoder_layers;\n        this.num_decoder_heads = this.config.decoder_attention_heads;\n        this.decoder_dim_kv = this.config.d_model / this.num_decoder_heads;\n\n        this.num_encoder_layers = this.config.encoder_layers;\n        this.num_encoder_heads = this.config.encoder_attention_heads;\n        this.encoder_dim_kv = this.config.d_model / this.num_encoder_heads;\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// Blenderbot models\nexport class BlenderbotSmallPreTrainedModel extends PreTrainedModel { };\n\n/**\n * The bare BlenderbotSmall Model outputting raw hidden-states without any specific head on top.\n */\nexport class BlenderbotSmallModel extends BlenderbotSmallPreTrainedModel { }\n\n/**\n * The BlenderbotSmall Model with a language modeling head. Can be used for summarization.\n */\nexport class BlenderbotSmallForConditionalGeneration extends BlenderbotSmallPreTrainedModel {\n\n    /**\n     * Creates a new instance of the `BlenderbotForConditionalGeneration` class.\n     * @param {any} config The model configuration.\n     * @param {any} session The ONNX session containing the encoder weights.\n     * @param {any} decoder_merged_session The ONNX session containing the merged decoder weights.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, decoder_merged_session, generation_config) {\n        super(config, session);\n        this.decoder_merged_session = decoder_merged_session;\n        this.generation_config = generation_config;\n\n        this.num_decoder_layers = this.config.decoder_layers;\n        this.num_decoder_heads = this.config.decoder_attention_heads;\n        this.decoder_dim_kv = this.config.d_model / this.num_decoder_heads;\n\n        this.num_encoder_layers = this.config.encoder_layers;\n        this.num_encoder_heads = this.config.encoder_attention_heads;\n        this.encoder_dim_kv = this.config.d_model / this.num_encoder_heads;\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// Roberta models\nexport class RobertaPreTrainedModel extends PreTrainedModel { }\nexport class RobertaModel extends RobertaPreTrainedModel { }\n\n/**\n * RobertaForMaskedLM class for performing masked language modeling on Roberta models.\n */\nexport class RobertaForMaskedLM extends RobertaPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<MaskedLMOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new MaskedLMOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * RobertaForSequenceClassification class for performing sequence classification on Roberta models.\n */\nexport class RobertaForSequenceClassification extends RobertaPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * RobertaForTokenClassification class for performing token classification on Roberta models.\n */\nexport class RobertaForTokenClassification extends RobertaPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.\n     */\n    async _call(model_inputs) {\n        return new TokenClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * RobertaForQuestionAnswering class for performing question answering on Roberta models.\n */\nexport class RobertaForQuestionAnswering extends RobertaPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<QuestionAnsweringModelOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new QuestionAnsweringModelOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// XLM models\n/**\n * An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained models.\n */\nexport class XLMPreTrainedModel extends PreTrainedModel { }\n\n/**\n * The bare XLM Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class XLMModel extends XLMPreTrainedModel { }\n\n/**\n * The XLM Model transformer with a language modeling head on top (linear layer with weights tied to the input embeddings).\n */\nexport class XLMWithLMHeadModel extends XLMPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<MaskedLMOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new MaskedLMOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * XLM Model with a sequence classification/regression head on top (a linear layer on top of the pooled output)\n */\nexport class XLMForSequenceClassification extends XLMPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * XLM Model with a token classification head on top (a linear layer on top of the hidden-states output)\n */\nexport class XLMForTokenClassification extends XLMPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.\n     */\n    async _call(model_inputs) {\n        return new TokenClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * XLM Model with a span classification head on top for extractive question-answering tasks\n */\nexport class XLMForQuestionAnswering extends XLMPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<QuestionAnsweringModelOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new QuestionAnsweringModelOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// XLMRoberta models\nexport class XLMRobertaPreTrainedModel extends PreTrainedModel { }\nexport class XLMRobertaModel extends XLMRobertaPreTrainedModel { }\n\n/**\n * XLMRobertaForMaskedLM class for performing masked language modeling on XLMRoberta models.\n */\nexport class XLMRobertaForMaskedLM extends XLMRobertaPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<MaskedLMOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new MaskedLMOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * XLMRobertaForSequenceClassification class for performing sequence classification on XLMRoberta models.\n */\nexport class XLMRobertaForSequenceClassification extends XLMRobertaPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * XLMRobertaForTokenClassification class for performing token classification on XLMRoberta models.\n */\nexport class XLMRobertaForTokenClassification extends XLMRobertaPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.\n     */\n    async _call(model_inputs) {\n        return new TokenClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * XLMRobertaForQuestionAnswering class for performing question answering on XLMRoberta models.\n */\nexport class XLMRobertaForQuestionAnswering extends XLMRobertaPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<QuestionAnsweringModelOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new QuestionAnsweringModelOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// Audio Spectrogram Transformer (AST) models\nexport class ASTPreTrainedModel extends PreTrainedModel { };\n\n/**\n * The bare AST Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class ASTModel extends ASTPreTrainedModel { }\n\n/**\n * Audio Spectrogram Transformer model with an audio classification head on top\n * (a linear layer on top of the pooled output) e.g. for datasets like AudioSet, Speech Commands v2.\n */\nexport class ASTForAudioClassification extends ASTPreTrainedModel { }\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// Whisper models\nexport class WhisperPreTrainedModel extends PreTrainedModel { };\n\n/**\n * WhisperModel class for training Whisper models without a language model head.\n */\nexport class WhisperModel extends WhisperPreTrainedModel { }\n\n/**\n * WhisperForConditionalGeneration class for generating conditional outputs from Whisper models.\n */\nexport class WhisperForConditionalGeneration extends WhisperPreTrainedModel {\n\n    requires_attention_mask = false;\n    main_input_name = 'input_features';\n\n    /**\n     * Creates a new instance of the `WhisperForConditionalGeneration` class.\n     * @param {Object} config Configuration object for the model.\n     * @param {Object} session ONNX Session object for the model.\n     * @param {Object} decoder_merged_session ONNX Session object for the decoder.\n     * @param {Object} generation_config Configuration object for the generation process.\n     */\n    constructor(config, session, decoder_merged_session, generation_config) {\n        super(config, session);\n        this.decoder_merged_session = decoder_merged_session;\n        this.generation_config = generation_config;\n\n        this.num_decoder_layers = this.config.decoder_layers;\n        this.num_decoder_heads = this.config.decoder_attention_heads;\n        this.decoder_dim_kv = this.config.d_model / this.num_decoder_heads;\n\n        this.num_encoder_layers = this.config.encoder_layers;\n        this.num_encoder_heads = this.config.encoder_attention_heads;\n        this.encoder_dim_kv = this.config.d_model / this.num_encoder_heads;\n    }\n\n    /**\n     * @typedef {Object} WhisperGenerationConfig\n     * @extends GenerationConfig\n     * @property {boolean} [return_timestamps=null] Whether to return the timestamps with the text. This enables the `WhisperTimestampsLogitsProcessor`.\n     * @property {boolean} [return_token_timestamps=null] Whether to return token-level timestamps\n     * with the text. This can be used with or without the `return_timestamps` option. To get word-level\n     * timestamps, use the tokenizer to group the tokens into words.\n     * @property {number} [num_frames=null]  The number of audio frames available in this chunk. This is only used generating word-level timestamps.\n     */\n\n    /**\n     * Generates outputs based on input and generation configuration.\n     * @param {Object} inputs Input data for the model.\n     * @param {WhisperGenerationConfig} generation_config Configuration object for the generation process.\n     * @param {Object} logits_processor Optional logits processor object.\n     * @returns {Promise<Object>} Promise object represents the generated outputs.\n     */\n    async generate(\n        inputs,\n        generation_config = null,\n        logits_processor = null,\n        // {\n        //     return_timestamps = null,\n        //     return_token_timestamps = null,\n        //     language = null,\n        //     task = null,\n        // } = {},\n    ) {\n        // Create generation config object\n        generation_config = this._get_generation_config(generation_config);\n\n\n        // Whisper has additional options for returning timestamps\n        generation_config.return_timestamps ??= false;\n\n        // TODO add language and task\n\n        if (generation_config.return_timestamps) {\n            logits_processor = [new WhisperTimeStampLogitsProcessor(generation_config)]\n        }\n\n        if (generation_config.return_token_timestamps) {\n            generation_config.output_attentions = true;\n            generation_config.return_dict_in_generate = true;\n\n            if (generation_config.task === 'translate') {\n                console.warn(\"Token-level timestamps may not be reliable for task 'translate'.\")\n            }\n\n            if (!generation_config.alignment_heads) {\n                throw new Error(\n                    \"Model generation config has no `alignment_heads`, token-level timestamps not available. \" +\n                    \"See https://gist.github.com/hollance/42e32852f24243b748ae6bc1f985b13a on how to add this property to the generation config.\"\n                )\n            }\n        }\n\n        const outputs = await super.generate(inputs, generation_config, logits_processor);\n\n        if (generation_config.return_token_timestamps && generation_config.alignment_heads) {\n            outputs[\"token_timestamps\"] = this._extract_token_timestamps(\n                outputs,\n                generation_config.alignment_heads,\n                generation_config.num_frames,\n            )\n        }\n\n        return outputs\n    }\n\n    /**\n     * Calculates token-level timestamps using the encoder-decoder cross-attentions and\n     * dynamic time-warping (DTW) to map each output token to a position in the input audio.\n     * @param {Object} generate_outputs Outputs generated by the model\n     * @param {Tensor[][][]} generate_outputs.cross_attentions The cross attentions output by the model\n     * @param {Tensor[][][]} generate_outputs.decoder_attentions The decoder attentions output by the model\n     * @param {number[][]} generate_outputs.sequences The sequences output by the model\n     * @param {number[][]} alignment_heads Alignment heads of the model\n     * @param {number} [num_frames=null] Number of frames in the input audio.\n     * @param {number} [time_precision=0.02] Precision of the timestamps in seconds\n     * @returns {Tensor} tensor containing the timestamps in seconds for each predicted token\n     */\n    _extract_token_timestamps(generate_outputs, alignment_heads, num_frames = null, time_precision = 0.02) {\n        if (!generate_outputs.cross_attentions) {\n            throw new Error(\n                \"Model outputs must contain cross attentions to extract timestamps. \" +\n                \"This is most likely because the model was not exported with `output_attentions=True`.\"\n            )\n        }\n\n        let median_filter_width = this.config.median_filter_width;\n        if (median_filter_width === undefined) {\n            console.warn(\"Model config has no `median_filter_width`, using default value of 7.\")\n            median_filter_width = 7;\n        }\n\n        const batchedMatrices = generate_outputs.cross_attentions.map(batch => {\n            // Create a list with `decoder_layers` elements, each a tensor of shape\n            // (batch size, attention_heads, output length, input length).\n            let cross_attentions = Array.from({ length: this.config.decoder_layers },\n                (_, i) => cat(batch.map(x => x[i]), 2)\n            );\n\n            let weights = stack(alignment_heads.map(([l, h]) => {\n                return num_frames\n                    ? cross_attentions[l].slice(null, h, null, [0, num_frames])\n                    : cross_attentions[l].slice(null, h);\n            }));\n            weights = weights.transpose(1, 0, 2, 3)\n\n            let [std, calculatedMean] = std_mean(weights, -2, 0, true);\n\n            // Normalize and smoothen the weights.\n            let smoothedWeights = weights.clone(); // [1, 8, seqLength, 1500]\n\n            for (let a = 0; a < smoothedWeights.dims[0]; ++a) {\n                let aTensor = smoothedWeights[a]; // [8, seqLength, 1500]\n\n                for (let b = 0; b < aTensor.dims[0]; ++b) {\n                    let bTensor = aTensor[b]; // [seqLength, 1500]\n\n                    const stdTensor = std[a][b][0]; // [1500]\n                    const meanTensor = calculatedMean[a][b][0]; // [1500]\n\n                    for (let c = 0; c < bTensor.dims[0]; ++c) {\n\n                        let cTensor = bTensor[c]; // [1500]\n                        for (let d = 0; d < cTensor.data.length; ++d) {\n                            cTensor.data[d] = (cTensor.data[d] - meanTensor.data[d]) / stdTensor.data[d]\n                        }\n\n                        // Apply median filter.\n                        cTensor.data.set(medianFilter(cTensor.data, median_filter_width))\n                    }\n                }\n            }\n\n            // Average the different cross-attention heads.\n            const matrix = mean(smoothedWeights, 1);\n            return matrix;\n        });\n\n        const timestampsShape = [generate_outputs.sequences.length, generate_outputs.sequences[0].length];\n\n        const timestamps = new Tensor(\n            'float32',\n            new Float32Array(timestampsShape[0] * timestampsShape[1]),\n            timestampsShape\n        );\n\n        // Perform dynamic time warping on each element of the batch.\n        for (let batch_idx = 0; batch_idx < timestampsShape[0]; ++batch_idx) {\n            // NOTE: Since we run only one batch at a time, we can squeeze to get the same dimensions\n            // as the python implementation\n            const matrix = batchedMatrices[batch_idx].neg().squeeze_(0);\n            let [text_indices, time_indices] = dynamicTimeWarping(matrix);\n\n            let diffs = Array.from({ length: text_indices.length - 1 }, (v, i) => text_indices[i + 1] - text_indices[i]);\n            let jumps = mergeArrays([1], diffs).map(x => !!x); // convert to boolean\n\n            let jump_times = [];\n            for (let i = 0; i < jumps.length; ++i) {\n                if (jumps[i]) {\n                    jump_times.push(time_indices[i] * time_precision);\n                    // NOTE: No point in rounding here, since we set to Float32Array later\n                }\n            }\n            timestamps[batch_idx].data.set(jump_times, 1)\n        }\n\n        return timestamps;\n    }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n/**\n * Vision Encoder-Decoder model based on OpenAI's GPT architecture for image captioning and other vision tasks\n */\nexport class VisionEncoderDecoderModel extends PreTrainedModel {\n    main_input_name = 'pixel_values';\n\n    /**\n     * Creates a new instance of the `VisionEncoderDecoderModel` class.\n     * @param {Object} config The configuration object specifying the hyperparameters and other model settings.\n     * @param {Object} session The ONNX session containing the encoder model.\n     * @param {any} decoder_merged_session The ONNX session containing the merged decoder model.\n     * @param {Object} generation_config Configuration object for the generation process.\n     */\n    constructor(config, session, decoder_merged_session, generation_config) {\n        super(config, session);\n        this.decoder_merged_session = decoder_merged_session;\n        this.generation_config = generation_config;\n\n        // Extract configs\n        const encoderConfig = this.config.encoder;\n        const decoderConfig = this.config.decoder;\n\n        // Validate encoder\n        const encoderModelType = encoderConfig.model_type;\n        const encoderModel =\n            MODEL_MAPPING_NAMES_ENCODER_ONLY.get(encoderModelType)\n            ?? MODEL_MAPPING_NAMES_ENCODER_DECODER.get(encoderModelType);\n        if (!encoderModel) {\n            console.warn(`Model type for encoder '${encoderModelType}' not found, assuming encoder-only architecture. Please report this at https://github.com/xenova/transformers.js/issues/new/choose.`);\n        }\n\n        // Validate decoder\n        const decoderModel = MODEL_WITH_LM_HEAD_MAPPING_NAMES.get(decoderConfig.model_type);\n        if (!decoderModel) {\n            throw new Error(`Unable to construct \\`VisionEncoderDecoder\\` due to unsupported decoder: \"${this.config.decoder.model_type}\"`);\n        }\n\n        // @ts-ignore\n        const decoderModelClass = decoderModel[1];\n        // @ts-ignore\n        const decoder = new decoderModelClass(decoderConfig, decoder_merged_session, generation_config);\n\n        this.add_encoder_pkv = 'num_decoder_layers' in decoder;\n        if (this.add_encoder_pkv) {\n            // Decoder is part of an encoder-decoder model\n            this.num_decoder_layers = decoder.num_decoder_layers;\n            this.num_decoder_heads = decoder.num_decoder_heads;\n            this.decoder_dim_kv = decoder.decoder_dim_kv;\n\n            this.num_encoder_layers = decoder.num_encoder_layers;\n            this.num_encoder_heads = decoder.num_encoder_heads;\n            this.encoder_dim_kv = decoder.encoder_dim_kv;\n\n        } else {\n            // Decoder is a decoder-only model\n            this.num_layers = decoder.num_layers;\n            this.num_heads = decoder.num_heads;\n            this.dim_kv = decoder.dim_kv;\n        }\n    }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// CLIP models\nexport class CLIPPreTrainedModel extends PreTrainedModel { }\n\n/**\n * CLIP Text and Vision Model with a projection layers on top\n * \n * **Example:** Perform zero-shot image classification with a `CLIPModel`.\n * \n * ```javascript\n * import { AutoTokenizer, AutoProcessor, CLIPModel, RawImage } from '@xenova/transformers';\n * \n * // Load tokenizer, processor, and model\n * let tokenizer = await AutoTokenizer.from_pretrained('Xenova/clip-vit-base-patch16');\n * let processor = await AutoProcessor.from_pretrained('Xenova/clip-vit-base-patch16');\n * let model = await CLIPModel.from_pretrained('Xenova/clip-vit-base-patch16');\n * \n * // Run tokenization\n * let texts = ['a photo of a car', 'a photo of a football match']\n * let text_inputs = tokenizer(texts, { padding: true, truncation: true });\n * \n * // Read image and run processor\n * let image = await RawImage.read('https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/football-match.jpg');\n * let image_inputs = await processor(image);\n * \n * // Run model with both text and pixel inputs\n * let output = await model({ ...text_inputs, ...image_inputs });\n * // {\n * //   logits_per_image: Tensor {\n * //     dims: [ 1, 2 ],\n * //     data: Float32Array(2) [ 18.579734802246094, 24.31830596923828 ],\n * //   },\n * //   logits_per_text: Tensor {\n * //     dims: [ 2, 1 ],\n * //     data: Float32Array(2) [ 18.579734802246094, 24.31830596923828 ],\n * //   },\n * //   text_embeds: Tensor {\n * //     dims: [ 2, 512 ],\n * //     data: Float32Array(1024) [ ... ],\n * //   },\n * //   image_embeds: Tensor {\n * //     dims: [ 1, 512 ],\n * //     data: Float32Array(512) [ ... ],\n * //   }\n * // }\n * ```\n */\nexport class CLIPModel extends CLIPPreTrainedModel { }\n\n/**\n * CLIP Text Model with a projection layer on top (a linear layer on top of the pooled output)\n * \n * **Example:** Compute text embeddings with `CLIPTextModelWithProjection`.\n * \n * ```javascript\n * import { AutoTokenizer, CLIPTextModelWithProjection } from '@xenova/transformers';\n * \n * // Load tokenizer and text model\n * const tokenizer = await AutoTokenizer.from_pretrained('Xenova/clip-vit-base-patch16');\n * const text_model = await CLIPTextModelWithProjection.from_pretrained('Xenova/clip-vit-base-patch16');\n * \n * // Run tokenization\n * let texts = ['a photo of a car', 'a photo of a football match'];\n * let text_inputs = tokenizer(texts, { padding: true, truncation: true });\n * \n * // Compute embeddings\n * const { text_embeds } = await text_model(text_inputs);\n * // Tensor {\n * //   dims: [ 2, 512 ],\n * //   type: 'float32',\n * //   data: Float32Array(1024) [ ... ],\n * //   size: 1024\n * // }\n * ```\n */\nexport class CLIPTextModelWithProjection extends CLIPPreTrainedModel {\n\n    /** @type {PreTrainedModel.from_pretrained} */\n    static async from_pretrained(pretrained_model_name_or_path, options = {}) {\n        // Update default model file name if not provided\n        options.model_file_name ??= 'text_model';\n        return super.from_pretrained(pretrained_model_name_or_path, options);\n    }\n}\n\n/**\n * CLIP Vision Model with a projection layer on top (a linear layer on top of the pooled output)\n * \n * **Example:** Compute vision embeddings with `CLIPVisionModelWithProjection`.\n * \n * ```javascript\n * import { AutoProcessor, CLIPVisionModelWithProjection, RawImage} from '@xenova/transformers';\n * \n * // Load processor and vision model\n * const processor = await AutoProcessor.from_pretrained('Xenova/clip-vit-base-patch16');\n * const vision_model = await CLIPVisionModelWithProjection.from_pretrained('Xenova/clip-vit-base-patch16');\n * \n * // Read image and run processor\n * let image = await RawImage.read('https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/football-match.jpg');\n * let image_inputs = await processor(image);\n * \n * // Compute embeddings\n * const { image_embeds } = await vision_model(image_inputs);\n * // Tensor {\n * //   dims: [ 1, 512 ],\n * //   type: 'float32',\n * //   data: Float32Array(512) [ ... ],\n * //   size: 512\n * // }\n * ```\n */\nexport class CLIPVisionModelWithProjection extends CLIPPreTrainedModel {\n    /** @type {PreTrainedModel.from_pretrained} */\n    static async from_pretrained(pretrained_model_name_or_path, options = {}) {\n        // Update default model file name if not provided\n        options.model_file_name ??= 'vision_model';\n        return super.from_pretrained(pretrained_model_name_or_path, options);\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// SigLIP models\nexport class SiglipPreTrainedModel extends PreTrainedModel { }\n\n/**\n * SigLIP Text and Vision Model with a projection layers on top\n * \n * **Example:** Perform zero-shot image classification with a `SiglipModel`.\n * \n * ```javascript\n * import { AutoTokenizer, AutoProcessor, SiglipModel, RawImage } from '@xenova/transformers';\n * \n * // Load tokenizer, processor, and model\n * const tokenizer = await AutoTokenizer.from_pretrained('Xenova/siglip-base-patch16-224');\n * const processor = await AutoProcessor.from_pretrained('Xenova/siglip-base-patch16-224');\n * const model = await SiglipModel.from_pretrained('Xenova/siglip-base-patch16-224');\n * \n * // Run tokenization\n * const texts = ['a photo of 2 cats', 'a photo of 2 dogs'];\n * const text_inputs = tokenizer(texts, { padding: 'max_length', truncation: true });\n * \n * // Read image and run processor\n * const image = await RawImage.read('http://images.cocodataset.org/val2017/000000039769.jpg');\n * const image_inputs = await processor(image);\n * \n * // Run model with both text and pixel inputs\n * const output = await model({ ...text_inputs, ...image_inputs });\n * // {\n * //   logits_per_image: Tensor {\n * //     dims: [ 1, 2 ],\n * //     data: Float32Array(2) [ -1.6019744873046875, -10.720091819763184 ],\n * //   },\n * //   logits_per_text: Tensor {\n * //     dims: [ 2, 1 ],\n * //     data: Float32Array(2) [ -1.6019744873046875, -10.720091819763184 ],\n * //   },\n * //   text_embeds: Tensor {\n * //     dims: [ 2, 768 ],\n * //     data: Float32Array(1536) [ ... ],\n * //   },\n * //   image_embeds: Tensor {\n * //     dims: [ 1, 768 ],\n * //     data: Float32Array(768) [ ... ],\n * //   }\n * // }\n * ```\n */\nexport class SiglipModel extends SiglipPreTrainedModel { }\n\n/**\n * The text model from SigLIP without any head or projection on top.\n * \n * **Example:** Compute text embeddings with `SiglipTextModel`.\n * \n * ```javascript\n * import { AutoTokenizer, SiglipTextModel } from '@xenova/transformers';\n * \n * // Load tokenizer and text model\n * const tokenizer = await AutoTokenizer.from_pretrained('Xenova/siglip-base-patch16-224');\n * const text_model = await SiglipTextModel.from_pretrained('Xenova/siglip-base-patch16-224');\n * \n * // Run tokenization\n * const texts = ['a photo of 2 cats', 'a photo of 2 dogs'];\n * const text_inputs = tokenizer(texts, { padding: 'max_length', truncation: true });\n * \n * // Compute embeddings\n * const { pooler_output } = await text_model(text_inputs);\n * // Tensor {\n * //   dims: [ 2, 768 ],\n * //   type: 'float32',\n * //   data: Float32Array(1536) [ ... ],\n * //   size: 1536\n * // }\n * ```\n */\nexport class SiglipTextModel extends SiglipPreTrainedModel {\n\n    /** @type {PreTrainedModel.from_pretrained} */\n    static async from_pretrained(pretrained_model_name_or_path, options = {}) {\n        // Update default model file name if not provided\n        options.model_file_name ??= 'text_model';\n        return super.from_pretrained(pretrained_model_name_or_path, options);\n    }\n}\n\n/**\n * The vision model from SigLIP without any head or projection on top.\n * \n * **Example:** Compute vision embeddings with `SiglipVisionModel`.\n * \n * ```javascript\n * import { AutoProcessor, SiglipVisionModel, RawImage} from '@xenova/transformers';\n * \n * // Load processor and vision model\n * const processor = await AutoProcessor.from_pretrained('Xenova/siglip-base-patch16-224');\n * const vision_model = await SiglipVisionModel.from_pretrained('Xenova/siglip-base-patch16-224');\n * \n * // Read image and run processor\n * const image = await RawImage.read('https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/football-match.jpg');\n * const image_inputs = await processor(image);\n * \n * // Compute embeddings\n * const { pooler_output } = await vision_model(image_inputs);\n * // Tensor {\n * //   dims: [ 1, 768 ],\n * //   type: 'float32',\n * //   data: Float32Array(768) [ ... ],\n * //   size: 768\n * // }\n * ```\n */\nexport class SiglipVisionModel extends CLIPPreTrainedModel {\n    /** @type {PreTrainedModel.from_pretrained} */\n    static async from_pretrained(pretrained_model_name_or_path, options = {}) {\n        // Update default model file name if not provided\n        options.model_file_name ??= 'vision_model';\n        return super.from_pretrained(pretrained_model_name_or_path, options);\n    }\n}\n//////////////////////////////////////////////////\n// ChineseCLIP models\nexport class ChineseCLIPPreTrainedModel extends PreTrainedModel { }\n\nexport class ChineseCLIPModel extends ChineseCLIPPreTrainedModel { }\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// CLIPSeg models\nexport class CLIPSegPreTrainedModel extends PreTrainedModel { }\n\nexport class CLIPSegModel extends CLIPSegPreTrainedModel { }\n\n/**\n * CLIPSeg model with a Transformer-based decoder on top for zero-shot and one-shot image segmentation.\n * \n * **Example:** Perform zero-shot image segmentation with a `CLIPSegForImageSegmentation` model.\n * \n * ```javascript\n * import { AutoTokenizer, AutoProcessor, CLIPSegForImageSegmentation, RawImage } from '@xenova/transformers';\n * \n * // Load tokenizer, processor, and model\n * const tokenizer = await AutoTokenizer.from_pretrained('Xenova/clipseg-rd64-refined');\n * const processor = await AutoProcessor.from_pretrained('Xenova/clipseg-rd64-refined');\n * const model = await CLIPSegForImageSegmentation.from_pretrained('Xenova/clipseg-rd64-refined');\n * \n * // Run tokenization\n * const texts = ['a glass', 'something to fill', 'wood', 'a jar'];\n * const text_inputs = tokenizer(texts, { padding: true, truncation: true });\n * \n * // Read image and run processor\n * const image = await RawImage.read('https://github.com/timojl/clipseg/blob/master/example_image.jpg?raw=true');\n * const image_inputs = await processor(image);\n * \n * // Run model with both text and pixel inputs\n * const { logits } = await model({ ...text_inputs, ...image_inputs });\n * // logits: Tensor {\n * //   dims: [4, 352, 352],\n * //   type: 'float32',\n * //   data: Float32Array(495616) [ ... ],\n * //   size: 495616\n * // }\n * ```\n * \n * You can visualize the predictions as follows:\n * ```javascript\n * const preds = logits\n *   .unsqueeze_(1)\n *   .sigmoid_()\n *   .mul_(255)\n *   .round_()\n *   .to('uint8');\n * \n * for (let i = 0; i < preds.dims[0]; ++i) {\n *   const img = RawImage.fromTensor(preds[i]);\n *   img.save(`prediction_${i}.png`);\n * }\n * ```\n */\nexport class CLIPSegForImageSegmentation extends CLIPSegPreTrainedModel { }\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// GPT2 models\nexport class GPT2PreTrainedModel extends PreTrainedModel {\n    /**\n     * Creates a new instance of the `GPT2PreTrainedModel` class.\n     * @param {Object} config The configuration of the model.\n     * @param {any} session The ONNX session containing the model weights.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, generation_config) {\n        super(config, session);\n        this.generation_config = generation_config;\n\n        // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n        this.config.pad_token_id = this.config.eos_token_id\n\n        this.num_heads = this.config.n_head\n        this.num_layers = this.config.n_layer\n        this.dim_kv = this.config.n_embd / this.num_heads;\n    }\n}\n\nexport class GPT2Model extends GPT2PreTrainedModel { }\n\n/**\n * GPT-2 language model head on top of the GPT-2 base model. This model is suitable for text generation tasks.\n */\nexport class GPT2LMHeadModel extends GPT2PreTrainedModel { }\n// export class GPT2ForSequenceClassification extends GPT2PreTrainedModel {\n// TODO\n// }\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// GPTNeo models\nexport class GPTNeoPreTrainedModel extends PreTrainedModel {\n    /**\n     * Creates a new instance of the `GPTNeoPreTrainedModel` class.\n     * @param {Object} config The configuration of the model.\n     * @param {any} session The ONNX session containing the model weights.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, generation_config) {\n        super(config, session);\n        this.generation_config = generation_config;\n\n        // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n        this.config.pad_token_id = this.config.eos_token_id\n\n        this.num_heads = this.config.num_heads;\n        this.num_layers = this.config.num_layers;\n        this.dim_kv = this.config.hidden_size / this.num_heads;\n    }\n}\nexport class GPTNeoModel extends GPTNeoPreTrainedModel { }\n\nexport class GPTNeoForCausalLM extends GPTNeoPreTrainedModel { }\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// GPTNeoX models\nexport class GPTNeoXPreTrainedModel extends PreTrainedModel {\n    /**\n     * Creates a new instance of the `GPTNeoXPreTrainedModel` class.\n     * @param {Object} config The configuration of the model.\n     * @param {any} session The ONNX session containing the model weights.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, generation_config) {\n        super(config, session);\n        this.generation_config = generation_config;\n\n        // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n        this.config.pad_token_id = this.config.eos_token_id\n\n        this.num_heads = this.config.num_attention_heads;\n        this.num_layers = this.config.num_hidden_layers;\n        this.dim_kv = this.config.hidden_size / this.num_heads;\n    }\n}\nexport class GPTNeoXModel extends GPTNeoXPreTrainedModel { }\n\nexport class GPTNeoXForCausalLM extends GPTNeoXPreTrainedModel { }\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// GPT-J models\nexport class GPTJPreTrainedModel extends PreTrainedModel {\n    /**\n     * Creates a new instance of the `GPTJPreTrainedModel` class.\n     * @param {Object} config The configuration of the model.\n     * @param {any} session The ONNX session containing the model weights.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, generation_config) {\n        super(config, session);\n        this.generation_config = generation_config;\n\n        // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n        this.config.pad_token_id = this.config.eos_token_id\n\n        this.num_heads = this.config.n_head\n        this.num_layers = this.config.n_layer\n        this.dim_kv = this.config.n_embd / this.num_heads;\n    }\n}\n\nexport class GPTJModel extends GPTJPreTrainedModel { }\n\nexport class GPTJForCausalLM extends GPTJPreTrainedModel { }\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// GPTBigCode models\nexport class GPTBigCodePreTrainedModel extends PreTrainedModel {\n    /**\n     * Creates a new instance of the `GPTBigCodePreTrainedModel` class.\n     * @param {Object} config The configuration of the model.\n     * @param {any} session The ONNX session containing the model weights.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, generation_config) {\n        super(config, session);\n        this.generation_config = generation_config;\n\n        // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n        this.config.pad_token_id = this.config.eos_token_id\n\n        this.num_heads = this.config.n_head\n        this.num_layers = this.config.n_layer\n        this.dim_kv = this.config.n_embd / this.num_heads;\n    }\n}\n\nexport class GPTBigCodeModel extends GPTBigCodePreTrainedModel { }\n\nexport class GPTBigCodeForCausalLM extends GPTBigCodePreTrainedModel { }\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// CodeGen models\nexport class CodeGenPreTrainedModel extends PreTrainedModel {\n    /**\n     * Creates a new instance of the `CodeGenPreTrainedModel` class.\n     * @param {Object} config The model configuration object.\n     * @param {Object} session The ONNX session object.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, generation_config) {\n        super(config, session);\n        this.generation_config = generation_config;\n\n        // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n        this.config.pad_token_id = this.config.eos_token_id\n\n        this.num_heads = this.config.n_head\n        this.num_layers = this.config.n_layer\n        this.dim_kv = this.config.n_embd / this.num_heads;\n    }\n}\n/**\n * CodeGenModel is a class representing a code generation model without a language model head.\n */\nexport class CodeGenModel extends CodeGenPreTrainedModel { }\n\n/**\n * CodeGenForCausalLM is a class that represents a code generation model based on the GPT-2 architecture. It extends the `CodeGenPreTrainedModel` class.\n */\nexport class CodeGenForCausalLM extends CodeGenPreTrainedModel { }\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// LLama models\n\n/**\n * The bare LLama Model outputting raw hidden-states without any specific head on top.\n */\nexport class LlamaPreTrainedModel extends PreTrainedModel {\n    /**\n     * Creates a new instance of the `LlamaPreTrainedModel` class.\n     * @param {Object} config The model configuration object.\n     * @param {Object} session The ONNX session object.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, generation_config) {\n        super(config, session);\n        this.generation_config = generation_config;\n\n        // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n        this.config.pad_token_id = this.config.eos_token_id\n\n        this.num_heads = this.config.num_key_value_heads ?? this.config.num_attention_heads\n        this.num_layers = this.config.num_hidden_layers\n        this.dim_kv = this.config.hidden_size / this.config.num_attention_heads\n    }\n}\n/**\n * The bare LLaMA Model outputting raw hidden-states without any specific head on top.\n */\nexport class LlamaModel extends LlamaPreTrainedModel { }\n\nexport class LlamaForCausalLM extends LlamaPreTrainedModel { }\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// Qwen2 models\n\n/**\n * The bare Qwen2 Model outputting raw hidden-states without any specific head on top.\n */\nexport class Qwen2PreTrainedModel extends PreTrainedModel {\n    /**\n     * Creates a new instance of the `Qwen2PreTrainedModel` class.\n     * @param {Object} config The model configuration object.\n     * @param {Object} session The ONNX session object.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, generation_config) {\n        super(config, session);\n        this.generation_config = generation_config;\n\n        // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n        this.config.pad_token_id = this.config.eos_token_id\n\n        this.num_heads = this.config.num_key_value_heads ?? this.config.num_attention_heads\n        this.num_layers = this.config.num_hidden_layers\n        this.dim_kv = this.config.hidden_size / this.config.num_attention_heads\n    }\n}\n/**\n * The bare Qwen2 Model outputting raw hidden-states without any specific head on top.\n */\nexport class Qwen2Model extends Qwen2PreTrainedModel { }\n\nexport class Qwen2ForCausalLM extends Qwen2PreTrainedModel { }\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// Phi models\n\nexport class PhiPreTrainedModel extends PreTrainedModel {\n    /**\n     * Creates a new instance of the `PhiPreTrainedModel` class.\n     * @param {Object} config The model configuration object.\n     * @param {Object} session The ONNX session object.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, generation_config) {\n        super(config, session);\n        this.generation_config = generation_config;\n\n        // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n        this.config.pad_token_id = this.config.eos_token_id;\n\n        this.num_heads = this.config.num_attention_heads;\n        this.num_layers = this.config.num_hidden_layers;\n        this.dim_kv = this.config.hidden_size / this.num_heads;\n    }\n}\n/**\n * The bare Phi Model outputting raw hidden-states without any specific head on top.\n */\nexport class PhiModel extends PhiPreTrainedModel { }\n\nexport class PhiForCausalLM extends PhiPreTrainedModel { }\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// Bloom models\n/**\n * The Bloom Model transformer with a language modeling head on top (linear layer with weights tied to the input embeddings).\n */\nexport class BloomPreTrainedModel extends PreTrainedModel {\n    /**\n     * Creates a new instance of the `BloomPreTrainedModel` class.\n     * @param {Object} config The configuration of the model.\n     * @param {any} session The ONNX session containing the model weights.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, generation_config) {\n        super(config, session);\n        this.generation_config = generation_config;\n\n        // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n        this.config.pad_token_id = this.config.eos_token_id\n\n        this.num_heads = this.config.n_head\n        this.num_layers = this.config.n_layer\n        this.dim_kv = this.config.hidden_size / this.num_heads;\n    }\n}\n\n/**\n * The bare Bloom Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class BloomModel extends BloomPreTrainedModel { }\n\n/**\n * The Bloom Model transformer with a language modeling head on top (linear layer with weights tied to the input embeddings).\n */\nexport class BloomForCausalLM extends BloomPreTrainedModel { }\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// MPT models\nexport class MptPreTrainedModel extends PreTrainedModel {\n    /**\n     * Creates a new instance of the `MptPreTrainedModel` class.\n     * @param {Object} config The model configuration object.\n     * @param {Object} session The ONNX session object.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, generation_config) {\n        super(config, session);\n        this.generation_config = generation_config;\n\n        // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n        this.config.pad_token_id = this.config.eos_token_id\n\n        this.num_heads = this.config.n_heads\n        this.num_layers = this.config.n_layers\n        this.dim_kv = this.config.d_model / this.num_heads;\n    }\n}\n\n/**\n * The bare Mpt Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class MptModel extends MptPreTrainedModel { }\n\n/**\n * The MPT Model transformer with a language modeling head on top (linear layer with weights tied to the input embeddings).\n */\nexport class MptForCausalLM extends MptPreTrainedModel { }\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// OPT models\nexport class OPTPreTrainedModel extends PreTrainedModel {\n    /**\n     * Creates a new instance of the `OPTPreTrainedModel` class.\n     * @param {Object} config The model configuration object.\n     * @param {Object} session The ONNX session object.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, generation_config) {\n        super(config, session);\n        this.generation_config = generation_config;\n\n        // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n        this.config.pad_token_id = this.config.eos_token_id\n\n        this.num_heads = this.config.num_attention_heads;\n        this.num_layers = this.config.num_hidden_layers;\n        this.dim_kv = this.config.hidden_size / this.num_heads;\n    }\n}\n\n/**\n * The bare OPT Model outputting raw hidden-states without any specific head on top.\n */\nexport class OPTModel extends OPTPreTrainedModel { }\n\n/**\n * The OPT Model transformer with a language modeling head on top (linear layer with weights tied to the input embeddings).\n */\nexport class OPTForCausalLM extends OPTPreTrainedModel { }\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\nexport class ViTPreTrainedModel extends PreTrainedModel { }\nexport class ViTModel extends ViTPreTrainedModel { }\nexport class ViTForImageClassification extends ViTPreTrainedModel {\n    /**\n     * @param {any} model_inputs\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\nexport class FastViTPreTrainedModel extends PreTrainedModel { }\nexport class FastViTModel extends FastViTPreTrainedModel { }\nexport class FastViTForImageClassification extends FastViTPreTrainedModel {\n    /**\n     * @param {any} model_inputs\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\nexport class VitMattePreTrainedModel extends PreTrainedModel { }\n\n/**\n * ViTMatte framework leveraging any vision backbone e.g. for ADE20k, CityScapes.\n * \n * **Example:** Perform image matting with a `VitMatteForImageMatting` model.\n * ```javascript\n * import { AutoProcessor, VitMatteForImageMatting, RawImage } from '@xenova/transformers';\n * \n * // Load processor and model\n * const processor = await AutoProcessor.from_pretrained('Xenova/vitmatte-small-distinctions-646');\n * const model = await VitMatteForImageMatting.from_pretrained('Xenova/vitmatte-small-distinctions-646');\n * \n * // Load image and trimap\n * const image = await RawImage.fromURL('https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/vitmatte_image.png');\n * const trimap = await RawImage.fromURL('https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/vitmatte_trimap.png');\n * \n * // Prepare image + trimap for the model\n * const inputs = await processor(image, trimap);\n * \n * // Predict alpha matte\n * const { alphas } = await model(inputs);\n * // Tensor {\n * //   dims: [ 1, 1, 640, 960 ],\n * //   type: 'float32',\n * //   size: 614400,\n * //   data: Float32Array(614400) [ 0.9894027709960938, 0.9970508813858032, ... ]\n * // }\n * ```\n * \n * You can visualize the alpha matte as follows:\n * ```javascript\n * import { Tensor, cat } from '@xenova/transformers';\n * \n * // Visualize predicted alpha matte\n * const imageTensor = image.toTensor();\n * \n * // Convert float (0-1) alpha matte to uint8 (0-255)\n * const alphaChannel = alphas\n *   .squeeze(0)\n *   .mul_(255)\n *   .clamp_(0, 255)\n *   .round_()\n *   .to('uint8');\n * \n * // Concatenate original image with predicted alpha\n * const imageData = cat([imageTensor, alphaChannel], 0);\n * \n * // Save output image\n * const outputImage = RawImage.fromTensor(imageData);\n * outputImage.save('output.png');\n * ```\n */\nexport class VitMatteForImageMatting extends VitMattePreTrainedModel {\n    /**\n     * @param {any} model_inputs\n     */\n    async _call(model_inputs) {\n        return new ImageMattingOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\nexport class MobileViTPreTrainedModel extends PreTrainedModel { }\nexport class MobileViTModel extends MobileViTPreTrainedModel { }\nexport class MobileViTForImageClassification extends MobileViTPreTrainedModel {\n    /**\n     * @param {any} model_inputs\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n// TODO: MobileViTForSemanticSegmentation\n\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\nexport class MobileViTV2PreTrainedModel extends PreTrainedModel { }\nexport class MobileViTV2Model extends MobileViTV2PreTrainedModel { }\nexport class MobileViTV2ForImageClassification extends MobileViTV2PreTrainedModel {\n    /**\n     * @param {any} model_inputs\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n// TODO: MobileViTV2ForSemanticSegmentation\n\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\nexport class OwlViTPreTrainedModel extends PreTrainedModel { }\nexport class OwlViTModel extends OwlViTPreTrainedModel { }\nexport class OwlViTForObjectDetection extends OwlViTPreTrainedModel { }\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\nexport class Owlv2PreTrainedModel extends PreTrainedModel { }\nexport class Owlv2Model extends Owlv2PreTrainedModel { }\nexport class Owlv2ForObjectDetection extends Owlv2PreTrainedModel { }\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// Beit Models\nexport class BeitPreTrainedModel extends PreTrainedModel { }\nexport class BeitModel extends BeitPreTrainedModel { }\nexport class BeitForImageClassification extends BeitPreTrainedModel {\n    /**\n     * @param {any} model_inputs\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\nexport class DetrPreTrainedModel extends PreTrainedModel { }\nexport class DetrModel extends DetrPreTrainedModel { }\nexport class DetrForObjectDetection extends DetrPreTrainedModel {\n    /**\n     * @param {any} model_inputs\n     */\n    async _call(model_inputs) {\n        return new DetrObjectDetectionOutput(await super._call(model_inputs));\n    }\n}\n\nexport class DetrForSegmentation extends DetrPreTrainedModel {\n    /**\n     * Runs the model with the provided inputs\n     * @param {Object} model_inputs Model inputs\n     * @returns {Promise<DetrSegmentationOutput>} Object containing segmentation outputs\n     */\n    async _call(model_inputs) {\n        return new DetrSegmentationOutput(await super._call(model_inputs));\n    }\n}\n\nexport class DetrObjectDetectionOutput extends ModelOutput {\n    /**\n     * @param {Object} output The output of the model.\n     * @param {Tensor} output.logits Classification logits (including no-object) for all queries.\n     * @param {Tensor} output.pred_boxes Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height).\n     * These values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding possible padding).\n     */\n    constructor({ logits, pred_boxes }) {\n        super();\n        this.logits = logits;\n        this.pred_boxes = pred_boxes;\n    }\n}\n\nexport class DetrSegmentationOutput extends ModelOutput {\n    /**\n     * @param {Object} output The output of the model.\n     * @param {Tensor} output.logits The output logits of the model.\n     * @param {Tensor} output.pred_boxes Predicted boxes.\n     * @param {Tensor} output.pred_masks Predicted masks.\n     */\n    constructor({ logits, pred_boxes, pred_masks }) {\n        super();\n        this.logits = logits;\n        this.pred_boxes = pred_boxes;\n        this.pred_masks = pred_masks;\n    }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\nexport class TableTransformerPreTrainedModel extends PreTrainedModel { }\n\n/**\n * The bare Table Transformer Model (consisting of a backbone and encoder-decoder Transformer)\n * outputting raw hidden-states without any specific head on top.\n */\nexport class TableTransformerModel extends TableTransformerPreTrainedModel { }\n\n/**\n * Table Transformer Model (consisting of a backbone and encoder-decoder Transformer)\n * with object detection heads on top, for tasks such as COCO detection.\n */\nexport class TableTransformerForObjectDetection extends TableTransformerPreTrainedModel {\n    /**\n     * @param {any} model_inputs\n     */\n    async _call(model_inputs) {\n        return new TableTransformerObjectDetectionOutput(await super._call(model_inputs));\n    }\n}\nexport class TableTransformerObjectDetectionOutput extends DetrObjectDetectionOutput { }\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\nexport class DeiTPreTrainedModel extends PreTrainedModel { }\nexport class DeiTModel extends DeiTPreTrainedModel { }\nexport class DeiTForImageClassification extends DeiTPreTrainedModel {\n    /**\n     * @param {any} model_inputs\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n/**\n * An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained models.\n */\nexport class ResNetPreTrainedModel extends PreTrainedModel { }\n\n/**\n * The bare ResNet model outputting raw features without any specific head on top.\n */\nexport class ResNetModel extends ResNetPreTrainedModel { }\n\n/**\n * ResNet Model with an image classification head on top (a linear layer on top of the pooled features), e.g. for ImageNet.\n */\nexport class ResNetForImageClassification extends ResNetPreTrainedModel {\n    /**\n     * @param {any} model_inputs\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\nexport class SwinPreTrainedModel extends PreTrainedModel { }\nexport class SwinModel extends SwinPreTrainedModel { }\nexport class SwinForImageClassification extends SwinPreTrainedModel {\n    /**\n     * @param {any} model_inputs\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\nexport class Swin2SRPreTrainedModel extends PreTrainedModel { }\n\n/**\n * The bare Swin2SR Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class Swin2SRModel extends Swin2SRPreTrainedModel { }\n\n/**\n * Swin2SR Model transformer with an upsampler head on top for image super resolution and restoration.\n * \n * **Example:** Super-resolution w/ `Xenova/swin2SR-classical-sr-x2-64`.\n * \n * ```javascript\n * import { AutoProcessor, Swin2SRForImageSuperResolution, RawImage } from '@xenova/transformers';\n * \n * // Load processor and model\n * const model_id = 'Xenova/swin2SR-classical-sr-x2-64';\n * const processor = await AutoProcessor.from_pretrained(model_id);\n * const model = await Swin2SRForImageSuperResolution.from_pretrained(model_id);\n * \n * // Prepare model inputs\n * const url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/butterfly.jpg';\n * const image = await RawImage.fromURL(url);\n * const inputs = await processor(image);\n * \n * // Run model\n * const outputs = await model(inputs);\n * \n * // Convert Tensor to RawImage\n * const output = outputs.reconstruction.squeeze().clamp_(0, 1).mul_(255).round_().to('uint8');\n * const outputImage = RawImage.fromTensor(output);\n * // RawImage {\n * //   data: Uint8Array(786432) [ 41, 31, 24, ... ],\n * //   width: 512,\n * //   height: 512,\n * //   channels: 3\n * // }\n * ```\n */\nexport class Swin2SRForImageSuperResolution extends Swin2SRPreTrainedModel { }\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\nexport class DPTPreTrainedModel extends PreTrainedModel { }\n\n/**\n * The bare DPT Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class DPTModel extends DPTPreTrainedModel { }\n\n/**\n * DPT Model with a depth estimation head on top (consisting of 3 convolutional layers) e.g. for KITTI, NYUv2.\n * \n * **Example:** Depth estimation w/ `Xenova/dpt-hybrid-midas`.\n * ```javascript\n * import { DPTForDepthEstimation, AutoProcessor, RawImage, interpolate, max } from '@xenova/transformers';\n * \n * // Load model and processor\n * const model_id = 'Xenova/dpt-hybrid-midas';\n * const model = await DPTForDepthEstimation.from_pretrained(model_id);\n * const processor = await AutoProcessor.from_pretrained(model_id);\n * \n * // Load image from URL\n * const url = 'http://images.cocodataset.org/val2017/000000039769.jpg';\n * const image = await RawImage.fromURL(url);\n * \n * // Prepare image for the model\n * const inputs = await processor(image);\n * \n * // Run model\n * const { predicted_depth } = await model(inputs);\n * \n * // Interpolate to original size\n * const prediction = interpolate(predicted_depth, image.size.reverse(), 'bilinear', false);\n * \n * // Visualize the prediction\n * const formatted = prediction.mul_(255 / max(prediction.data)[0]).to('uint8');\n * const depth = RawImage.fromTensor(formatted);\n * // RawImage {\n * //   data: Uint8Array(307200) [ 85, 85, 84, ... ],\n * //   width: 640,\n * //   height: 480,\n * //   channels: 1\n * // }\n * ```\n */\nexport class DPTForDepthEstimation extends DPTPreTrainedModel { }\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\nexport class DepthAnythingPreTrainedModel extends PreTrainedModel { }\n\n/**\n * Depth Anything Model with a depth estimation head on top (consisting of 3 convolutional layers) e.g. for KITTI, NYUv2.\n */\nexport class DepthAnythingForDepthEstimation extends DepthAnythingPreTrainedModel { }\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\nexport class GLPNPreTrainedModel extends PreTrainedModel { }\n\n/**\n * The bare GLPN encoder (Mix-Transformer) outputting raw hidden-states without any specific head on top.\n */\nexport class GLPNModel extends GLPNPreTrainedModel { }\n\n/**\n * GLPN Model transformer with a lightweight depth estimation head on top e.g. for KITTI, NYUv2.\n * \n * **Example:** Depth estimation w/ `Xenova/glpn-kitti`.\n * ```javascript\n * import { GLPNForDepthEstimation, AutoProcessor, RawImage, interpolate, max } from '@xenova/transformers';\n * \n * // Load model and processor\n * const model_id = 'Xenova/glpn-kitti';\n * const model = await GLPNForDepthEstimation.from_pretrained(model_id);\n * const processor = await AutoProcessor.from_pretrained(model_id);\n * \n * // Load image from URL\n * const url = 'http://images.cocodataset.org/val2017/000000039769.jpg';\n * const image = await RawImage.fromURL(url);\n * \n * // Prepare image for the model\n * const inputs = await processor(image);\n * \n * // Run model\n * const { predicted_depth } = await model(inputs);\n * \n * // Interpolate to original size\n * const prediction = interpolate(predicted_depth, image.size.reverse(), 'bilinear', false);\n * \n * // Visualize the prediction\n * const formatted = prediction.mul_(255 / max(prediction.data)[0]).to('uint8');\n * const depth = RawImage.fromTensor(formatted);\n * // RawImage {\n * //   data: Uint8Array(307200) [ 207, 169, 154, ... ],\n * //   width: 640,\n * //   height: 480,\n * //   channels: 1\n * // }\n * ```\n */\nexport class GLPNForDepthEstimation extends GLPNPreTrainedModel { }\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\nexport class DonutSwinPreTrainedModel extends PreTrainedModel { }\n\n/**\n * The bare Donut Swin Model transformer outputting raw hidden-states without any specific head on top.\n * \n * **Example:** Step-by-step Document Parsing.\n * \n * ```javascript\n * import { AutoProcessor, AutoTokenizer, AutoModelForVision2Seq, RawImage } from '@xenova/transformers';\n * \n * // Choose model to use\n * const model_id = 'Xenova/donut-base-finetuned-cord-v2';\n * \n * // Prepare image inputs\n * const processor = await AutoProcessor.from_pretrained(model_id);\n * const url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/receipt.png';\n * const image = await RawImage.read(url);\n * const image_inputs = await processor(image);\n * \n * // Prepare decoder inputs\n * const tokenizer = await AutoTokenizer.from_pretrained(model_id);\n * const task_prompt = '<s_cord-v2>';\n * const decoder_input_ids = tokenizer(task_prompt, {\n *   add_special_tokens: false,\n * }).input_ids;\n * \n * // Create the model\n * const model = await AutoModelForVision2Seq.from_pretrained(model_id);\n * \n * // Run inference\n * const output = await model.generate(image_inputs.pixel_values, {\n *   decoder_input_ids,\n *   max_length: model.config.decoder.max_position_embeddings,\n * });\n * \n * // Decode output\n * const decoded = tokenizer.batch_decode(output)[0];\n * // <s_cord-v2><s_menu><s_nm> CINNAMON SUGAR</s_nm><s_unitprice> 17,000</s_unitprice><s_cnt> 1 x</s_cnt><s_price> 17,000</s_price></s_menu><s_sub_total><s_subtotal_price> 17,000</s_subtotal_price></s_sub_total><s_total><s_total_price> 17,000</s_total_price><s_cashprice> 20,000</s_cashprice><s_changeprice> 3,000</s_changeprice></s_total></s>\n * ```\n * \n * **Example:** Step-by-step Document Visual Question Answering (DocVQA)\n * \n * ```javascript\n * import { AutoProcessor, AutoTokenizer, AutoModelForVision2Seq, RawImage } from '@xenova/transformers';\n * \n * // Choose model to use\n * const model_id = 'Xenova/donut-base-finetuned-docvqa';\n * \n * // Prepare image inputs\n * const processor = await AutoProcessor.from_pretrained(model_id);\n * const url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/invoice.png';\n * const image = await RawImage.read(url);\n * const image_inputs = await processor(image);\n * \n * // Prepare decoder inputs\n * const tokenizer = await AutoTokenizer.from_pretrained(model_id);\n * const question = 'What is the invoice number?';\n * const task_prompt = `<s_docvqa><s_question>${question}</s_question><s_answer>`;\n * const decoder_input_ids = tokenizer(task_prompt, {\n *   add_special_tokens: false,\n * }).input_ids;\n * \n * // Create the model\n * const model = await AutoModelForVision2Seq.from_pretrained(model_id);\n * \n * // Run inference\n * const output = await model.generate(image_inputs.pixel_values, {\n *   decoder_input_ids,\n *   max_length: model.config.decoder.max_position_embeddings,\n * });\n * \n * // Decode output\n * const decoded = tokenizer.batch_decode(output)[0];\n * // <s_docvqa><s_question> What is the invoice number?</s_question><s_answer> us-001</s_answer></s>\n * ```\n */\nexport class DonutSwinModel extends DonutSwinPreTrainedModel { }\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\nexport class ConvNextPreTrainedModel extends PreTrainedModel { }\n\n/**\n * The bare ConvNext model outputting raw features without any specific head on top.\n */\nexport class ConvNextModel extends ConvNextPreTrainedModel { }\n\n/**\n * ConvNext Model with an image classification head on top (a linear layer on top of the pooled features), e.g. for ImageNet.\n */\nexport class ConvNextForImageClassification extends ConvNextPreTrainedModel {\n    /**\n     * @param {any} model_inputs\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\nexport class ConvNextV2PreTrainedModel extends PreTrainedModel { }\n\n/**\n * The bare ConvNextV2 model outputting raw features without any specific head on top.\n */\nexport class ConvNextV2Model extends ConvNextV2PreTrainedModel { }\n\n/**\n * ConvNextV2 Model with an image classification head on top (a linear layer on top of the pooled features), e.g. for ImageNet.\n */\nexport class ConvNextV2ForImageClassification extends ConvNextV2PreTrainedModel {\n    /**\n     * @param {any} model_inputs\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\nexport class Dinov2PreTrainedModel extends PreTrainedModel { }\n\n/**\n * The bare DINOv2 Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class Dinov2Model extends Dinov2PreTrainedModel { }\n\n/**\n * Dinov2 Model transformer with an image classification head on top (a linear layer on top of the final hidden state of the [CLS] token) e.g. for ImageNet.\n */\nexport class Dinov2ForImageClassification extends Dinov2PreTrainedModel {\n    /**\n     * @param {any} model_inputs\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\nexport class YolosPreTrainedModel extends PreTrainedModel { }\nexport class YolosModel extends YolosPreTrainedModel { }\nexport class YolosForObjectDetection extends YolosPreTrainedModel {\n    /**\n     * @param {any} model_inputs\n     */\n    async _call(model_inputs) {\n        return new YolosObjectDetectionOutput(await super._call(model_inputs));\n    }\n}\n\nexport class YolosObjectDetectionOutput extends ModelOutput {\n    /**\n     * @param {Object} output The output of the model.\n     * @param {Tensor} output.logits Classification logits (including no-object) for all queries.\n     * @param {Tensor} output.pred_boxes Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height).\n     * These values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding possible padding).\n     */\n    constructor({ logits, pred_boxes }) {\n        super();\n        this.logits = logits;\n        this.pred_boxes = pred_boxes;\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\nexport class SamPreTrainedModel extends PreTrainedModel { }\n\n/**\n * Segment Anything Model (SAM) for generating segmentation masks, given an input image\n * and optional 2D location and bounding boxes.\n * \n * **Example:** Perform mask generation w/ `Xenova/sam-vit-base`.\n * ```javascript\n * import { SamModel, AutoProcessor, RawImage } from '@xenova/transformers';\n * \n * const model = await SamModel.from_pretrained('Xenova/sam-vit-base');\n * const processor = await AutoProcessor.from_pretrained('Xenova/sam-vit-base');\n * \n * const img_url = 'https://huggingface.co/ybelkada/segment-anything/resolve/main/assets/car.png';\n * const raw_image = await RawImage.read(img_url);\n * const input_points = [[[450, 600]]] // 2D localization of a window\n * \n * const inputs = await processor(raw_image, input_points);\n * const outputs = await model(inputs);\n * \n * const masks = await processor.post_process_masks(outputs.pred_masks, inputs.original_sizes, inputs.reshaped_input_sizes);\n * // [\n * //   Tensor {\n * //     dims: [ 1, 3, 1764, 2646 ],\n * //     type: 'bool',\n * //     data: Uint8Array(14002632) [ ... ],\n * //     size: 14002632\n * //   }\n * // ]\n * const scores = outputs.iou_scores;\n * // Tensor {\n * //   dims: [ 1, 1, 3 ],\n * //   type: 'float32',\n * //   data: Float32Array(3) [\n * //     0.8892380595207214,\n * //     0.9311248064041138,\n * //     0.983696699142456\n * //   ],\n * //   size: 3\n * // }\n * ```\n */\nexport class SamModel extends SamPreTrainedModel {\n    /**\n     * Creates a new instance of the `SamModel` class.\n     * @param {Object} config The configuration object specifying the hyperparameters and other model settings.\n     * @param {Object} vision_encoder The ONNX session containing the vision encoder model.\n     * @param {any} prompt_encoder_mask_decoder The ONNX session containing the prompt encoder and mask decoder model.\n     */\n    constructor(config, vision_encoder, prompt_encoder_mask_decoder) {\n        super(config, vision_encoder);\n        this.prompt_encoder_mask_decoder = prompt_encoder_mask_decoder;\n    }\n\n    /**\n     * Compute image embeddings and positional image embeddings, given the pixel values of an image.\n     * @param {Object} model_inputs Object containing the model inputs.\n     * @param {Tensor} model_inputs.pixel_values Pixel values obtained using a `SamProcessor`.\n     * @returns {Promise<{ image_embeddings: Tensor, image_positional_embeddings: Tensor }>} The image embeddings and positional image embeddings.\n     */\n    async get_image_embeddings({ pixel_values }) {\n        // in:\n        //  - pixel_values: tensor.float32[batch_size,3,1024,1024]\n        // \n        // out:\n        //  - image_embeddings: tensor.float32[batch_size,256,64,64]\n        //  - image_positional_embeddings: tensor.float32[batch_size,256,64,64]\n        return await encoderForward(this, { pixel_values })\n    }\n\n    /**\n     * @typedef {Object} SamModelInputs Object containing the model inputs.\n     * @property {Tensor} pixel_values Pixel values as a Tensor with shape `(batch_size, num_channels, height, width)`.\n     * These can be obtained using a `SamProcessor`.\n     * @property {Tensor} input_points Input 2D spatial points with shape `(batch_size, num_points, 2)`.\n     * This is used by the prompt encoder to encode the prompt.\n     * @property {Tensor} [input_labels] Input labels for the points, as a Tensor of shape `(batch_size, point_batch_size, num_points)`.\n     * This is used by the prompt encoder to encode the prompt. There are 4 types of labels:\n     *  - `1`: the point is a point that contains the object of interest\n     *  - `0`: the point is a point that does not contain the object of interest\n     *  - `-1`: the point corresponds to the background\n     *  - `-10`: the point is a padding point, thus should be ignored by the prompt encoder\n     * @property {Tensor} [image_embeddings] Image embeddings used by the mask decoder.\n     * @property {Tensor} [image_positional_embeddings] Image positional embeddings used by the mask decoder.\n     */\n\n    /**\n     * @param {SamModelInputs} model_inputs Object containing the model inputs.\n     * @returns {Promise<Object>} The output of the model.\n     */\n    async forward(model_inputs) {\n        if (!model_inputs.image_embeddings || !model_inputs.image_positional_embeddings) {\n            // Compute the image embeddings if they are missing\n            model_inputs = {\n                ...model_inputs,\n                ...(await this.get_image_embeddings(model_inputs))\n            }\n        }\n\n        if (!model_inputs.input_labels) {\n            // Set default input labels if they are missing\n            const shape = model_inputs.input_points.dims.slice(0, -1);\n            const numElements = shape.reduce((a, b) => a * b, 1);\n            model_inputs.input_labels = new Tensor(\n                'int64',\n                new BigInt64Array(numElements).fill(1n),\n                shape\n            );\n        }\n\n        // Returns:\n        //  - iou_scores: tensor.float32[batch_size,point_batch_size,3]\n        //  - pred_masks: tensor.float32[batch_size,point_batch_size,3,256,256]\n        return await sessionRun(this.prompt_encoder_mask_decoder, {\n            input_points: model_inputs.input_points,\n            input_labels: model_inputs.input_labels,\n            image_embeddings: model_inputs.image_embeddings,\n            image_positional_embeddings: model_inputs.image_positional_embeddings,\n        });\n    }\n\n    /**\n     * Runs the model with the provided inputs\n     * @param {Object} model_inputs Model inputs\n     * @returns {Promise<SamImageSegmentationOutput>} Object containing segmentation outputs\n     */\n    async _call(model_inputs) {\n        return new SamImageSegmentationOutput(await super._call(model_inputs));\n    }\n}\n\n\n/**\n * Base class for Segment-Anything model's output.\n */\nexport class SamImageSegmentationOutput extends ModelOutput {\n    /**\n     * @param {Object} output The output of the model.\n     * @param {Tensor} output.iou_scores The output logits of the model.\n     * @param {Tensor} output.pred_masks Predicted boxes.\n     */\n    constructor({ iou_scores, pred_masks }) {\n        super();\n        this.iou_scores = iou_scores;\n        this.pred_masks = pred_masks;\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// MarianMT models\nexport class MarianPreTrainedModel extends PreTrainedModel { };\n\nexport class MarianModel extends MarianPreTrainedModel { }\n\nexport class MarianMTModel extends MarianPreTrainedModel {\n\n    /**\n     * Creates a new instance of the `MarianMTModel` class.\n    * @param {Object} config The model configuration object.\n    * @param {Object} session The ONNX session object.\n    * @param {any} decoder_merged_session \n    * @param {any} generation_config \n    */\n    constructor(config, session, decoder_merged_session, generation_config) {\n        super(config, session);\n        this.decoder_merged_session = decoder_merged_session;\n        this.generation_config = generation_config;\n\n        this.num_decoder_layers = this.config.decoder_layers;\n        this.num_decoder_heads = this.config.decoder_attention_heads;\n        this.decoder_dim_kv = this.config.d_model / this.num_decoder_heads;\n\n        this.num_encoder_layers = this.config.encoder_layers;\n        this.num_encoder_heads = this.config.encoder_attention_heads;\n        this.encoder_dim_kv = this.config.d_model / this.num_encoder_heads;\n    }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// M2M100 models\nexport class M2M100PreTrainedModel extends PreTrainedModel { };\n\nexport class M2M100Model extends M2M100PreTrainedModel { }\n\nexport class M2M100ForConditionalGeneration extends M2M100PreTrainedModel {\n\n    /**\n     * Creates a new instance of the `M2M100ForConditionalGeneration` class.\n    * @param {Object} config The model configuration object.\n    * @param {Object} session The ONNX session object.\n    * @param {any} decoder_merged_session \n    * @param {any} generation_config \n    */\n    constructor(config, session, decoder_merged_session, generation_config) {\n        super(config, session);\n        this.decoder_merged_session = decoder_merged_session;\n        this.generation_config = generation_config;\n\n        this.num_decoder_layers = this.config.decoder_layers;\n        this.num_decoder_heads = this.config.decoder_attention_heads;\n        this.decoder_dim_kv = this.config.d_model / this.num_decoder_heads;\n\n        this.num_encoder_layers = this.config.encoder_layers;\n        this.num_encoder_heads = this.config.encoder_attention_heads;\n        this.encoder_dim_kv = this.config.d_model / this.num_encoder_heads;\n    }\n\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// Wav2Vec2 models\nexport class Wav2Vec2PreTrainedModel extends PreTrainedModel { };\n\n/**\n * The bare Wav2Vec2 Model transformer outputting raw hidden-states without any specific head on top.\n * \n * **Example:** Load and run a `Wav2Vec2Model` for feature extraction.\n * \n * ```javascript\n * import { AutoProcessor, AutoModel, read_audio } from '@xenova/transformers';\n * \n * // Read and preprocess audio\n * const processor = await AutoProcessor.from_pretrained('Xenova/mms-300m');\n * const audio = await read_audio('https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac', 16000);\n * const inputs = await processor(audio);\n * \n * // Run model with inputs\n * const model = await AutoModel.from_pretrained('Xenova/mms-300m');\n * const output = await model(inputs);\n * // {\n * //   last_hidden_state: Tensor {\n * //     dims: [ 1, 1144, 1024 ],\n * //     type: 'float32',\n * //     data: Float32Array(1171456) [ ... ],\n * //     size: 1171456\n * //   }\n * // }\n * ```\n */\nexport class Wav2Vec2Model extends Wav2Vec2PreTrainedModel { }\n\nexport class Wav2Vec2ForCTC extends Wav2Vec2PreTrainedModel {\n    /**\n     * @param {Object} model_inputs\n     * @param {Tensor} model_inputs.input_values Float values of input raw speech waveform.\n     * @param {Tensor} model_inputs.attention_mask Mask to avoid performing convolution and attention on padding token indices. Mask values selected in [0, 1]\n     */\n    async _call(model_inputs) {\n        return new CausalLMOutput(await super._call(model_inputs));\n    }\n}\n\nexport class Wav2Vec2ForSequenceClassification extends Wav2Vec2PreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * Wav2Vec2 Model with a frame classification head on top for tasks like Speaker Diarization.\n */\nexport class Wav2Vec2ForAudioFrameClassification extends Wav2Vec2PreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for sequence classification.\n     */\n    async _call(model_inputs) {\n        return new TokenClassifierOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// UniSpeech models\nexport class UniSpeechPreTrainedModel extends PreTrainedModel { };\n\n/**\n * The bare UniSpeech Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class UniSpeechModel extends UniSpeechPreTrainedModel { }\n\n/**\n * UniSpeech Model with a `language modeling` head on top for Connectionist Temporal Classification (CTC).\n */\nexport class UniSpeechForCTC extends UniSpeechPreTrainedModel {\n    /**\n     * @param {Object} model_inputs\n     * @param {Tensor} model_inputs.input_values Float values of input raw speech waveform.\n     * @param {Tensor} model_inputs.attention_mask Mask to avoid performing convolution and attention on padding token indices. Mask values selected in [0, 1]\n     */\n    async _call(model_inputs) {\n        return new CausalLMOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * UniSpeech Model with a sequence classification head on top (a linear layer over the pooled output).\n */\nexport class UniSpeechForSequenceClassification extends UniSpeechPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// UniSpeechSat models\nexport class UniSpeechSatPreTrainedModel extends PreTrainedModel { };\n\n/**\n * The bare UniSpeechSat Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class UniSpeechSatModel extends UniSpeechSatPreTrainedModel { }\n\n/**\n * UniSpeechSat Model with a `language modeling` head on top for Connectionist Temporal Classification (CTC).\n */\nexport class UniSpeechSatForCTC extends UniSpeechSatPreTrainedModel {\n    /**\n     * @param {Object} model_inputs\n     * @param {Tensor} model_inputs.input_values Float values of input raw speech waveform.\n     * @param {Tensor} model_inputs.attention_mask Mask to avoid performing convolution and attention on padding token indices. Mask values selected in [0, 1]\n     */\n    async _call(model_inputs) {\n        return new CausalLMOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * UniSpeechSat Model with a sequence classification head on top (a linear layer over the pooled output).\n */\nexport class UniSpeechSatForSequenceClassification extends UniSpeechSatPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * UniSpeechSat Model with a frame classification head on top for tasks like Speaker Diarization.\n */\nexport class UniSpeechSatForAudioFrameClassification extends UniSpeechSatPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for sequence classification.\n     */\n    async _call(model_inputs) {\n        return new TokenClassifierOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// Wav2Vec2Bert models\nexport class Wav2Vec2BertPreTrainedModel extends PreTrainedModel { };\n\n/**\n * The bare Wav2Vec2Bert Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class Wav2Vec2BertModel extends Wav2Vec2BertPreTrainedModel { }\n\n/**\n * Wav2Vec2Bert Model with a `language modeling` head on top for Connectionist Temporal Classification (CTC).\n */\nexport class Wav2Vec2BertForCTC extends Wav2Vec2BertPreTrainedModel {\n    /**\n     * @param {Object} model_inputs\n     * @param {Tensor} model_inputs.input_features Float values of input mel-spectrogram.\n     * @param {Tensor} model_inputs.attention_mask Mask to avoid performing convolution and attention on padding token indices. Mask values selected in [0, 1]\n     */\n    async _call(model_inputs) {\n        return new CausalLMOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * Wav2Vec2Bert Model with a sequence classification head on top (a linear layer over the pooled output).\n */\nexport class Wav2Vec2BertForSequenceClassification extends Wav2Vec2BertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// Hubert models\nexport class HubertPreTrainedModel extends PreTrainedModel { }\n\n/**\n * The bare Hubert Model transformer outputting raw hidden-states without any specific head on top.\n * \n * **Example:** Load and run a `HubertModel` for feature extraction.\n * \n * ```javascript\n * import { AutoProcessor, AutoModel, read_audio } from '@xenova/transformers';\n * \n * // Read and preprocess audio\n * const processor = await AutoProcessor.from_pretrained('Xenova/hubert-base-ls960');\n * const audio = await read_audio('https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/jfk.wav', 16000);\n * const inputs = await processor(audio);\n * \n * // Load and run model with inputs\n * const model = await AutoModel.from_pretrained('Xenova/hubert-base-ls960');\n * const output = await model(inputs);\n * // {\n * //   last_hidden_state: Tensor {\n * //     dims: [ 1, 549, 768 ],\n * //     type: 'float32',\n * //     data: Float32Array(421632) [0.0682469978928566, 0.08104046434164047, -0.4975186586380005, ...],\n * //     size: 421632\n * //   }\n * // }\n * ```\n */\nexport class HubertModel extends Wav2Vec2PreTrainedModel { }\n\n/**\n * Hubert Model with a `language modeling` head on top for Connectionist Temporal Classification (CTC).\n */\nexport class HubertForCTC extends Wav2Vec2PreTrainedModel {\n    /**\n     * @param {Object} model_inputs\n     * @param {Tensor} model_inputs.input_values Float values of input raw speech waveform.\n     * @param {Tensor} model_inputs.attention_mask Mask to avoid performing convolution and attention on padding token indices. Mask values selected in [0, 1]\n     */\n    async _call(model_inputs) {\n        return new CausalLMOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * Hubert Model with a sequence classification head on top (a linear layer over the pooled output) for tasks like SUPERB Keyword Spotting.\n */\nexport class HubertForSequenceClassification extends Wav2Vec2PreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// WavLM models\n/**\n * An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained models.\n */\nexport class WavLMPreTrainedModel extends PreTrainedModel { };\n\n/**\n * The bare WavLM Model transformer outputting raw hidden-states without any specific head on top.\n * \n * **Example:** Load and run a `WavLMModel` for feature extraction.\n * \n * ```javascript\n * import { AutoProcessor, AutoModel, read_audio } from '@xenova/transformers';\n * \n * // Read and preprocess audio\n * const processor = await AutoProcessor.from_pretrained('Xenova/wavlm-base');\n * const audio = await read_audio('https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/jfk.wav', 16000);\n * const inputs = await processor(audio);\n * \n * // Run model with inputs\n * const model = await AutoModel.from_pretrained('Xenova/wavlm-base');\n * const output = await model(inputs);\n * // {\n * //   last_hidden_state: Tensor {\n * //     dims: [ 1, 549, 768 ],\n * //     type: 'float32',\n * //     data: Float32Array(421632) [-0.349443256855011, -0.39341306686401367,  0.022836603224277496, ...],\n * //     size: 421632\n * //   }\n * // }\n * ```\n */\nexport class WavLMModel extends WavLMPreTrainedModel { }\n\n/**\n * WavLM Model with a `language modeling` head on top for Connectionist Temporal Classification (CTC).\n */\nexport class WavLMForCTC extends WavLMPreTrainedModel {\n    /**\n     * @param {Object} model_inputs\n     * @param {Tensor} model_inputs.input_values Float values of input raw speech waveform.\n     * @param {Tensor} model_inputs.attention_mask Mask to avoid performing convolution and attention on padding token indices. Mask values selected in [0, 1]\n     */\n    async _call(model_inputs) {\n        return new CausalLMOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * WavLM Model with a sequence classification head on top (a linear layer over the pooled output).\n */\nexport class WavLMForSequenceClassification extends WavLMPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * WavLM Model with an XVector feature extraction head on top for tasks like Speaker Verification.\n * \n * **Example:** Extract speaker embeddings with `WavLMForXVector`.\n * ```javascript\n * import { AutoProcessor, AutoModel, read_audio } from '@xenova/transformers';\n * \n * // Read and preprocess audio\n * const processor = await AutoProcessor.from_pretrained('Xenova/wavlm-base-plus-sv');\n * const url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/jfk.wav';\n * const audio = await read_audio(url, 16000);\n * const inputs = await processor(audio);\n * \n * // Run model with inputs\n * const model = await AutoModel.from_pretrained('Xenova/wavlm-base-plus-sv');\n * const outputs = await model(inputs);\n * // {\n * //   logits: Tensor {\n * //     dims: [ 1, 512 ],\n * //     type: 'float32',\n * //     data: Float32Array(512) [0.5847219228744507, ...],\n * //     size: 512\n * //   },\n * //   embeddings: Tensor {\n * //     dims: [ 1, 512 ],\n * //     type: 'float32',\n * //     data: Float32Array(512) [-0.09079201519489288, ...],\n * //     size: 512\n * //   }\n * // }\n * ```\n */\nexport class WavLMForXVector extends WavLMPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<XVectorOutput>} An object containing the model's output logits and speaker embeddings.\n     */\n    async _call(model_inputs) {\n        return new XVectorOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * WavLM Model with a frame classification head on top for tasks like Speaker Diarization.\n * \n * **Example:** Perform speaker diarization with `WavLMForAudioFrameClassification`.\n * ```javascript\n * import { AutoProcessor, AutoModelForAudioFrameClassification, read_audio } from '@xenova/transformers';\n * \n * // Read and preprocess audio\n * const processor = await AutoProcessor.from_pretrained('Xenova/wavlm-base-plus-sd');\n * const url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/jfk.wav';\n * const audio = await read_audio(url, 16000);\n * const inputs = await processor(audio);\n * \n * // Run model with inputs\n * const model = await AutoModelForAudioFrameClassification.from_pretrained('Xenova/wavlm-base-plus-sd');\n * const { logits } = await model(inputs);\n * // {\n * //   logits: Tensor {\n * //     dims: [ 1, 549, 2 ],  // [batch_size, num_frames, num_speakers]\n * //     type: 'float32',\n * //     data: Float32Array(1098) [-3.5301010608673096, ...],\n * //     size: 1098\n * //   }\n * // }\n * \n * const labels = logits[0].sigmoid().tolist().map(\n *     frames => frames.map(speaker => speaker > 0.5 ? 1 : 0)\n * );\n * console.log(labels); // labels is a one-hot array of shape (num_frames, num_speakers)\n * // [\n * //     [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0],\n * //     [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0],\n * //     [0, 0], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1],\n * //     ...\n * // ]\n * ```\n */\nexport class WavLMForAudioFrameClassification extends WavLMPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for sequence classification.\n     */\n    async _call(model_inputs) {\n        return new TokenClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n//////////////////////////////////////////////////\n// SpeechT5 models\n/**\n * An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained models.\n */\nexport class SpeechT5PreTrainedModel extends PreTrainedModel { };\n\n/**\n * The bare SpeechT5 Encoder-Decoder Model outputting raw hidden-states without any specific pre- or post-nets.\n */\nexport class SpeechT5Model extends SpeechT5PreTrainedModel { };\n\n/**\n * SpeechT5 Model with a speech encoder and a text decoder.\n * \n * **Example:** Generate speech from text with `SpeechT5ForSpeechToText`.\n * ```javascript\n * import { AutoTokenizer, AutoProcessor, SpeechT5ForTextToSpeech, SpeechT5HifiGan, Tensor } from '@xenova/transformers';\n * \n * // Load the tokenizer and processor\n * const tokenizer = await AutoTokenizer.from_pretrained('Xenova/speecht5_tts');\n * const processor = await AutoProcessor.from_pretrained('Xenova/speecht5_tts');\n * \n * // Load the models\n * // NOTE: We use the unquantized versions as they are more accurate\n * const model = await SpeechT5ForTextToSpeech.from_pretrained('Xenova/speecht5_tts', { quantized: false });\n * const vocoder = await SpeechT5HifiGan.from_pretrained('Xenova/speecht5_hifigan', { quantized: false });\n * \n * // Load speaker embeddings from URL\n * const speaker_embeddings_data = new Float32Array(\n *     await (await fetch('https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/speaker_embeddings.bin')).arrayBuffer()\n * );\n * const speaker_embeddings = new Tensor(\n *     'float32',\n *     speaker_embeddings_data,\n *     [1, speaker_embeddings_data.length]\n * )\n * \n * // Run tokenization\n * const { input_ids } = tokenizer('Hello, my dog is cute');\n * \n * // Generate waveform\n * const { waveform } = await model.generate_speech(input_ids, speaker_embeddings, { vocoder });\n * console.log(waveform)\n * // Tensor {\n * //   dims: [ 26112 ],\n * //   type: 'float32',\n * //   size: 26112,\n * //   data: Float32Array(26112) [ -0.00043630177970044315, -0.00018082228780258447, ... ],\n * // }\n * ```\n */\nexport class SpeechT5ForSpeechToText extends SpeechT5PreTrainedModel { }\n\n/**\n * SpeechT5 Model with a text encoder and a speech decoder.\n */\nexport class SpeechT5ForTextToSpeech extends SpeechT5PreTrainedModel {\n\n    /**\n     * Creates a new instance of the `SpeechT5ForTextToSpeech` class.\n     * @param {Object} config The model configuration.\n     * @param {any} session session for the model.\n     * @param {any} decoder_merged_session session for the decoder.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, decoder_merged_session, generation_config) {\n        super(config, session);\n        this.decoder_merged_session = decoder_merged_session;\n        this.generation_config = generation_config;\n\n        this.num_decoder_layers = this.config.decoder_layers;\n        this.num_decoder_heads = this.config.decoder_attention_heads;\n        this.decoder_dim_kv = this.config.hidden_size / this.num_decoder_heads;\n\n        this.num_encoder_layers = this.config.encoder_layers;\n        this.num_encoder_heads = this.config.encoder_attention_heads;\n        this.encoder_dim_kv = this.config.hidden_size / this.num_encoder_heads;\n    }\n\n    /**\n     * @typedef {Object} SpeechOutput\n     * @property {Tensor} [spectrogram] The predicted log-mel spectrogram of shape\n     * `(output_sequence_length, config.num_mel_bins)`. Returned when no `vocoder` is provided\n     * @property {Tensor} [waveform] The predicted waveform of shape `(num_frames,)`. Returned when a `vocoder` is provided.\n     * @property {Tensor} [cross_attentions] The outputs of the decoder's cross-attention layers of shape\n     * `(config.decoder_layers, config.decoder_attention_heads, output_sequence_length, input_sequence_length)`. returned when `output_cross_attentions` is `true`.\n     */\n\n    /**\n     * Converts a sequence of input tokens into a sequence of mel spectrograms, which are subsequently turned into a speech waveform using a vocoder.\n     * @param {Tensor} input_values Indices of input sequence tokens in the vocabulary.\n     * @param {Tensor} speaker_embeddings Tensor containing the speaker embeddings.\n     * @param {Object} options Optional parameters for generating speech.\n     * @param {number} [options.threshold=0.5] The generated sequence ends when the predicted stop token probability exceeds this value.\n     * @param {number} [options.minlenratio=0.0] Used to calculate the minimum required length for the output sequence.\n     * @param {number} [options.maxlenratio=20.0] Used to calculate the maximum allowed length for the output sequence.\n     * @param {Object} [options.vocoder=null] The vocoder that converts the mel spectrogram into a speech waveform. If `null`, the output is the mel spectrogram.\n     * @param {boolean} [options.output_cross_attentions=false] Whether or not to return the attentions tensors of the decoder's cross-attention layers.\n     * @returns {Promise<SpeechOutput>} A promise which resolves to an object containing the spectrogram, waveform, and cross-attention tensors.\n     */\n    async generate_speech(input_values, speaker_embeddings, {\n        threshold = 0.5,\n        minlenratio = 0.0,\n        maxlenratio = 20.0,\n        vocoder = null,\n        // output_cross_attentions = false, // TODO add\n    } = {}) {\n\n        const model_inputs = {\n            input_ids: input_values\n        }\n\n        const { encoder_outputs, encoder_attention_mask } = await encoderForward(this, model_inputs);\n\n        const r = encoder_outputs.dims[1] / this.config.reduction_factor;\n        const maxlen = Math.floor(r * maxlenratio);\n        const minlen = Math.floor(r * minlenratio);\n\n        const num_mel_bins = this.config.num_mel_bins;\n\n        let spectrogramParts = [];\n        let past_key_values = null;\n        let decoder_outputs = null;\n        let idx = 0;\n\n        while (true) {\n            ++idx;\n\n            const use_cache_branch = boolTensor(!!decoder_outputs);\n            let output_sequence;\n            if (decoder_outputs) {\n                output_sequence = decoder_outputs.output_sequence_out;\n            } else {\n                output_sequence = new Tensor(\n                    'float32',\n                    new Float32Array(num_mel_bins),\n                    [1, 1, num_mel_bins],\n                )\n            }\n            let decoderFeeds = {\n                use_cache_branch,\n                output_sequence,\n                encoder_attention_mask: encoder_attention_mask,\n                speaker_embeddings: speaker_embeddings,\n                encoder_hidden_states: encoder_outputs,\n            };\n\n            this.addPastKeyValues(decoderFeeds, past_key_values);\n            decoder_outputs = await sessionRun(this.decoder_merged_session, decoderFeeds);\n            past_key_values = this.getPastKeyValues(decoder_outputs, past_key_values);\n\n            const { prob, spectrum } = decoder_outputs;\n            spectrogramParts.push(spectrum);\n\n            if (idx >= minlen && (\n                // Finished when stop token or maximum length is reached.\n                Array.from(prob.data).filter(p => p >= threshold).length > 0 || idx >= maxlen\n            )) {\n                break;\n            }\n        }\n\n        const spectrogram = cat(spectrogramParts);\n        const { waveform } = await sessionRun(vocoder.session, { spectrogram });\n\n        return {\n            spectrogram,\n            waveform,\n            // cross_attentions: null, // TODO add\n        }\n    }\n}\n\n/**\n * HiFi-GAN vocoder.\n * \n * See [SpeechT5ForSpeechToText](./models#module_models.SpeechT5ForSpeechToText) for example usage.\n */\nexport class SpeechT5HifiGan extends PreTrainedModel {\n    main_input_name = 'spectrogram';\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// TrOCR models\nexport class TrOCRPreTrainedModel extends PreTrainedModel {\n    /**\n     * Creates a new instance of the `TrOCRPreTrainedModel` class.\n     * @param {Object} config The configuration of the model.\n     * @param {any} session The ONNX session containing the model weights.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, generation_config) {\n        super(config, session);\n        this.generation_config = generation_config;\n\n        // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n        this.config.pad_token_id = this.config.eos_token_id;\n\n        this.num_encoder_layers = this.num_decoder_layers = this.config.decoder_layers;\n        this.num_encoder_heads = this.num_decoder_heads = this.config.decoder_attention_heads;\n        this.encoder_dim_kv = this.decoder_dim_kv = this.config.d_model / this.num_decoder_heads;\n    }\n}\n\n/**\n * The TrOCR Decoder with a language modeling head.\n */\nexport class TrOCRForCausalLM extends TrOCRPreTrainedModel { }\n\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// Mistral models\n/**\n * The bare Mistral Model outputting raw hidden-states without any specific head on top.\n */\nexport class MistralPreTrainedModel extends PreTrainedModel {\n    /**\n     * Creates a new instance of the `MistralPreTrainedModel` class.\n     * @param {Object} config The configuration of the model.\n     * @param {any} session The ONNX session containing the model weights.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, generation_config) {\n        super(config, session);\n        this.generation_config = generation_config;\n\n        // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n        this.config.pad_token_id = this.config.eos_token_id\n\n        this.num_heads = this.config.num_key_value_heads;\n        this.num_layers = this.config.num_hidden_layers;\n        this.dim_kv = this.config.hidden_size / this.config.num_attention_heads;\n    }\n}\n\nexport class MistralModel extends MistralPreTrainedModel { }\n\nexport class MistralForCausalLM extends MistralPreTrainedModel { }\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// Starcoder2 models\n/**\n * The bare Starcoder2 Model outputting raw hidden-states without any specific head on top.\n */\nexport class Starcoder2PreTrainedModel extends PreTrainedModel {\n    /**\n     * Creates a new instance of the `Starcoder2PreTrainedModel` class.\n     * @param {Object} config The configuration of the model.\n     * @param {any} session The ONNX session containing the model weights.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, generation_config) {\n        super(config, session);\n        this.generation_config = generation_config;\n\n        // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n        this.config.pad_token_id = this.config.eos_token_id\n\n        this.num_heads = this.config.num_key_value_heads;\n        this.num_layers = this.config.num_hidden_layers;\n        this.dim_kv = this.config.hidden_size / this.config.num_attention_heads;\n    }\n}\n\nexport class Starcoder2Model extends Starcoder2PreTrainedModel { }\n\nexport class Starcoder2ForCausalLM extends Starcoder2PreTrainedModel { }\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// Falcon models\n/**\n * The bare Falcon Model outputting raw hidden-states without any specific head on top.\n */\nexport class FalconPreTrainedModel extends PreTrainedModel {\n    /**\n     * Creates a new instance of the `FalconPreTrainedModel` class.\n     * @param {Object} config The configuration of the model.\n     * @param {any} session The ONNX session containing the model weights.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, generation_config) {\n        super(config, session);\n        this.generation_config = generation_config;\n\n        // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n        this.config.pad_token_id = this.config.eos_token_id\n\n        this.num_heads = this.config.num_attention_heads;\n        this.num_layers = this.config.num_hidden_layers;\n        this.dim_kv = this.config.hidden_size / this.config.num_attention_heads;\n    }\n}\n\nexport class FalconModel extends FalconPreTrainedModel { }\n\nexport class FalconForCausalLM extends FalconPreTrainedModel { }\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// CLAP models\nexport class ClapPreTrainedModel extends PreTrainedModel { }\n\nexport class ClapModel extends ClapPreTrainedModel { }\n\n/**\n * CLAP Text Model with a projection layer on top (a linear layer on top of the pooled output).\n * \n * **Example:** Compute text embeddings with `ClapTextModelWithProjection`.\n * \n * ```javascript\n * import { AutoTokenizer, ClapTextModelWithProjection } from '@xenova/transformers';\n * \n * // Load tokenizer and text model\n * const tokenizer = await AutoTokenizer.from_pretrained('Xenova/clap-htsat-unfused');\n * const text_model = await ClapTextModelWithProjection.from_pretrained('Xenova/clap-htsat-unfused');\n * \n * // Run tokenization\n * const texts = ['a sound of a cat', 'a sound of a dog'];\n * const text_inputs = tokenizer(texts, { padding: true, truncation: true });\n * \n * // Compute embeddings\n * const { text_embeds } = await text_model(text_inputs);\n * // Tensor {\n * //   dims: [ 2, 512 ],\n * //   type: 'float32',\n * //   data: Float32Array(1024) [ ... ],\n * //   size: 1024\n * // }\n * ```\n */\nexport class ClapTextModelWithProjection extends ClapPreTrainedModel {\n\n    /** @type {PreTrainedModel.from_pretrained} */\n    static async from_pretrained(pretrained_model_name_or_path, options = {}) {\n        // Update default model file name if not provided\n        options.model_file_name ??= 'text_model';\n        return super.from_pretrained(pretrained_model_name_or_path, options);\n    }\n}\n\n/**\n * CLAP Audio Model with a projection layer on top (a linear layer on top of the pooled output).\n * \n * **Example:** Compute audio embeddings with `ClapAudioModelWithProjection`.\n * \n * ```javascript\n * import { AutoProcessor, ClapAudioModelWithProjection, read_audio } from '@xenova/transformers';\n * \n * // Load processor and audio model\n * const processor = await AutoProcessor.from_pretrained('Xenova/clap-htsat-unfused');\n * const audio_model = await ClapAudioModelWithProjection.from_pretrained('Xenova/clap-htsat-unfused');\n * \n * // Read audio and run processor\n * const audio = await read_audio('https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/cat_meow.wav');\n * const audio_inputs = await processor(audio);\n * \n * // Compute embeddings\n * const { audio_embeds } = await audio_model(audio_inputs);\n * // Tensor {\n * //   dims: [ 1, 512 ],\n * //   type: 'float32',\n * //   data: Float32Array(512) [ ... ],\n * //   size: 512\n * // }\n * ```\n */\nexport class ClapAudioModelWithProjection extends ClapPreTrainedModel {\n    /** @type {PreTrainedModel.from_pretrained} */\n    static async from_pretrained(pretrained_model_name_or_path, options = {}) {\n        // Update default model file name if not provided\n        options.model_file_name ??= 'audio_model';\n        return super.from_pretrained(pretrained_model_name_or_path, options);\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// VITS models\nexport class VitsPreTrainedModel extends PreTrainedModel { }\n\n/**\n * The complete VITS model, for text-to-speech synthesis.\n * \n * **Example:** Generate speech from text with `VitsModel`.\n * ```javascript\n * import { AutoTokenizer, VitsModel } from '@xenova/transformers';\n * \n * // Load the tokenizer and model\n * const tokenizer = await AutoTokenizer.from_pretrained('Xenova/mms-tts-eng');\n * const model = await VitsModel.from_pretrained('Xenova/mms-tts-eng');\n * \n * // Run tokenization\n * const inputs = tokenizer('I love transformers');\n * \n * // Generate waveform\n * const { waveform } = await model(inputs);\n * // Tensor {\n * //   dims: [ 1, 35328 ],\n * //   type: 'float32',\n * //   data: Float32Array(35328) [ ... ],\n * //   size: 35328,\n * // }\n * ```\n */\nexport class VitsModel extends VitsPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<VitsModelOutput>} The outputs for the VITS model.\n     */\n    async _call(model_inputs) {\n        return new VitsModelOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// Segformer models\nexport class SegformerPreTrainedModel extends PreTrainedModel { }\n\n/**\n * The bare SegFormer encoder (Mix-Transformer) outputting raw hidden-states without any specific head on top.\n */\nexport class SegformerModel extends SegformerPreTrainedModel { }\n\n/**\n * SegFormer Model transformer with an image classification head on top (a linear layer on top of the final hidden states) e.g. for ImageNet.\n */\nexport class SegformerForImageClassification extends SegformerPreTrainedModel { }\n\n/**\n * SegFormer Model transformer with an all-MLP decode head on top e.g. for ADE20k, CityScapes.\n */\nexport class SegformerForSemanticSegmentation extends SegformerPreTrainedModel { }\n\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// StableLm models\nexport class StableLmPreTrainedModel extends PreTrainedModel {\n    /**\n     * Creates a new instance of the `StableLmPreTrainedModel` class.\n     * @param {Object} config The configuration of the model.\n     * @param {any} session The ONNX session containing the model weights.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, generation_config) {\n        super(config, session);\n        this.generation_config = generation_config;\n\n        // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n        this.config.pad_token_id = this.config.eos_token_id\n\n        this.num_heads = this.config.num_attention_heads;\n        this.num_layers = this.config.num_hidden_layers;\n        this.dim_kv = this.config.hidden_size / this.num_heads;\n    }\n}\n\n/**\n * The bare StableLm Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class StableLmModel extends StableLmPreTrainedModel { }\n\n/**\n * StableLm Model with a `language modeling` head on top for Causal Language Modeling (with past).\n */\nexport class StableLmForCausalLM extends StableLmPreTrainedModel { }\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\nexport class EfficientNetPreTrainedModel extends PreTrainedModel { }\n\n/**\n * The bare EfficientNet model outputting raw features without any specific head on top.\n */\nexport class EfficientNetModel extends EfficientNetPreTrainedModel { }\n\n/**\n * EfficientNet Model with an image classification head on top (a linear layer on top of the pooled features).\n */\nexport class EfficientNetForImageClassification extends EfficientNetPreTrainedModel {\n    /**\n     * @param {any} model_inputs\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// AutoModels, used to simplify construction of PreTrainedModels\n// (uses config to instantiate correct class)\n\n/**\n * Base class of all AutoModels. Contains the `from_pretrained` function\n * which is used to instantiate pretrained models.\n */\nexport class PretrainedMixin {\n    /**\n     * Mapping from model type to model class.\n     * @type {Map<string, Object>[]}\n     */\n    static MODEL_CLASS_MAPPINGS = null;\n\n    /**\n     * Whether to attempt to instantiate the base class (`PretrainedModel`) if \n     * the model type is not found in the mapping.\n     */\n    static BASE_IF_FAIL = false;\n\n\n    /** @type {PreTrainedModel.from_pretrained} */\n    static async from_pretrained(pretrained_model_name_or_path, {\n        quantized = true,\n        progress_callback = null,\n        config = null,\n        cache_dir = null,\n        local_files_only = false,\n        revision = 'main',\n        model_file_name = null,\n    } = {}) {\n\n        let options = {\n            quantized,\n            progress_callback,\n            config,\n            cache_dir,\n            local_files_only,\n            revision,\n            model_file_name,\n        }\n        config = await AutoConfig.from_pretrained(pretrained_model_name_or_path, options);\n        if (!options.config) {\n            // If no config was passed, reuse this config for future processing\n            options.config = config;\n        }\n\n        if (!this.MODEL_CLASS_MAPPINGS) {\n            throw new Error(\"`MODEL_CLASS_MAPPINGS` not implemented for this type of `AutoClass`: \" + this.name);\n        }\n\n        for (let MODEL_CLASS_MAPPING of this.MODEL_CLASS_MAPPINGS) {\n            const modelInfo = MODEL_CLASS_MAPPING.get(config.model_type);\n            if (!modelInfo) {\n                continue; // Item not found in this mapping\n            }\n            return await modelInfo[1].from_pretrained(pretrained_model_name_or_path, options);\n        }\n\n        if (this.BASE_IF_FAIL) {\n            console.warn(`Unknown model class \"${config.model_type}\", attempting to construct from base class.`);\n            return await PreTrainedModel.from_pretrained(pretrained_model_name_or_path, options);\n        } else {\n            throw Error(`Unsupported model type: ${config.model_type}`)\n        }\n    }\n}\n\nconst MODEL_MAPPING_NAMES_ENCODER_ONLY = new Map([\n    ['bert', ['BertModel', BertModel]],\n    ['nomic_bert', ['NomicBertModel', NomicBertModel]],\n    ['roformer', ['RoFormerModel', RoFormerModel]],\n    ['electra', ['ElectraModel', ElectraModel]],\n    ['esm', ['EsmModel', EsmModel]],\n    ['convbert', ['ConvBertModel', ConvBertModel]],\n    ['camembert', ['CamembertModel', CamembertModel]],\n    ['deberta', ['DebertaModel', DebertaModel]],\n    ['deberta-v2', ['DebertaV2Model', DebertaV2Model]],\n    ['mpnet', ['MPNetModel', MPNetModel]],\n    ['albert', ['AlbertModel', AlbertModel]],\n    ['distilbert', ['DistilBertModel', DistilBertModel]],\n    ['roberta', ['RobertaModel', RobertaModel]],\n    ['xlm', ['XLMModel', XLMModel]],\n    ['xlm-roberta', ['XLMRobertaModel', XLMRobertaModel]],\n    ['clap', ['ClapModel', ClapModel]],\n    ['clip', ['CLIPModel', CLIPModel]],\n    ['clipseg', ['CLIPSegModel', CLIPSegModel]],\n    ['chinese_clip', ['ChineseCLIPModel', ChineseCLIPModel]],\n    ['siglip', ['SiglipModel', SiglipModel]],\n    ['mobilebert', ['MobileBertModel', MobileBertModel]],\n    ['squeezebert', ['SqueezeBertModel', SqueezeBertModel]],\n    ['wav2vec2', ['Wav2Vec2Model', Wav2Vec2Model]],\n    ['wav2vec2-bert', ['Wav2Vec2BertModel', Wav2Vec2BertModel]],\n    ['unispeech', ['UniSpeechModel', UniSpeechModel]],\n    ['unispeech-sat', ['UniSpeechSatModel', UniSpeechSatModel]],\n    ['hubert', ['HubertModel', HubertModel]],\n    ['wavlm', ['WavLMModel', WavLMModel]],\n    ['audio-spectrogram-transformer', ['ASTModel', ASTModel]],\n    ['vits', ['VitsModel', VitsModel]],\n\n    ['detr', ['DetrModel', DetrModel]],\n    ['table-transformer', ['TableTransformerModel', TableTransformerModel]],\n    ['vit', ['ViTModel', ViTModel]],\n    ['fastvit', ['FastViTModel', FastViTModel]],\n    ['mobilevit', ['MobileViTModel', MobileViTModel]],\n    ['mobilevitv2', ['MobileViTV2Model', MobileViTV2Model]],\n    ['owlvit', ['OwlViTModel', OwlViTModel]],\n    ['owlv2', ['Owlv2Model', Owlv2Model]],\n    ['beit', ['BeitModel', BeitModel]],\n    ['deit', ['DeiTModel', DeiTModel]],\n    ['convnext', ['ConvNextModel', ConvNextModel]],\n    ['convnextv2', ['ConvNextV2Model', ConvNextV2Model]],\n    ['dinov2', ['Dinov2Model', Dinov2Model]],\n    ['resnet', ['ResNetModel', ResNetModel]],\n    ['swin', ['SwinModel', SwinModel]],\n    ['swin2sr', ['Swin2SRModel', Swin2SRModel]],\n    ['donut-swin', ['DonutSwinModel', DonutSwinModel]],\n    ['yolos', ['YolosModel', YolosModel]],\n    ['dpt', ['DPTModel', DPTModel]],\n    ['glpn', ['GLPNModel', GLPNModel]],\n\n    ['hifigan', ['SpeechT5HifiGan', SpeechT5HifiGan]],\n    ['efficientnet', ['EfficientNetModel', EfficientNetModel]],\n\n]);\n\nconst MODEL_MAPPING_NAMES_ENCODER_DECODER = new Map([\n    ['t5', ['T5Model', T5Model]],\n    ['longt5', ['LongT5Model', LongT5Model]],\n    ['mt5', ['MT5Model', MT5Model]],\n    ['bart', ['BartModel', BartModel]],\n    ['mbart', ['MBartModel', MBartModel]],\n    ['marian', ['MarianModel', MarianModel]],\n    ['whisper', ['WhisperModel', WhisperModel]],\n    ['m2m_100', ['M2M100Model', M2M100Model]],\n    ['blenderbot', ['BlenderbotModel', BlenderbotModel]],\n    ['blenderbot-small', ['BlenderbotSmallModel', BlenderbotSmallModel]],\n]);\n\n\nconst MODEL_MAPPING_NAMES_DECODER_ONLY = new Map([\n    ['bloom', ['BloomModel', BloomModel]],\n    ['gpt2', ['GPT2Model', GPT2Model]],\n    ['gptj', ['GPTJModel', GPTJModel]],\n    ['gpt_bigcode', ['GPTBigCodeModel', GPTBigCodeModel]],\n    ['gpt_neo', ['GPTNeoModel', GPTNeoModel]],\n    ['gpt_neox', ['GPTNeoXModel', GPTNeoXModel]],\n    ['codegen', ['CodeGenModel', CodeGenModel]],\n    ['llama', ['LlamaModel', LlamaModel]],\n    ['qwen2', ['Qwen2Model', Qwen2Model]],\n    ['phi', ['PhiModel', PhiModel]],\n    ['mpt', ['MptModel', MptModel]],\n    ['opt', ['OPTModel', OPTModel]],\n    ['mistral', ['MistralModel', MistralModel]],\n    ['starcoder2', ['Starcoder2Model', Starcoder2Model]],\n    ['falcon', ['FalconModel', FalconModel]],\n]);\n\nconst MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING_NAMES = new Map([\n    ['speecht5', ['SpeechT5ForSpeechToText', SpeechT5ForSpeechToText]],\n    ['whisper', ['WhisperForConditionalGeneration', WhisperForConditionalGeneration]],\n]);\n\nconst MODEL_FOR_TEXT_TO_SPECTROGRAM_MAPPING_NAMES = new Map([\n    ['speecht5', ['SpeechT5ForTextToSpeech', SpeechT5ForTextToSpeech]],\n]);\n\nconst MODEL_FOR_TEXT_TO_WAVEFORM_MAPPING_NAMES = new Map([\n    ['vits', ['VitsModel', VitsModel]],\n]);\n\nconst MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES = new Map([\n    ['bert', ['BertForSequenceClassification', BertForSequenceClassification]],\n    ['roformer', ['RoFormerForSequenceClassification', RoFormerForSequenceClassification]],\n    ['electra', ['ElectraForSequenceClassification', ElectraForSequenceClassification]],\n    ['esm', ['EsmForSequenceClassification', EsmForSequenceClassification]],\n    ['convbert', ['ConvBertForSequenceClassification', ConvBertForSequenceClassification]],\n    ['camembert', ['CamembertForSequenceClassification', CamembertForSequenceClassification]],\n    ['deberta', ['DebertaForSequenceClassification', DebertaForSequenceClassification]],\n    ['deberta-v2', ['DebertaV2ForSequenceClassification', DebertaV2ForSequenceClassification]],\n    ['mpnet', ['MPNetForSequenceClassification', MPNetForSequenceClassification]],\n    ['albert', ['AlbertForSequenceClassification', AlbertForSequenceClassification]],\n    ['distilbert', ['DistilBertForSequenceClassification', DistilBertForSequenceClassification]],\n    ['roberta', ['RobertaForSequenceClassification', RobertaForSequenceClassification]],\n    ['xlm', ['XLMForSequenceClassification', XLMForSequenceClassification]],\n    ['xlm-roberta', ['XLMRobertaForSequenceClassification', XLMRobertaForSequenceClassification]],\n    ['bart', ['BartForSequenceClassification', BartForSequenceClassification]],\n    ['mbart', ['MBartForSequenceClassification', MBartForSequenceClassification]],\n    ['mobilebert', ['MobileBertForSequenceClassification', MobileBertForSequenceClassification]],\n    ['squeezebert', ['SqueezeBertForSequenceClassification', SqueezeBertForSequenceClassification]],\n]);\n\nconst MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING_NAMES = new Map([\n    ['bert', ['BertForTokenClassification', BertForTokenClassification]],\n    ['roformer', ['RoFormerForTokenClassification', RoFormerForTokenClassification]],\n    ['electra', ['ElectraForTokenClassification', ElectraForTokenClassification]],\n    ['esm', ['EsmForTokenClassification', EsmForTokenClassification]],\n    ['convbert', ['ConvBertForTokenClassification', ConvBertForTokenClassification]],\n    ['camembert', ['CamembertForTokenClassification', CamembertForTokenClassification]],\n    ['deberta', ['DebertaForTokenClassification', DebertaForTokenClassification]],\n    ['deberta-v2', ['DebertaV2ForTokenClassification', DebertaV2ForTokenClassification]],\n    ['mpnet', ['MPNetForTokenClassification', MPNetForTokenClassification]],\n    ['distilbert', ['DistilBertForTokenClassification', DistilBertForTokenClassification]],\n    ['roberta', ['RobertaForTokenClassification', RobertaForTokenClassification]],\n    ['xlm', ['XLMForTokenClassification', XLMForTokenClassification]],\n    ['xlm-roberta', ['XLMRobertaForTokenClassification', XLMRobertaForTokenClassification]],\n]);\n\nconst MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES = new Map([\n    ['t5', ['T5ForConditionalGeneration', T5ForConditionalGeneration]],\n    ['longt5', ['LongT5ForConditionalGeneration', LongT5ForConditionalGeneration]],\n    ['mt5', ['MT5ForConditionalGeneration', MT5ForConditionalGeneration]],\n    ['bart', ['BartForConditionalGeneration', BartForConditionalGeneration]],\n    ['mbart', ['MBartForConditionalGeneration', MBartForConditionalGeneration]],\n    ['marian', ['MarianMTModel', MarianMTModel]],\n    ['m2m_100', ['M2M100ForConditionalGeneration', M2M100ForConditionalGeneration]],\n    ['blenderbot', ['BlenderbotForConditionalGeneration', BlenderbotForConditionalGeneration]],\n    ['blenderbot-small', ['BlenderbotSmallForConditionalGeneration', BlenderbotSmallForConditionalGeneration]],\n]);\n\nconst MODEL_WITH_LM_HEAD_MAPPING_NAMES = new Map([\n    ['bloom', ['BloomForCausalLM', BloomForCausalLM]],\n    ['gpt2', ['GPT2LMHeadModel', GPT2LMHeadModel]],\n    ['gptj', ['GPTJForCausalLM', GPTJForCausalLM]],\n    ['gpt_bigcode', ['GPTBigCodeForCausalLM', GPTBigCodeForCausalLM]],\n    ['gpt_neo', ['GPTNeoForCausalLM', GPTNeoForCausalLM]],\n    ['gpt_neox', ['GPTNeoXForCausalLM', GPTNeoXForCausalLM]],\n    ['codegen', ['CodeGenForCausalLM', CodeGenForCausalLM]],\n    ['llama', ['LlamaForCausalLM', LlamaForCausalLM]],\n    ['qwen2', ['Qwen2ForCausalLM', Qwen2ForCausalLM]],\n    ['phi', ['PhiForCausalLM', PhiForCausalLM]],\n    ['mpt', ['MptForCausalLM', MptForCausalLM]],\n    ['opt', ['OPTForCausalLM', OPTForCausalLM]],\n    ['mbart', ['MBartForCausalLM', MBartForCausalLM]],\n    ['mistral', ['MistralForCausalLM', MistralForCausalLM]],\n    ['starcoder2', ['Starcoder2ForCausalLM', Starcoder2ForCausalLM]],\n    ['falcon', ['FalconForCausalLM', FalconForCausalLM]],\n    ['trocr', ['TrOCRForCausalLM', TrOCRForCausalLM]],\n    ['stablelm', ['StableLmForCausalLM', StableLmForCausalLM]],\n]);\n\nconst MODEL_FOR_MASKED_LM_MAPPING_NAMES = new Map([\n    ['bert', ['BertForMaskedLM', BertForMaskedLM]],\n    ['roformer', ['RoFormerForMaskedLM', RoFormerForMaskedLM]],\n    ['electra', ['ElectraForMaskedLM', ElectraForMaskedLM]],\n    ['esm', ['EsmForMaskedLM', EsmForMaskedLM]],\n    ['convbert', ['ConvBertForMaskedLM', ConvBertForMaskedLM]],\n    ['camembert', ['CamembertForMaskedLM', CamembertForMaskedLM]],\n    ['deberta', ['DebertaForMaskedLM', DebertaForMaskedLM]],\n    ['deberta-v2', ['DebertaV2ForMaskedLM', DebertaV2ForMaskedLM]],\n    ['mpnet', ['MPNetForMaskedLM', MPNetForMaskedLM]],\n    ['albert', ['AlbertForMaskedLM', AlbertForMaskedLM]],\n    ['distilbert', ['DistilBertForMaskedLM', DistilBertForMaskedLM]],\n    ['roberta', ['RobertaForMaskedLM', RobertaForMaskedLM]],\n    ['xlm', ['XLMWithLMHeadModel', XLMWithLMHeadModel]],\n    ['xlm-roberta', ['XLMRobertaForMaskedLM', XLMRobertaForMaskedLM]],\n    ['mobilebert', ['MobileBertForMaskedLM', MobileBertForMaskedLM]],\n    ['squeezebert', ['SqueezeBertForMaskedLM', SqueezeBertForMaskedLM]],\n]);\n\nconst MODEL_FOR_QUESTION_ANSWERING_MAPPING_NAMES = new Map([\n    ['bert', ['BertForQuestionAnswering', BertForQuestionAnswering]],\n    ['roformer', ['RoFormerForQuestionAnswering', RoFormerForQuestionAnswering]],\n    ['electra', ['ElectraForQuestionAnswering', ElectraForQuestionAnswering]],\n    ['convbert', ['ConvBertForQuestionAnswering', ConvBertForQuestionAnswering]],\n    ['camembert', ['CamembertForQuestionAnswering', CamembertForQuestionAnswering]],\n    ['deberta', ['DebertaForQuestionAnswering', DebertaForQuestionAnswering]],\n    ['deberta-v2', ['DebertaV2ForQuestionAnswering', DebertaV2ForQuestionAnswering]],\n    ['mpnet', ['MPNetForQuestionAnswering', MPNetForQuestionAnswering]],\n    ['albert', ['AlbertForQuestionAnswering', AlbertForQuestionAnswering]],\n    ['distilbert', ['DistilBertForQuestionAnswering', DistilBertForQuestionAnswering]],\n    ['roberta', ['RobertaForQuestionAnswering', RobertaForQuestionAnswering]],\n    ['xlm', ['XLMForQuestionAnswering', XLMForQuestionAnswering]],\n    ['xlm-roberta', ['XLMRobertaForQuestionAnswering', XLMRobertaForQuestionAnswering]],\n    ['mobilebert', ['MobileBertForQuestionAnswering', MobileBertForQuestionAnswering]],\n    ['squeezebert', ['SqueezeBertForQuestionAnswering', SqueezeBertForQuestionAnswering]],\n]);\n\nconst MODEL_FOR_VISION_2_SEQ_MAPPING_NAMES = new Map([\n    ['vision-encoder-decoder', ['VisionEncoderDecoderModel', VisionEncoderDecoderModel]],\n]);\n\nconst MODEL_FOR_DOCUMENT_QUESTION_ANSWERING_MAPPING_NAMES = new Map([\n    ['vision-encoder-decoder', ['VisionEncoderDecoderModel', VisionEncoderDecoderModel]],\n]);\n\nconst MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING_NAMES = new Map([\n    ['vit', ['ViTForImageClassification', ViTForImageClassification]],\n    ['fastvit', ['FastViTForImageClassification', FastViTForImageClassification]],\n    ['mobilevit', ['MobileViTForImageClassification', MobileViTForImageClassification]],\n    ['mobilevitv2', ['MobileViTV2ForImageClassification', MobileViTV2ForImageClassification]],\n    ['beit', ['BeitForImageClassification', BeitForImageClassification]],\n    ['deit', ['DeiTForImageClassification', DeiTForImageClassification]],\n    ['convnext', ['ConvNextForImageClassification', ConvNextForImageClassification]],\n    ['convnextv2', ['ConvNextV2ForImageClassification', ConvNextV2ForImageClassification]],\n    ['dinov2', ['Dinov2ForImageClassification', Dinov2ForImageClassification]],\n    ['resnet', ['ResNetForImageClassification', ResNetForImageClassification]],\n    ['swin', ['SwinForImageClassification', SwinForImageClassification]],\n    ['segformer', ['SegformerForImageClassification', SegformerForImageClassification]],\n    ['efficientnet', ['EfficientNetForImageClassification', EfficientNetForImageClassification]],\n]);\n\nconst MODEL_FOR_OBJECT_DETECTION_MAPPING_NAMES = new Map([\n    ['detr', ['DetrForObjectDetection', DetrForObjectDetection]],\n    ['table-transformer', ['TableTransformerForObjectDetection', TableTransformerForObjectDetection]],\n    ['yolos', ['YolosForObjectDetection', YolosForObjectDetection]],\n]);\n\nconst MODEL_FOR_ZERO_SHOT_OBJECT_DETECTION_MAPPING_NAMES = new Map([\n    ['owlvit', ['OwlViTForObjectDetection', OwlViTForObjectDetection]],\n    ['owlv2', ['Owlv2ForObjectDetection', Owlv2ForObjectDetection]],\n]);\n\nconst MODEL_FOR_IMAGE_SEGMENTATION_MAPPING_NAMES = new Map([\n    ['detr', ['DetrForSegmentation', DetrForSegmentation]],\n    ['clipseg', ['CLIPSegForImageSegmentation', CLIPSegForImageSegmentation]],\n]);\n\nconst MODEL_FOR_SEMANTIC_SEGMENTATION_MAPPING_NAMES = new Map([\n    ['segformer', ['SegformerForSemanticSegmentation', SegformerForSemanticSegmentation]],\n]);\n\nconst MODEL_FOR_MASK_GENERATION_MAPPING_NAMES = new Map([\n    ['sam', ['SamModel', SamModel]],\n]);\n\nconst MODEL_FOR_CTC_MAPPING_NAMES = new Map([\n    ['wav2vec2', ['Wav2Vec2ForCTC', Wav2Vec2ForCTC]],\n    ['wav2vec2-bert', ['Wav2Vec2BertForCTC', Wav2Vec2BertForCTC]],\n    ['unispeech', ['UniSpeechForCTC', UniSpeechForCTC]],\n    ['unispeech-sat', ['UniSpeechSatForCTC', UniSpeechSatForCTC]],\n    ['wavlm', ['WavLMForCTC', WavLMForCTC]],\n    ['hubert', ['HubertForCTC', HubertForCTC]],\n]);\n\nconst MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING_NAMES = new Map([\n    ['wav2vec2', ['Wav2Vec2ForSequenceClassification', Wav2Vec2ForSequenceClassification]],\n    ['wav2vec2-bert', ['Wav2Vec2BertForSequenceClassification', Wav2Vec2BertForSequenceClassification]],\n    ['unispeech', ['UniSpeechForSequenceClassification', UniSpeechForSequenceClassification]],\n    ['unispeech-sat', ['UniSpeechSatForSequenceClassification', UniSpeechSatForSequenceClassification]],\n    ['wavlm', ['WavLMForSequenceClassification', WavLMForSequenceClassification]],\n    ['hubert', ['HubertForSequenceClassification', HubertForSequenceClassification]],\n    ['audio-spectrogram-transformer', ['ASTForAudioClassification', ASTForAudioClassification]],\n]);\n\nconst MODEL_FOR_AUDIO_XVECTOR_MAPPING_NAMES = new Map([\n    ['wavlm', ['WavLMForXVector', WavLMForXVector]],\n]);\n\nconst MODEL_FOR_AUDIO_FRAME_CLASSIFICATION_MAPPING_NAMES = new Map([\n    ['unispeech-sat', ['UniSpeechSatForAudioFrameClassification', UniSpeechSatForAudioFrameClassification]],\n    ['wavlm', ['WavLMForAudioFrameClassification', WavLMForAudioFrameClassification]],\n    ['wav2vec2', ['Wav2Vec2ForAudioFrameClassification', Wav2Vec2ForAudioFrameClassification]],\n]);\n\nconst MODEL_FOR_IMAGE_MATTING_MAPPING_NAMES = new Map([\n    ['vitmatte', ['VitMatteForImageMatting', VitMatteForImageMatting]],\n]);\n\nconst MODEL_FOR_IMAGE_TO_IMAGE_MAPPING_NAMES = new Map([\n    ['swin2sr', ['Swin2SRForImageSuperResolution', Swin2SRForImageSuperResolution]],\n])\n\nconst MODEL_FOR_DEPTH_ESTIMATION_MAPPING_NAMES = new Map([\n    ['dpt', ['DPTForDepthEstimation', DPTForDepthEstimation]],\n    ['depth_anything', ['DepthAnythingForDepthEstimation', DepthAnythingForDepthEstimation]],\n    ['glpn', ['GLPNForDepthEstimation', GLPNForDepthEstimation]],\n])\n\n// NOTE: This is custom to Transformers.js, and is necessary because certain models\n// (e.g., CLIP) are split into vision and text components\nconst MODEL_FOR_IMAGE_FEATURE_EXTRACTION_MAPPING_NAMES = new Map([\n    ['clip', ['CLIPVisionModelWithProjection', CLIPVisionModelWithProjection]],\n    ['siglip', ['SiglipVisionModel', SiglipVisionModel]],\n])\n\nconst MODEL_CLASS_TYPE_MAPPING = [\n    [MODEL_MAPPING_NAMES_ENCODER_ONLY, MODEL_TYPES.EncoderOnly],\n    [MODEL_MAPPING_NAMES_ENCODER_DECODER, MODEL_TYPES.EncoderDecoder],\n    [MODEL_MAPPING_NAMES_DECODER_ONLY, MODEL_TYPES.DecoderOnly],\n    [MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES, MODEL_TYPES.EncoderOnly],\n    [MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING_NAMES, MODEL_TYPES.EncoderOnly],\n    [MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES, MODEL_TYPES.Seq2Seq],\n    [MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING_NAMES, MODEL_TYPES.Seq2Seq],\n    [MODEL_WITH_LM_HEAD_MAPPING_NAMES, MODEL_TYPES.DecoderOnly],\n    [MODEL_FOR_MASKED_LM_MAPPING_NAMES, MODEL_TYPES.EncoderOnly],\n    [MODEL_FOR_QUESTION_ANSWERING_MAPPING_NAMES, MODEL_TYPES.EncoderOnly],\n    [MODEL_FOR_VISION_2_SEQ_MAPPING_NAMES, MODEL_TYPES.Vision2Seq],\n    [MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING_NAMES, MODEL_TYPES.EncoderOnly],\n    [MODEL_FOR_IMAGE_SEGMENTATION_MAPPING_NAMES, MODEL_TYPES.EncoderOnly],\n    [MODEL_FOR_SEMANTIC_SEGMENTATION_MAPPING_NAMES, MODEL_TYPES.EncoderOnly],\n    [MODEL_FOR_IMAGE_MATTING_MAPPING_NAMES, MODEL_TYPES.EncoderOnly],\n    [MODEL_FOR_IMAGE_TO_IMAGE_MAPPING_NAMES, MODEL_TYPES.EncoderOnly],\n    [MODEL_FOR_DEPTH_ESTIMATION_MAPPING_NAMES, MODEL_TYPES.EncoderOnly],\n    [MODEL_FOR_OBJECT_DETECTION_MAPPING_NAMES, MODEL_TYPES.EncoderOnly],\n    [MODEL_FOR_ZERO_SHOT_OBJECT_DETECTION_MAPPING_NAMES, MODEL_TYPES.EncoderOnly],\n    [MODEL_FOR_MASK_GENERATION_MAPPING_NAMES, MODEL_TYPES.MaskGeneration],\n    [MODEL_FOR_CTC_MAPPING_NAMES, MODEL_TYPES.EncoderOnly],\n    [MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING_NAMES, MODEL_TYPES.EncoderOnly],\n    [MODEL_FOR_TEXT_TO_SPECTROGRAM_MAPPING_NAMES, MODEL_TYPES.Seq2Seq],\n    [MODEL_FOR_TEXT_TO_WAVEFORM_MAPPING_NAMES, MODEL_TYPES.EncoderOnly],\n    [MODEL_FOR_AUDIO_XVECTOR_MAPPING_NAMES, MODEL_TYPES.EncoderOnly],\n    [MODEL_FOR_AUDIO_FRAME_CLASSIFICATION_MAPPING_NAMES, MODEL_TYPES.EncoderOnly],\n\n    // Custom:\n    [MODEL_FOR_IMAGE_FEATURE_EXTRACTION_MAPPING_NAMES, MODEL_TYPES.EncoderOnly],\n];\n\nfor (const [mappings, type] of MODEL_CLASS_TYPE_MAPPING) {\n    // @ts-ignore\n    for (const [name, model] of mappings.values()) {\n        MODEL_TYPE_MAPPING.set(name, type);\n        MODEL_CLASS_TO_NAME_MAPPING.set(model, name);\n        MODEL_NAME_TO_CLASS_MAPPING.set(name, model);\n    }\n}\n\nconst CUSTOM_MAPPING = [\n    ['CLIPTextModelWithProjection', CLIPTextModelWithProjection, MODEL_TYPES.EncoderOnly],\n    ['SiglipTextModel', SiglipTextModel, MODEL_TYPES.EncoderOnly],\n    ['ClapTextModelWithProjection', ClapTextModelWithProjection, MODEL_TYPES.EncoderOnly],\n    ['ClapAudioModelWithProjection', ClapAudioModelWithProjection, MODEL_TYPES.EncoderOnly],\n]\nfor (const [name, model, type] of CUSTOM_MAPPING) {\n    MODEL_TYPE_MAPPING.set(name, type);\n    MODEL_CLASS_TO_NAME_MAPPING.set(model, name);\n    MODEL_NAME_TO_CLASS_MAPPING.set(name, model);\n}\n\n\n/**\n * Helper class which is used to instantiate pretrained models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModel.from_pretrained('bert-base-uncased');\n */\nexport class AutoModel extends PretrainedMixin {\n    /** @type {Map<string, Object>[]} */\n    // @ts-ignore\n    static MODEL_CLASS_MAPPINGS = MODEL_CLASS_TYPE_MAPPING.map(x => x[0]);\n    static BASE_IF_FAIL = true;\n}\n\n/**\n * Helper class which is used to instantiate pretrained sequence classification models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english');\n */\nexport class AutoModelForSequenceClassification extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES];\n}\n\n/**\n * Helper class which is used to instantiate pretrained token classification models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModelForTokenClassification.from_pretrained('Davlan/distilbert-base-multilingual-cased-ner-hrl');\n */\nexport class AutoModelForTokenClassification extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING_NAMES];\n}\n\n/**\n * Helper class which is used to instantiate pretrained sequence-to-sequence models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModelForSeq2SeqLM.from_pretrained('t5-small');\n */\nexport class AutoModelForSeq2SeqLM extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES];\n}\n\n/**\n * Helper class which is used to instantiate pretrained sequence-to-sequence speech-to-text models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModelForSpeechSeq2Seq.from_pretrained('openai/whisper-tiny.en');\n */\nexport class AutoModelForSpeechSeq2Seq extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING_NAMES];\n}\n\n/**\n * Helper class which is used to instantiate pretrained sequence-to-sequence text-to-spectrogram models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModelForTextToSpectrogram.from_pretrained('microsoft/speecht5_tts');\n */\nexport class AutoModelForTextToSpectrogram extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_TEXT_TO_SPECTROGRAM_MAPPING_NAMES];\n}\n\n/**\n * Helper class which is used to instantiate pretrained text-to-waveform models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModelForTextToSpectrogram.from_pretrained('facebook/mms-tts-eng');\n */\nexport class AutoModelForTextToWaveform extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_TEXT_TO_WAVEFORM_MAPPING_NAMES];\n}\n\n/**\n * Helper class which is used to instantiate pretrained causal language models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModelForCausalLM.from_pretrained('gpt2');\n */\nexport class AutoModelForCausalLM extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_WITH_LM_HEAD_MAPPING_NAMES];\n}\n\n/**\n * Helper class which is used to instantiate pretrained masked language models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModelForMaskedLM.from_pretrained('bert-base-uncased');\n */\nexport class AutoModelForMaskedLM extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_MASKED_LM_MAPPING_NAMES];\n}\n\n/**\n * Helper class which is used to instantiate pretrained question answering models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModelForQuestionAnswering.from_pretrained('distilbert-base-cased-distilled-squad');\n */\nexport class AutoModelForQuestionAnswering extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_QUESTION_ANSWERING_MAPPING_NAMES];\n}\n\n/**\n * Helper class which is used to instantiate pretrained vision-to-sequence models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModelForVision2Seq.from_pretrained('nlpconnect/vit-gpt2-image-captioning');\n */\nexport class AutoModelForVision2Seq extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_VISION_2_SEQ_MAPPING_NAMES];\n}\n\n/**\n * Helper class which is used to instantiate pretrained image classification models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModelForImageClassification.from_pretrained('google/vit-base-patch16-224');\n */\nexport class AutoModelForImageClassification extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING_NAMES];\n}\n\n/**\n * Helper class which is used to instantiate pretrained image segmentation models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModelForImageSegmentation.from_pretrained('facebook/detr-resnet-50-panoptic');\n */\nexport class AutoModelForImageSegmentation extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_IMAGE_SEGMENTATION_MAPPING_NAMES];\n}\n\n/**\n * Helper class which is used to instantiate pretrained image segmentation models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModelForSemanticSegmentation.from_pretrained('nvidia/segformer-b3-finetuned-cityscapes-1024-1024');\n */\nexport class AutoModelForSemanticSegmentation extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_SEMANTIC_SEGMENTATION_MAPPING_NAMES];\n}\n\n/**\n * Helper class which is used to instantiate pretrained object detection models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModelForObjectDetection.from_pretrained('facebook/detr-resnet-50');\n */\nexport class AutoModelForObjectDetection extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_OBJECT_DETECTION_MAPPING_NAMES];\n}\n\nexport class AutoModelForZeroShotObjectDetection extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_ZERO_SHOT_OBJECT_DETECTION_MAPPING_NAMES];\n}\n\n\n/**\n * Helper class which is used to instantiate pretrained mask generation models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModelForMaskGeneration.from_pretrained('Xenova/sam-vit-base');\n */\nexport class AutoModelForMaskGeneration extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_MASK_GENERATION_MAPPING_NAMES];\n}\n\nexport class AutoModelForCTC extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_CTC_MAPPING_NAMES];\n}\n\nexport class AutoModelForAudioClassification extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING_NAMES];\n}\n\nexport class AutoModelForXVector extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_AUDIO_XVECTOR_MAPPING_NAMES];\n}\n\nexport class AutoModelForAudioFrameClassification extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_AUDIO_FRAME_CLASSIFICATION_MAPPING_NAMES];\n}\n\nexport class AutoModelForDocumentQuestionAnswering extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_DOCUMENT_QUESTION_ANSWERING_MAPPING_NAMES];\n}\n\nexport class AutoModelForImageMatting extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_IMAGE_MATTING_MAPPING_NAMES];\n}\n\nexport class AutoModelForImageToImage extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_IMAGE_TO_IMAGE_MAPPING_NAMES];\n}\n\nexport class AutoModelForDepthEstimation extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_DEPTH_ESTIMATION_MAPPING_NAMES];\n}\n\nexport class AutoModelForImageFeatureExtraction extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_IMAGE_FEATURE_EXTRACTION_MAPPING_NAMES];\n}\n\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\nexport class Seq2SeqLMOutput extends ModelOutput {\n    /**\n     * @param {Object} output The output of the model.\n     * @param {Tensor} output.logits The output logits of the model.\n     * @param {Tensor} output.past_key_values An tensor of key/value pairs that represent the previous state of the model.\n     * @param {Tensor} output.encoder_outputs The output of the encoder in a sequence-to-sequence model.\n     * @param {Tensor} [output.decoder_attentions] Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the self-attention heads.\n     * @param {Tensor} [output.cross_attentions] Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the weighted average in the cross-attention heads.\n     */\n    constructor({ logits, past_key_values, encoder_outputs, decoder_attentions = null, cross_attentions = null }) {\n        super();\n        this.logits = logits;\n        this.past_key_values = past_key_values;\n        this.encoder_outputs = encoder_outputs;\n        this.decoder_attentions = decoder_attentions;\n        this.cross_attentions = cross_attentions;\n    }\n}\n\n/**\n * Base class for outputs of sentence classification models.\n */\nexport class SequenceClassifierOutput extends ModelOutput {\n    /**\n     * @param {Object} output The output of the model.\n     * @param {Tensor} output.logits classification (or regression if config.num_labels==1) scores (before SoftMax).\n     */\n    constructor({ logits }) {\n        super();\n        this.logits = logits;\n    }\n}\n\n/**\n * Base class for outputs of XVector models.\n */\nexport class XVectorOutput extends ModelOutput {\n    /**\n     * @param {Object} output The output of the model.\n     * @param {Tensor} output.logits Classification hidden states before AMSoftmax, of shape `(batch_size, config.xvector_output_dim)`.\n     * @param {Tensor} output.embeddings Utterance embeddings used for vector similarity-based retrieval, of shape `(batch_size, config.xvector_output_dim)`.\n     */\n    constructor({ logits, embeddings }) {\n        super();\n        this.logits = logits;\n        this.embeddings = embeddings;\n    }\n}\n\n/**\n * Base class for outputs of token classification models.\n */\nexport class TokenClassifierOutput extends ModelOutput {\n    /**\n     * @param {Object} output The output of the model.\n     * @param {Tensor} output.logits Classification scores (before SoftMax).\n     */\n    constructor({ logits }) {\n        super();\n        this.logits = logits;\n    }\n}\n\n/**\n * Base class for masked language models outputs.\n */\nexport class MaskedLMOutput extends ModelOutput {\n    /**\n     * @param {Object} output The output of the model.\n     * @param {Tensor} output.logits Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n     */\n    constructor({ logits }) {\n        super();\n        this.logits = logits;\n    }\n}\n\n/**\n * Base class for outputs of question answering models.\n */\nexport class QuestionAnsweringModelOutput extends ModelOutput {\n    /**\n     * @param {Object} output The output of the model.\n     * @param {Tensor} output.start_logits Span-start scores (before SoftMax).\n     * @param {Tensor} output.end_logits Span-end scores (before SoftMax).\n     */\n    constructor({ start_logits, end_logits }) {\n        super();\n        this.start_logits = start_logits;\n        this.end_logits = end_logits;\n    }\n}\n\n\n/**\n * Base class for causal language model (or autoregressive) outputs.\n */\nexport class CausalLMOutput extends ModelOutput {\n    /**\n     * @param {Object} output The output of the model.\n     * @param {Tensor} output.logits Prediction scores of the language modeling head (scores for each vocabulary token before softmax).\n     */\n    constructor({ logits }) {\n        super();\n        this.logits = logits;\n    }\n}\n\n/**\n * Base class for causal language model (or autoregressive) outputs.\n */\nexport class CausalLMOutputWithPast extends ModelOutput {\n    /**\n     * @param {Object} output The output of the model.\n     * @param {Tensor} output.logits Prediction scores of the language modeling head (scores for each vocabulary token before softmax).\n     * @param {Tensor} output.past_key_values Contains pre-computed hidden-states (key and values in the self-attention blocks)\n     * that can be used (see `past_key_values` input) to speed up sequential decoding.\n     */\n    constructor({ logits, past_key_values }) {\n        super();\n        this.logits = logits;\n        this.past_key_values = past_key_values;\n    }\n}\n\nexport class ImageMattingOutput extends ModelOutput {\n    /**\n     * @param {Object} output The output of the model.\n     * @param {Tensor} output.alphas Estimated alpha values, of shape `(batch_size, num_channels, height, width)`.\n     */\n    constructor({ alphas }) {\n        super();\n        this.alphas = alphas;\n    }\n}\n\n/**\n * Describes the outputs for the VITS model.\n */\nexport class VitsModelOutput extends ModelOutput {\n    /**\n     * @param {Object} output The output of the model.\n     * @param {Tensor} output.waveform The final audio waveform predicted by the model, of shape `(batch_size, sequence_length)`.\n     * @param {Tensor} output.spectrogram The log-mel spectrogram predicted at the output of the flow model.\n     * This spectrogram is passed to the Hi-Fi GAN decoder model to obtain the final audio waveform.\n     */\n    constructor({ waveform, spectrogram }) {\n        super();\n        this.waveform = waveform;\n        this.spectrogram = spectrogram;\n    }\n}\n"],"mappings":"AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,SACIA,UAAU,QACP,cAAc;AAErB,SACIC,QAAQ,EACRC,gBAAgB,EAChBC,YAAY,EACZC,WAAW,QACR,iBAAiB;AAExB,SACIC,YAAY,EACZC,YAAY,QACT,gBAAgB;AAEvB,SACIC,mBAAmB,EACnBC,gBAAgB,EAChBC,0BAA0B,EAC1BC,6BAA6B,EAC7BC,6BAA6B,EAC7BC,oCAAoC,EACpCC,+BAA+B,EAC/BC,4BAA4B,EAC5BC,gCAAgC,EAChCC,yBAAyB,EACzBC,wBAAwB,EACxBC,iCAAiC,EAEjCC,OAAO,QACJ,uBAAuB;AAE9B,SACIC,GAAG,EACHC,kBAAkB,EAClBC,IAAI,EACJC,SAAS,EACTC,KAAK,EACLC,QAAQ,EACRC,MAAM,QACH,mBAAmB;AAE1B,SAASC,kBAAkB,EAAEC,IAAI,QAAQ,oBAAoB;AAC7D,SAASC,YAAY,QAAQ,mBAAmB;AAChD,MAAM;EAAEC,gBAAgB;EAAEJ,MAAM,EAAEK,UAAU;EAAEC;AAAI,CAAC,GAAGJ,IAAI;;AAE1D;;AAEA;AACA;AACA,MAAMK,WAAW,GAAG;EAChBC,WAAW,EAAE,CAAC;EACdC,cAAc,EAAE,CAAC;EACjBC,OAAO,EAAE,CAAC;EACVC,UAAU,EAAE,CAAC;EACbC,WAAW,EAAE,CAAC;EACdC,cAAc,EAAE;AACpB,CAAC;AACD;;AAGA;AACA;;AAEA;AACA,MAAMC,kBAAkB,GAAG,IAAIC,GAAG,CAAC,CAAC;AACpC,MAAMC,2BAA2B,GAAG,IAAID,GAAG,CAAC,CAAC;AAC7C,MAAME,2BAA2B,GAAG,IAAIF,GAAG,CAAC,CAAC;;AAG7C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,eAAeG,gBAAgBA,CAACC,6BAA6B,EAAEC,QAAQ,EAAEC,OAAO,EAAE;EAC9E;EACA,IAAIC,aAAa,GAAG,QAAQF,QAAQ,GAAGC,OAAO,CAACE,SAAS,GAAG,YAAY,GAAG,EAAE,OAAO;EACnF,IAAIC,MAAM,GAAG,MAAM7C,YAAY,CAACwC,6BAA6B,EAAEG,aAAa,EAAE,IAAI,EAAED,OAAO,CAAC;EAE5F,IAAI;IACA,OAAO,MAAMjB,gBAAgB,CAACqB,MAAM,CAACD,MAAM,EAAE;MACzCvB;IACJ,CAAC,CAAC;EACN,CAAC,CAAC,OAAOyB,GAAG,EAAE;IACV;IACA,IAAIzB,kBAAkB,CAAC0B,MAAM,KAAK,CAAC,IAAI1B,kBAAkB,CAAC,CAAC,CAAC,KAAK,MAAM,EAAE;MACrE,MAAMyB,GAAG;IACb;IAEAE,OAAO,CAACC,IAAI,CAACH,GAAG,CAAC;IACjBE,OAAO,CAACC,IAAI,CACR,oFAAoF,GACpF,8BACJ,CAAC;IACD,OAAO,MAAMzB,gBAAgB,CAACqB,MAAM,CAACD,MAAM,EAAE;MACzCvB,kBAAkB,EAAE,CAAC,MAAM;IAC/B,CAAC,CAAC;EACN;AACJ;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS6B,cAAcA,CAACC,OAAO,EAAEC,MAAM,EAAE;EACrC;AACJ;AACA;AACA;EACI,MAAMC,aAAa,GAAGC,MAAM,CAACT,MAAM,CAAC,IAAI,CAAC;EACzC,MAAMU,aAAa,GAAG,EAAE;EACxB,KAAK,MAAMC,SAAS,IAAIL,OAAO,CAACM,UAAU,EAAE;IACxC,MAAMC,MAAM,GAAGN,MAAM,CAACI,SAAS,CAAC;IAChC;IACA;IACA;IACA,IAAI,EAAEE,MAAM,YAAYtC,MAAM,CAAC,EAAE;MAC7BmC,aAAa,CAACI,IAAI,CAACH,SAAS,CAAC;MAC7B;IACJ;IACA;IACA;IACA;IACAH,aAAa,CAACG,SAAS,CAAC,GAAG9B,GAAG,CAACkC,IAAI,CAACC,KAAK,GAAGH,MAAM,CAACI,KAAK,CAAC,CAAC,GAAGJ,MAAM;EACvE;EACA,IAAIH,aAAa,CAACR,MAAM,GAAG,CAAC,EAAE;IAC1B,MAAM,IAAIgB,KAAK,CACX,4EAA4ER,aAAa,CAACS,IAAI,CAAC,IAAI,CAAC,GAAG,CAAC;EAChH;EAEA,MAAMC,iBAAiB,GAAGX,MAAM,CAACY,IAAI,CAACd,MAAM,CAAC,CAACL,MAAM;EACpD,MAAMoB,eAAe,GAAGhB,OAAO,CAACM,UAAU,CAACV,MAAM;EACjD,IAAIkB,iBAAiB,GAAGE,eAAe,EAAE;IACrC;IACA;IACA,IAAIC,OAAO,GAAGd,MAAM,CAACY,IAAI,CAACd,MAAM,CAAC,CAACiB,MAAM,CAACb,SAAS,IAAI,CAACL,OAAO,CAACM,UAAU,CAACa,QAAQ,CAACd,SAAS,CAAC,CAAC;IAC9FR,OAAO,CAACC,IAAI,CAAC,2CAA2CgB,iBAAiB,MAAME,eAAe,6CAA6CC,OAAO,CAACJ,IAAI,CAAC,IAAI,CAAC,IAAI,CAAC;EACtK;EAEA,OAAOX,aAAa;AACxB;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,eAAekB,UAAUA,CAACpB,OAAO,EAAEC,MAAM,EAAE;EACvC,MAAMC,aAAa,GAAGH,cAAc,CAACC,OAAO,EAAEC,MAAM,CAAC;EACrD,IAAI;IACA;IACA,IAAIoB,MAAM,GAAG,MAAMrB,OAAO,CAACsB,GAAG,CAACpB,aAAa,CAAC;IAC7CmB,MAAM,GAAGE,cAAc,CAACF,MAAM,CAAC;IAC/B,OAAOA,MAAM;EACjB,CAAC,CAAC,OAAOG,CAAC,EAAE;IACR;IACA3B,OAAO,CAAC4B,KAAK,CAAC,8CAA8CD,CAAC,IAAI,CAAC;IAClE3B,OAAO,CAAC4B,KAAK,CAAC,wBAAwB,EAAEvB,aAAa,CAAC;IACtD,MAAMsB,CAAC;EACX;AACJ;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,SAASD,cAAcA,CAACG,GAAG,EAAE;EACzB,KAAK,IAAIC,IAAI,IAAID,GAAG,EAAE;IAClB,IAAIA,GAAG,CAACC,IAAI,CAAC,YAAYrD,UAAU,EAAE;MACjCoD,GAAG,CAACC,IAAI,CAAC,GAAG,IAAI1D,MAAM,CAACyD,GAAG,CAACC,IAAI,CAAC,CAAC;IACrC,CAAC,MAAM,IAAI,OAAOD,GAAG,CAACC,IAAI,CAAC,KAAK,QAAQ,EAAE;MACtCJ,cAAc,CAACG,GAAG,CAACC,IAAI,CAAC,CAAC;IAC7B;EACJ;EACA,OAAOD,GAAG;AACd;;AAGA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAASE,WAAWA,CAACC,KAAK,EAAE;EACxB,IAAIA,KAAK,YAAY5D,MAAM,EAAE;IACzB,OAAO4D,KAAK;EAChB;EACA;EACA,IAAIA,KAAK,CAACjC,MAAM,KAAK,CAAC,EAAE;IACpB,MAAMgB,KAAK,CAAC,yBAAyB,CAAC;EAC1C;EAEA,IAAIkB,KAAK,CAACC,OAAO,CAACF,KAAK,CAAC,CAAC,CAAC,CAAC,EAAE;IACzB;IACA,IAAIA,KAAK,CAACG,IAAI,CAACC,CAAC,IAAIA,CAAC,CAACrC,MAAM,KAAKiC,KAAK,CAAC,CAAC,CAAC,CAACjC,MAAM,CAAC,EAAE;MAC/C,MAAMgB,KAAK,CAAC,4KAA4K,CAAC;IAC7L;IAEA,OAAO,IAAI3C,MAAM,CAAC,OAAO,EACrBiE,aAAa,CAACC,IAAI,CAACN,KAAK,CAACO,IAAI,CAAC,CAAC,CAACC,GAAG,CAACJ,CAAC,IAAIK,MAAM,CAACL,CAAC,CAAC,CAAC,CAAC,EACpD,CAACJ,KAAK,CAACjC,MAAM,EAAEiC,KAAK,CAAC,CAAC,CAAC,CAACjC,MAAM,CAClC,CAAC;EACL,CAAC,MAAM;IACH;IACA,OAAO,IAAI3B,MAAM,CAAC,OAAO,EACrBiE,aAAa,CAACC,IAAI,CAACN,KAAK,CAACQ,GAAG,CAACJ,CAAC,IAAIK,MAAM,CAACL,CAAC,CAAC,CAAC,CAAC,EAC7C,CAAC,CAAC,EAAEJ,KAAK,CAACjC,MAAM,CACpB,CAAC;EACL;AACJ;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS2C,oBAAoBA,CAACC,IAAI,EAAEC,MAAM,EAAE;EAExC;EACA,IAAIC,YAAY,GAAGF,IAAI,CAACG,MAAM,CAACD,YAAY,IAAI,IAAI;EACnD,IAAIE,YAAY,GAAGJ,IAAI,CAACG,MAAM,CAACC,YAAY,IAAI,IAAI;EACnD,IAAInG,gBAAgB,CAACmG,YAAY,CAAC,EAAE;IAChCA,YAAY,GAAG,CAACA,YAAY,CAAC;EACjC;EAEA,IAAIC,sBAAsB,GAAGJ,MAAM,CAACK,OAAO,CAACJ,YAAY,CAAC,KAAK,CAAC,CAAC;EAChE,IAAIK,sCAAsC,GAAIH,YAAY,KAAK,IAAI,IAAK,CAACA,YAAY,CAACzB,QAAQ,CAACuB,YAAY,CAAC;EAE5G,IAAIG,sBAAsB,IAAIE,sCAAsC,EAAE;IAClE,IAAIC,IAAI,GAAGd,aAAa,CAACC,IAAI;IACzB;IACA;IACAM,MAAM,CAACO,IAAI,CAACX,GAAG,CAACJ,CAAC,IAAIA,CAAC,IAAIS,YAAY,CAC1C,CAAC;IACD,OAAO,IAAIzE,MAAM,CAAC,OAAO,EAAE+E,IAAI,EAAEP,MAAM,CAACQ,IAAI,CAAC;EACjD,CAAC,MAAM;IACH,OAAOnF,SAAS,CAAC2E,MAAM,CAAC;EAC5B;AACJ;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAASS,kBAAkBA,CAAClD,OAAO,EAAEmD,KAAK,EAAEC,gBAAgB,EAAE;EAC1D,IAAI,CAACpD,OAAO,CAACM,UAAU,CAACa,QAAQ,CAAC,cAAc,CAAC,EAAE;EAElD,MAAM6B,IAAI,GAAG,IAAId,aAAa,CAACiB,KAAK,CAACE,cAAc,CAACL,IAAI,CAACpD,MAAM,CAAC;;EAEhE;EACA,KAAK,IAAI0D,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGH,KAAK,CAACE,cAAc,CAACJ,IAAI,CAAC,CAAC,CAAC,EAAE,EAAEK,CAAC,EAAE;IACnD,IAAIC,KAAK,GAAGD,CAAC,GAAGH,KAAK,CAACE,cAAc,CAACJ,IAAI,CAAC,CAAC,CAAC;IAC5C,IAAIO,GAAG,GAAGlB,MAAM,CAAC,CAAC,CAAC;IACnB,KAAK,IAAImB,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGN,KAAK,CAACE,cAAc,CAACJ,IAAI,CAAC,CAAC,CAAC,EAAE,EAAEQ,CAAC,EAAE;MACnD,MAAMC,KAAK,GAAGH,KAAK,GAAGE,CAAC;MACvB,IAAIN,KAAK,CAACE,cAAc,CAACL,IAAI,CAACU,KAAK,CAAC,KAAK,EAAE,EAAE;QACzCV,IAAI,CAACU,KAAK,CAAC,GAAGpB,MAAM,CAAC,CAAC,CAAC;MAC3B,CAAC,MAAM;QAAE;QACLU,IAAI,CAACU,KAAK,CAAC,GAAGF,GAAG;QACjBA,GAAG,IAAIL,KAAK,CAACE,cAAc,CAACL,IAAI,CAACU,KAAK,CAAC;MAC3C;IACJ;EACJ;EAEAP,KAAK,CAACQ,YAAY,GAAG,IAAI1F,MAAM,CAAC,OAAO,EAAE+E,IAAI,EAAEG,KAAK,CAACE,cAAc,CAACJ,IAAI,CAAC;EAEzE,IAAIG,gBAAgB,EAAE;IAClBD,KAAK,CAACQ,YAAY,GAAGR,KAAK,CAACQ,YAAY,CAACC,KAAK,CAAC,IAAI,EAAE,CAAC,CAAC,CAAC,CAACC,UAAU,CAAC,CAAC,CAAC,CAAC;EAC1E;AACJ;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,SAASC,UAAUA,CAACC,KAAK,EAAE;EACvB,OAAO,IAAI9F,MAAM,CAAC,MAAM,EAAE,CAAC8F,KAAK,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;AAC3C;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,eAAeC,cAAcA,CAACxB,IAAI,EAAEyB,YAAY,EAAE;EAE9C,IAAI;IAAEC,eAAe;IAAEC;EAAgB,CAAC,GAAGF,YAAY;EAEvD,IAAI,CAACC,eAAe,EAAE;IAClB;IACAA,eAAe,GAAG,CAAC,MAAME,cAAc,CAAC5B,IAAI,EAAEyB,YAAY,CAAC,EAAEI,iBAAiB;EAClF;EACA,IAAIC,YAAY,GAAG;IACfC,SAAS,EAAEN,YAAY,CAACO,iBAAiB;IACzCC,qBAAqB,EAAEP;EAC3B,CAAC;EACD,MAAMd,gBAAgB,GAAG,CAAC,CAACe,eAAe;EAE1C,IAAI3B,IAAI,CAACkC,sBAAsB,CAACpE,UAAU,CAACa,QAAQ,CAAC,kBAAkB,CAAC,EAAE;IACrEmD,YAAY,CAAClB,gBAAgB,GAAGU,UAAU,CAACV,gBAAgB,CAAC;EAChE;EAEA,IAAIZ,IAAI,CAACkC,sBAAsB,CAACpE,UAAU,CAACa,QAAQ,CAAC,wBAAwB,CAAC,EAAE;IAC3EmD,YAAY,CAACK,sBAAsB,GAAGV,YAAY,CAACZ,cAAc;EACrE;EAEAH,kBAAkB,CAACV,IAAI,CAACkC,sBAAsB,EAAEJ,YAAY,EAAElB,gBAAgB,CAAC;EAC/EZ,IAAI,CAACoC,gBAAgB,CAACN,YAAY,EAAEH,eAAe,CAAC;EAEpD,MAAMU,cAAc,GAAG,MAAMzD,UAAU,CAACoB,IAAI,CAACkC,sBAAsB,EAAEJ,YAAY,CAAC;EAClF,IAAIQ,MAAM,GAAGD,cAAc,CAACC,MAAM;EAClCX,eAAe,GAAG3B,IAAI,CAACuC,gBAAgB,CAACF,cAAc,EAAEV,eAAe,CAAC;;EAExE;EACA,MAAMa,KAAK,GAAGxC,IAAI,CAACyC,aAAa,CAACJ,cAAc,CAAC;EAEhD,OAAO,IAAIK,eAAe,CAAC;IAAEJ,MAAM;IAAEX,eAAe;IAAED,eAAe;IAAE,GAAGc;EAAM,CAAC,CAAC;AACtF;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAASG,iBAAiBA,CAAC3C,IAAI,EAAE4C,aAAa,EAAEC,iBAAiB,EAAEC,eAAe,EAAE;EAChF,IAAIC,KAAK,GAAG,EAAE;EACd,IAAIC,MAAM,GAAG,CAAC;;EAEd;EACA,MAAMC,uBAAuB,GAAGjD,IAAI,CAACiD,uBAAuB,IAAI,IAAI;;EAEpE;EACA,IAAIjB,iBAAiB,GACjBa,iBAAiB,CAACb,iBAAiB,IAChCa,iBAAiB,CAACK,sBAAsB,IACxCL,iBAAiB,CAACM,YAAY,IAC9BN,iBAAiB,CAACzC,YAAY;;EAErC;EACA;EACA,IAAI4B,iBAAiB,YAAYvG,MAAM,EAAE;IACrCuG,iBAAiB,GAAGA,iBAAiB,CAACoB,MAAM,CAAC,CAAC,CAACxD,IAAI,CAAC,CAAC;EACzD,CAAC,MAAM,IAAI,CAACN,KAAK,CAACC,OAAO,CAACyC,iBAAiB,CAAC,EAAE;IAC1CA,iBAAiB,GAAG,CAACA,iBAAiB,CAAC;EAC3C;EAEA,KAAK,IAAI/B,MAAM,IAAI2C,aAAa,EAAE;IAC9B;IACA;IACA;IACA3C,MAAM,CAACQ,IAAI,GAAG,CAAC,CAAC,EAAE,GAAGR,MAAM,CAACQ,IAAI,CAAC;;IAEjC;IACA,IAAIM,KAAK,GAAG;MACRtD,MAAM,EAAEwC,MAAM;MACdyB,eAAe,EAAE,IAAI;MACrB2B,kBAAkB,EAAE,IAAI;MAExBC,gBAAgB,EAAEtB,iBAAiB;MACnCuB,IAAI,EAAE,KAAK;MACXC,KAAK,EAAE,CAAC;MACRC,EAAE,EAAET,MAAM,EAAE,CAAC;IACjB,CAAC;IAED,IAAIC,uBAAuB,EAAE;MACzBlC,KAAK,CAACF,cAAc,GAAGd,oBAAoB,CAACC,IAAI,EAAEC,MAAM,CAAC;IAC7D;IAEA8C,KAAK,CAAC/E,IAAI,CAAC+C,KAAK,CAAC;EACrB;EAEA,OAAOgC,KAAK;AAChB;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,eAAeW,cAAcA,CAAC1D,IAAI,EAAE2D,IAAI,EAAE;EACtC,MAAMC,UAAU,GAAG5D,IAAI,CAAC6D,eAAe;EAEvC,IAAI7B,iBAAiB,GAAG2B,IAAI,CAACL,gBAAgB;EAC7C,IAAIK,IAAI,CAACN,kBAAkB,EAAE;IACzB;IACA;IACArB,iBAAiB,GAAGA,iBAAiB,CAACZ,KAAK,CAAC,CAAC,CAAC,CAAC;EACnD;;EAEA;EACA,IAAIK,YAAY,GAAG;IACf,CAACmC,UAAU,GAAGD,IAAI,CAAClG,MAAM;IACzBuE,iBAAiB,EAAE5C,WAAW,CAAC4C,iBAAiB,CAAC;IACjDN,eAAe,EAAEiC,IAAI,CAACjC,eAAe;IACrCC,eAAe,EAAEgC,IAAI,CAACN,kBAAkB,EAAE1B;EAC9C,CAAC;EACD,IAAIgC,IAAI,CAAC9C,cAAc,EAAE;IACrBY,YAAY,CAACZ,cAAc,GAAG8C,IAAI,CAAC9C,cAAc;EACrD;;EAEA;EACA,IAAIhC,MAAM,GAAG,MAAMmB,IAAI,CAAC8D,OAAO,CAACrC,YAAY,CAAC;;EAE7C;EACAkC,IAAI,CAACN,kBAAkB,GAAGxE,MAAM;EAChC8E,IAAI,CAACjC,eAAe,GAAG7C,MAAM,CAAC6C,eAAe;EAE7C,OAAO7C,MAAM;AACjB;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,SAASkF,iBAAiBA,CAACJ,IAAI,EAAEK,UAAU,EAAE;EACzCL,IAAI,CAACL,gBAAgB,GAAG,CAAC,GAAGK,IAAI,CAACL,gBAAgB,EAAEU,UAAU,CAAC;AAClE;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,eAAepC,cAAcA,CAAC5B,IAAI,EAAEyB,YAAY,EAAE;EAC9C,MAAMwC,YAAY,GAAGtG,MAAM,CAACT,MAAM,CAAC,IAAI,CAAC;EACxC,KAAK,MAAMgH,GAAG,IAAIlE,IAAI,CAACxC,OAAO,CAACM,UAAU,EAAE;IACvCmG,YAAY,CAACC,GAAG,CAAC,GAAGzC,YAAY,CAACyC,GAAG,CAAC;EACzC;EACA,IAAIlE,IAAI,CAACxC,OAAO,CAACM,UAAU,CAACa,QAAQ,CAAC,gBAAgB,CAAC,IAAI,CAACsF,YAAY,CAACE,cAAc,EAAE;IACpF;IACA;IACAF,YAAY,CAACE,cAAc,GAAG,IAAI1I,MAAM,CACpC,OAAO,EACP,IAAIiE,aAAa,CAACuE,YAAY,CAAClC,SAAS,CAACvB,IAAI,CAACpD,MAAM,CAAC,EACrD6G,YAAY,CAAClC,SAAS,CAACtB,IAC3B,CAAC;EACL;EACA,OAAO,MAAM7B,UAAU,CAACoB,IAAI,CAACxC,OAAO,EAAEyG,YAAY,CAAC;AACvD;;AAGA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,eAAeG,cAAcA,CAACpE,IAAI,EAAEyB,YAAY,EAAE;EAC9C,IAAI;IAAEM,SAAS;IAAEJ,eAAe;IAAEd;EAAe,CAAC,GAAGY,YAAY;EACjE,IAAIK,YAAY,GAAG;IACfC,SAAS,EAAEA,SAAS;IACpBlB,cAAc,EAAEA,cAAc,IAAId,oBAAoB,CAACC,IAAI,EAAE+B,SAAS;EAC1E,CAAC;EACD,MAAMnB,gBAAgB,GAAG,CAAC,CAACe,eAAe;EAE1C,IAAI3B,IAAI,CAACxC,OAAO,CAACM,UAAU,CAACa,QAAQ,CAAC,kBAAkB,CAAC,EAAE;IACtDmD,YAAY,CAAClB,gBAAgB,GAAGU,UAAU,CAACV,gBAAgB,CAAC;EAChE;EAEAF,kBAAkB,CAACV,IAAI,CAACxC,OAAO,EAAEsE,YAAY,EAAElB,gBAAgB,CAAC;EAEhEZ,IAAI,CAACoC,gBAAgB,CAACN,YAAY,EAAEH,eAAe,CAAC;EAEpD,IAAIU,cAAc,GAAG,MAAMzD,UAAU,CAACoB,IAAI,CAACxC,OAAO,EAAEsE,YAAY,CAAC;EAEjE,IAAIQ,MAAM,GAAGD,cAAc,CAACC,MAAM;EAElCX,eAAe,GAAG3B,IAAI,CAACuC,gBAAgB,CAACF,cAAc,EAAEV,eAAe,CAAC;EACxE,OAAO;IAAEW,MAAM;IAAEX;EAAgB,CAAC;AACtC;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS0C,iBAAiBA,CAACrE,IAAI,EAAE4C,aAAa,EAAEC,iBAAiB,EAAEC,eAAe,EAAEwB,qBAAqB,EAAE;EACvG,IAAIvB,KAAK,GAAG,EAAE;EAEd,IAAIC,MAAM,GAAG,CAAC;EACd,KAAK,IAAI/C,MAAM,IAAI2C,aAAa,EAAE;IAC9B,IAAIU,gBAAgB,GAAGrD,MAAM,CAACmD,MAAM,CAAC,CAAC,CAACvD,GAAG,CAAC0E,MAAM,CAAC;;IAElD;IACA;IACA;IACAtE,MAAM,CAACQ,IAAI,GAAG,CAAC,CAAC,EAAE,GAAGR,MAAM,CAACQ,IAAI,CAAC;IAEjC,IAAI+D,SAAS;IACb,IAAIF,qBAAqB,EAAE;MACvBE,SAAS,GAAGF,qBAAqB,CAACtB,MAAM,CAAC;MACzCwB,SAAS,CAAC/D,IAAI,GAAG,CAAC,CAAC,EAAE,GAAG+D,SAAS,CAAC/D,IAAI,CAAC;IAE3C,CAAC,MAAM;MACH+D,SAAS,GAAGzE,oBAAoB,CAACC,IAAI,EAAEC,MAAM,CAAC;IAClD;IAEA,IAAIc,KAAK,GAAG;MACR0D,KAAK,EAAExE,MAAM;MACbyE,eAAe,EAAEzE,MAAM;MACvBY,cAAc,EAAE2D,SAAS;MACzBnB,kBAAkB,EAAE,IAAI;MAExBC,gBAAgB,EAAEA,gBAAgB;MAClCqB,iBAAiB,EAAE7B,eAAe;MAElCS,IAAI,EAAE,KAAK;MACXC,KAAK,EAAE,CAAC;MACRC,EAAE,EAAET,MAAM,EAAE,CAAC;IACjB,CAAC;IAEDD,KAAK,CAAC/E,IAAI,CAAC+C,KAAK,CAAC;EACrB;EACA,OAAOgC,KAAK;AAChB;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,eAAe6B,cAAcA,CAAC5E,IAAI,EAAE2D,IAAI,EAAE;EACtC,IAAIkB,YAAY,GAAG,IAAInF,aAAa,CAACiE,IAAI,CAACL,gBAAgB,CAAClG,MAAM,CAAC,CAAC0H,IAAI,CAAC,EAAE,CAAC;;EAE3E;EACA,IAAIrD,YAAY,GAAG;IACfM,SAAS,EAAE4B,IAAI,CAACe,eAAe;IAC/B7D,cAAc,EAAE,IAAIpF,MAAM,CACtB,OAAO,EACPoJ,YAAY,EACZ,CAAC,CAAC,EAAEA,YAAY,CAACzH,MAAM,CAC3B,CAAC;IACDuE,eAAe,EAAEgC,IAAI,CAACN,kBAAkB,EAAE1B;EAC9C,CAAC;;EAED;EACA,IAAI9C,MAAM,GAAG,MAAMmB,IAAI,CAAC8D,OAAO,CAACrC,YAAY,CAAC;;EAE7C;EACAkC,IAAI,CAACN,kBAAkB,GAAGxE,MAAM;EAEhC,OAAOA,MAAM;AACjB;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,SAASkG,iBAAiBA,CAACpB,IAAI,EAAEK,UAAU,EAAE;EACzCL,IAAI,CAACL,gBAAgB,GAAG,CAAC,GAAGK,IAAI,CAACL,gBAAgB,EAAEU,UAAU,CAAC;EAC9DL,IAAI,CAACe,eAAe,GAAG,IAAIjJ,MAAM,CAAC,OAAO,EAAE,CAACqE,MAAM,CAACkE,UAAU,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;AAC5E;;AAEA;;AAEA;AACA;AACA;AACA;AACA,OAAO,MAAMgB,eAAe,SAAShL,QAAQ,CAAC;EAC1C6J,eAAe,GAAG,WAAW;;EAE7B;AACJ;AACA;AACA;AACA;EACIoB,WAAWA,CAAC9E,MAAM,EAAE3C,OAAO,EAAE;IACzB,KAAK,CAAC,CAAC;IAEP,IAAI,CAAC2C,MAAM,GAAGA,MAAM;IACpB,IAAI,CAAC3C,OAAO,GAAGA,OAAO;IAEtB,MAAM0H,SAAS,GAAGxI,2BAA2B,CAACyI,GAAG,CAAC,IAAI,CAACF,WAAW,CAAC;IACnE,MAAMG,SAAS,GAAG7I,kBAAkB,CAAC4I,GAAG,CAACD,SAAS,CAAC;IAEnD,IAAI,CAACG,YAAY,GAAG,KAAK;IACzB,IAAI,CAACC,QAAQ,GAAG,IAAI;IACpB,IAAI,CAACC,cAAc,GAAG,IAAI;IAC1B,IAAI,CAACC,WAAW,GAAG,IAAI;IACvB,IAAI,CAACC,QAAQ,GAAG,IAAI;IACpB,IAAIL,SAAS,KAAKpJ,WAAW,CAACK,WAAW,EAAE;MACvC,IAAI,CAACgJ,YAAY,GAAG,IAAI;MAExB,IAAI,CAACC,QAAQ,GAAGV,cAAc;MAC9B,IAAI,CAACW,cAAc,GAAGlB,iBAAiB;MACvC,IAAI,CAACmB,WAAW,GAAGT,iBAAiB;MACpC,IAAI,CAACU,QAAQ,GAAGrB,cAAc;IAElC,CAAC,MAAM,IAAIgB,SAAS,KAAKpJ,WAAW,CAACG,OAAO,IAAIiJ,SAAS,KAAKpJ,WAAW,CAACI,UAAU,EAAE;MAClF,IAAI,CAACiJ,YAAY,GAAG,IAAI;MAExB,IAAI,CAACC,QAAQ,GAAG5B,cAAc;MAC9B,IAAI,CAAC6B,cAAc,GAAG5C,iBAAiB;MACvC,IAAI,CAAC6C,WAAW,GAAGzB,iBAAiB;MACpC,IAAI,CAAC0B,QAAQ,GAAGjE,cAAc;IAElC,CAAC,MAAM,IAAI4D,SAAS,KAAKpJ,WAAW,CAACE,cAAc,EAAE;MACjD,IAAI,CAACuJ,QAAQ,GAAG7D,cAAc;IAElC,CAAC,MAAM;MAAE;MACL,IAAI,CAAC6D,QAAQ,GAAG7D,cAAc;IAClC;EACJ;;EAEA;AACJ;AACA;AACA;AACA;EACI,MAAM8D,OAAOA,CAAA,EAAG;IACZ,MAAMC,QAAQ,GAAG,EAAE;IACnB,KAAK,IAAIzB,GAAG,IAAIvG,MAAM,CAACY,IAAI,CAAC,IAAI,CAAC,EAAE;MAC/B,MAAMqH,IAAI,GAAG,IAAI,CAAC1B,GAAG,CAAC;MACtB;MACA,IAAI0B,IAAI,YAAY/J,gBAAgB,EAAE;QAClC8J,QAAQ,CAAC3H,IAAI,CAAC4H,IAAI,CAACC,OAAO,CAACH,OAAO,CAAC,CAAC,CAAC;MACzC;IACJ;IACA,OAAO,MAAMI,OAAO,CAACC,GAAG,CAACJ,QAAQ,CAAC;EACtC;;EAEA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACI,aAAaK,eAAeA,CAACpJ,6BAA6B,EAAE;IACxDI,SAAS,GAAG,IAAI;IAChBiJ,iBAAiB,GAAG,IAAI;IACxB9F,MAAM,GAAG,IAAI;IACb+F,SAAS,GAAG,IAAI;IAChBC,gBAAgB,GAAG,KAAK;IACxBC,QAAQ,GAAG,MAAM;IACjBC,eAAe,GAAG;EACtB,CAAC,GAAG,CAAC,CAAC,EAAE;IAEJ,IAAIvJ,OAAO,GAAG;MACVE,SAAS;MACTiJ,iBAAiB;MACjB9F,MAAM;MACN+F,SAAS;MACTC,gBAAgB;MAChBC,QAAQ;MACRC;IACJ,CAAC;IAED,MAAMnB,SAAS,GAAGxI,2BAA2B,CAACyI,GAAG,CAAC,IAAI,CAAC;IACvD,MAAMC,SAAS,GAAG7I,kBAAkB,CAAC4I,GAAG,CAACD,SAAS,CAAC;IAEnD,IAAIoB,IAAI;IACR,IAAIlB,SAAS,KAAKpJ,WAAW,CAACK,WAAW,EAAE;MACvCiK,IAAI,GAAG,MAAMR,OAAO,CAACC,GAAG,CAAC,CACrBhM,UAAU,CAACiM,eAAe,CAACpJ,6BAA6B,EAAEE,OAAO,CAAC,EAClEH,gBAAgB,CAACC,6BAA6B,EAAEE,OAAO,CAACuJ,eAAe,IAAI,sBAAsB,EAAEvJ,OAAO,CAAC,EAC3GzC,YAAY,CAACuC,6BAA6B,EAAE,wBAAwB,EAAE,KAAK,EAAEE,OAAO,CAAC,CACxF,CAAC;IAEN,CAAC,MAAM,IAAIsI,SAAS,KAAKpJ,WAAW,CAACG,OAAO,IAAIiJ,SAAS,KAAKpJ,WAAW,CAACI,UAAU,EAAE;MAClFkK,IAAI,GAAG,MAAMR,OAAO,CAACC,GAAG,CAAC,CACrBhM,UAAU,CAACiM,eAAe,CAACpJ,6BAA6B,EAAEE,OAAO,CAAC,EAClEH,gBAAgB,CAACC,6BAA6B,EAAE,eAAe,EAAEE,OAAO,CAAC,EACzEH,gBAAgB,CAACC,6BAA6B,EAAE,sBAAsB,EAAEE,OAAO,CAAC,EAChFzC,YAAY,CAACuC,6BAA6B,EAAE,wBAAwB,EAAE,KAAK,EAAEE,OAAO,CAAC,CACxF,CAAC;IAEN,CAAC,MAAM,IAAIsI,SAAS,KAAKpJ,WAAW,CAACM,cAAc,EAAE;MACjDgK,IAAI,GAAG,MAAMR,OAAO,CAACC,GAAG,CAAC,CACrBhM,UAAU,CAACiM,eAAe,CAACpJ,6BAA6B,EAAEE,OAAO,CAAC,EAClEH,gBAAgB,CAACC,6BAA6B,EAAE,gBAAgB,EAAEE,OAAO,CAAC,EAC1EH,gBAAgB,CAACC,6BAA6B,EAAE,6BAA6B,EAAEE,OAAO,CAAC,CAC1F,CAAC;IAEN,CAAC,MAAM,IAAIsI,SAAS,KAAKpJ,WAAW,CAACE,cAAc,EAAE;MACjDoK,IAAI,GAAG,MAAMR,OAAO,CAACC,GAAG,CAAC,CACrBhM,UAAU,CAACiM,eAAe,CAACpJ,6BAA6B,EAAEE,OAAO,CAAC,EAClEH,gBAAgB,CAACC,6BAA6B,EAAE,eAAe,EAAEE,OAAO,CAAC,EACzEH,gBAAgB,CAACC,6BAA6B,EAAE,sBAAsB,EAAEE,OAAO,CAAC,CACnF,CAAC;IAEN,CAAC,MAAM;MAAE;MACL,IAAIsI,SAAS,KAAKpJ,WAAW,CAACC,WAAW,EAAE;QACvCoB,OAAO,CAACC,IAAI,CAAC,mBAAmB4H,SAAS,IAAI/E,MAAM,EAAEoG,UAAU,qIAAqI,CAAC;MACzM;MACAD,IAAI,GAAG,MAAMR,OAAO,CAACC,GAAG,CAAC,CACrBhM,UAAU,CAACiM,eAAe,CAACpJ,6BAA6B,EAAEE,OAAO,CAAC,EAClEH,gBAAgB,CAACC,6BAA6B,EAAEE,OAAO,CAACuJ,eAAe,IAAI,OAAO,EAAEvJ,OAAO,CAAC,CAC/F,CAAC;IACN;;IAEA;IACA,OAAO,IAAI,IAAI,CAAC,GAAGwJ,IAAI,CAAC;EAC5B;;EAEA;AACJ;AACA;AACA;AACA;EACI,MAAME,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,MAAM,IAAI,CAACqC,OAAO,CAACrC,YAAY,CAAC;EAC3C;;EAEA;AACJ;AACA;AACA;AACA;AACA;AACA;EACI,MAAMqC,OAAOA,CAACrC,YAAY,EAAE;IACxB,OAAO,MAAM,IAAI,CAACgE,QAAQ,CAAC,IAAI,EAAEhE,YAAY,CAAC;EAClD;;EAEA;AACJ;AACA;AACA;AACA;AACA;EACIgF,qBAAqBA,CACjB5D,iBAAiB,EACjB6D,oBAAoB;EACpB;EACA;EACAC,gBAAgB,GAAG,IAAI,EACzB;IACE,MAAMC,UAAU,GAAG,IAAItM,mBAAmB,CAAC,CAAC;;IAE5C;IACA;IACA;IACA;IACA;IACA;IACA;;IAEA;IACA;IACA;IACA;IACA;IACA;;IAEA,IAAIuI,iBAAiB,CAACgE,kBAAkB,KAAK,IAAI,IAAIhE,iBAAiB,CAACgE,kBAAkB,KAAK,GAAG,EAAE;MAC/FD,UAAU,CAAC5I,IAAI,CAAC,IAAIlD,gCAAgC,CAAC+H,iBAAiB,CAACgE,kBAAkB,CAAC,CAAC;IAC/F;IAEA,IAAIhE,iBAAiB,CAACiE,oBAAoB,KAAK,IAAI,IAAIjE,iBAAiB,CAACiE,oBAAoB,GAAG,CAAC,EAAE;MAC/FF,UAAU,CAAC5I,IAAI,CAAC,IAAInD,4BAA4B,CAACgI,iBAAiB,CAACiE,oBAAoB,CAAC,CAAC;IAC7F;;IAEA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;;IAEA,IAAIjE,iBAAiB,CAACkE,aAAa,KAAK,IAAI,EAAE;MAC1CH,UAAU,CAAC5I,IAAI,CAAC,IAAIjD,yBAAyB,CAAC8H,iBAAiB,CAACkE,aAAa,EAAElE,iBAAiB,CAACzC,YAAY,CAAC,CAAC;IACnH;IAEA,IAAIyC,iBAAiB,CAACmE,UAAU,KAAK,IAAI,IAAInE,iBAAiB,CAACzC,YAAY,KAAK,IAAI,IAAIyC,iBAAiB,CAACmE,UAAU,GAAG,CAAC,EAAE;MACtHJ,UAAU,CAAC5I,IAAI,CAAC,IAAIhD,wBAAwB,CAAC6H,iBAAiB,CAACmE,UAAU,EAAEnE,iBAAiB,CAACzC,YAAY,CAAC,CAAC;IAC/G;IAEA,IAAIyC,iBAAiB,CAACoE,cAAc,KAAK,IAAI,IAAIpE,iBAAiB,CAACzC,YAAY,KAAK,IAAI,IAAIyC,iBAAiB,CAACoE,cAAc,GAAG,CAAC,EAAE;MAC9HL,UAAU,CAAC5I,IAAI,CAAC,IAAI/C,iCAAiC,CACjDyL,oBAAoB,EACpB7D,iBAAiB,CAACoE,cAAc,EAChCpE,iBAAiB,CAACzC,YACtB,CAAC,CAAC;IACN;;IAEA;IACA;IACA;IACA;IACA;IACA;;IAGA,IAAIyC,iBAAiB,CAACqE,mBAAmB,KAAK,IAAI,EAAE;MAChDN,UAAU,CAAC5I,IAAI,CAAC,IAAIvD,6BAA6B,CAACoI,iBAAiB,CAACqE,mBAAmB,CAAC,CAAC;IAC7F;IAEA,IAAIrE,iBAAiB,CAACsE,mBAAmB,KAAK,IAAI,EAAE;MAChDP,UAAU,CAAC5I,IAAI,CAAC,IAAItD,6BAA6B,CAC7CmI,iBAAiB,CAACuE,UAAU,EAC5BvE,iBAAiB,CAACsE,mBACtB,CAAC,CAAC;IACN;;IAEA;IACA;IACA;;IAEA;IACA;IACA;IACA;IACA;IACA;IACA;;IAEA;IACA;IACA;;IAEA,IAAItE,iBAAiB,CAACwE,qBAAqB,KAAK,IAAI,EAAE;MAClD,IAAIC,WAAW,GAAIZ,oBAAoB,GAAG,CAAC,IAAI7D,iBAAiB,CAACqE,mBAAmB,KAAK,IAAI,GACvFR,oBAAoB,GACpBA,oBAAoB,GAAG,CAAC;MAE9B,IAAI7D,iBAAiB,CAAC0E,kBAAkB,KAAK,IAAI,EAAE;QAC/C;QACAD,WAAW,IAAIzE,iBAAiB,CAAC0E,kBAAkB,CAAC1E,iBAAiB,CAAC0E,kBAAkB,CAACnK,MAAM,GAAG,CAAC,CAAC,CAAC,CAAC,CAAC;MAC3G;MACAwJ,UAAU,CAAC5I,IAAI,CAAC,IAAIrD,oCAAoC,CAACkI,iBAAiB,CAACwE,qBAAqB,EAAEC,WAAW,CAAC,CAAC;IACnH;IAEA,IAAIzE,iBAAiB,CAAC0E,kBAAkB,KAAK,IAAI,EAAE;MAC/CX,UAAU,CAAC5I,IAAI,CAAC,IAAIxD,0BAA0B,CAACqI,iBAAiB,CAAC0E,kBAAkB,CAAC,CAAC;IACzF;IAEA,IAAIZ,gBAAgB,KAAK,IAAI,EAAE;MAC3BC,UAAU,CAACY,MAAM,CAACb,gBAAgB,CAAC;IACvC;;IAEA;IACA;IACA;IACA;;IAEA,OAAOC,UAAU;EACrB;;EAEA;AACJ;AACA;AACA;AACA;AACA;EACIa,sBAAsBA,CAAC5E,iBAAiB,EAAE;IACtC;IACA;IACA,IAAI6E,UAAU,GAAG,IAAInN,gBAAgB,CAAC,IAAI,CAAC4F,MAAM,CAAC;;IAElD;IACA,IAAI,mBAAmB,IAAI,IAAI,EAAE;MAC7BxC,MAAM,CAACgK,MAAM,CAACD,UAAU,EAAE,IAAI,CAAC7E,iBAAiB,CAAC;IACrD;;IAEA;IACA;IACA,IAAIA,iBAAiB,KAAK,IAAI,EAAE;MAC5BlF,MAAM,CAACgK,MAAM,CAACD,UAAU,EAAE7E,iBAAiB,CAAC;IAChD;IACA,OAAO6E,UAAU;EACrB;;EAEA;AACJ;AACA;;EAEI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACI,MAAME,QAAQA,CACVnK,MAAM,EACNoF,iBAAiB,GAAG,IAAI,EACxB8D,gBAAgB,GAAG,IAAI,EACvB;IACIrC,qBAAqB,GAAG;EAC5B,CAAC,GAAG,CAAC,CAAC,EACR;IACE,IAAI,CAAC,IAAI,CAACe,YAAY,EAAE;MACpB,MAAMH,SAAS,GAAGxI,2BAA2B,CAACyI,GAAG,CAAC,IAAI,CAACF,WAAW,CAAC;MACnE,IAAI4C,YAAY,GAAG,4BAA4B3C,SAAS,qFAAqF;MAE7I,MAAME,SAAS,GAAG,IAAI,CAACjF,MAAM,CAACoG,UAAU;MACxC,MAAMuB,YAAY,GACdC,gCAAgC,CAAC5C,GAAG,CAACC,SAAS,CAAC,IAC5C4C,4CAA4C,CAAC7C,GAAG,CAACC,SAAS,CAAC,IAC3D6C,wCAAwC,CAAC9C,GAAG,CAACC,SAAS;MACzD;MAAA,GACG8C,oCAAoC,CAAC/C,GAAG,CAACC,SAAS,CAAC;MAE1D,IAAI0C,YAAY,EAAE;QACd;QACAD,YAAY,IAAI,6CAA6CC,YAAY,CAAC,CAAC,CAAC,GAAG;MACnF;MACA,MAAM1J,KAAK,CAACyJ,YAAY,CAAC;IAC7B;IAEA,IAAI,EAAEpK,MAAM,YAAYhC,MAAM,CAAC,IAAI,CAACvB,YAAY,CAACuD,MAAM,CAAC,IAAI,CAAC6B,KAAK,CAACC,OAAO,CAAC9B,MAAM,CAAC,EAAE;MAChF,MAAMW,KAAK,CAAC,8DAA8DX,MAAM,CAACwH,WAAW,CAACkD,IAAI,IAAI,CAAC;IAC1G;IAEA,IAAIzB,oBAAoB;;IAExB;IACA;IACA,IAAI,IAAI,CAACvG,MAAM,CAACiI,kBAAkB,EAAE;MAChC;MACA1B,oBAAoB,GAAG,CAAC;IAE5B,CAAC,MAAM;MACHA,oBAAoB,GAAGjJ,MAAM,YAAYhC,MAAM,GAAGgC,MAAM,CAACgD,IAAI,CAAC4H,EAAE,CAAC,CAAC,CAAC,CAAC,GAAG5K,MAAM,CAACL,MAAM;;MAEpF;MACA,IAAIsJ,oBAAoB,KAAK,CAAC,EAAE;QAC5B,MAAMtI,KAAK,CAAC,mDAAmD,CAAC;MACpE;IACJ;;IAEA;IACAyE,iBAAiB,GAAG,IAAI,CAAC4E,sBAAsB,CAAC5E,iBAAiB,CAAC;IAElE8D,gBAAgB,GAAGA,gBAAgB,IAAI,IAAIrM,mBAAmB,CAAC,CAAC;;IAEhE;IACAqM,gBAAgB,GAAG,IAAI,CAACF,qBAAqB,CACzC5D,iBAAiB,EACjB6D,oBAAoB,EACpBC,gBACJ,CAAC;;IAED;IACA,IAAI2B,aAAa,GAAGzF,iBAAiB,CAACzC,YAAY;IAClD,IAAIkI,aAAa,KAAK,IAAI,IAAI,CAAChJ,KAAK,CAACC,OAAO,CAAC+I,aAAa,CAAC,EAAE;MACzDA,aAAa,GAAG,CAACA,aAAa,CAAC;IACnC;;IAEA;IACA;;IAEA,IAAIxF,eAAe,GAAG,CAAC;IACvB,MAAMyF,eAAe,GAAGzF,eAAe,IAAID,iBAAiB,CAAC2F,cAAc,IAAIC,QAAQ,CAAC;;IAExF;IACA,MAAMC,YAAY,GAAGnE,MAAM,CAACoE,SAAS,CAAC9F,iBAAiB,CAACuE,UAAU,CAAC,IAAI,CAACvE,iBAAiB,CAAC2F,cAAc,IAAI,IAAI,MAAM,IAAI;IAC1H,IAAII,OAAO,GAAG1N,OAAO,CAAC2N,UAAU,CAAChG,iBAAiB,CAAC;;IAEnD;IACA,IAAIE,KAAK,GAAG,IAAI,CAAC+F,aAAa,CAACrL,MAAM,EAAEoF,iBAAiB,EAAEC,eAAe,EAAEwB,qBAAqB,CAAC;IAEjG,OAAOvB,KAAK,CAACvD,IAAI,CAACC,CAAC,IAAI,CAACA,CAAC,CAAC8D,IAAI,CAAC,IAAIT,eAAe,GAAGyF,eAAe,EAAE;MAClE,IAAIQ,YAAY,GAAG,EAAE;MACrB,KAAK,IAAIpF,IAAI,IAAIZ,KAAK,EAAE;QACpB,IAAIY,IAAI,CAACJ,IAAI,EAAE;UACX;UACAwF,YAAY,CAAC/K,IAAI,CAAC2F,IAAI,CAAC;UACvB;QACJ;QACA,IAAI+E,YAAY,IAAI/E,IAAI,CAACL,gBAAgB,CAAClG,MAAM,IAAIyF,iBAAiB,CAACuE,UAAU,EAAE;UAC9E;UACAzD,IAAI,CAACJ,IAAI,GAAG,IAAI;UAChBwF,YAAY,CAAC/K,IAAI,CAAC2F,IAAI,CAAC;UACvB;QACJ;;QAEA;QACA,IAAI9E,MAAM,GAAG,MAAM,IAAI,CAACmK,OAAO,CAACrF,IAAI,CAAC;;QAErC;QACA,IAAId,iBAAiB,CAACoG,iBAAiB,EAAE;UACrC,IAAI,CAACC,mBAAmB,CAACvF,IAAI,EAAE9E,MAAM,CAAC;QAC1C;QACA,IAAIgE,iBAAiB,CAACsG,aAAa,EAAE;UACjC;QAAA;;QAGJ;QACA;QACA;QACA;QACA,IAAI7G,MAAM,GAAGzD,MAAM,CAACyD,MAAM,CAAClB,KAAK,CAAC,IAAI,EAAE,CAAC,CAAC,EAAE,IAAI,CAAC;;QAEhD;QACAuF,gBAAgB,CAAChD,IAAI,CAACL,gBAAgB,EAAEhB,MAAM,CAAC;QAE/C,IAAI8G,aAAa,GAAGR,OAAO,CAACtG,MAAM,CAAC;QACnC,KAAK,IAAI,CAAC0B,UAAU,EAAEqF,OAAO,CAAC,IAAID,aAAa,EAAE;UAC7C;UACA,IAAIE,OAAO,GAAG;YAAE,GAAG3F;UAAK,CAAC;;UAEzB;UACA;UACA,IAAI,CAAC4F,UAAU,CAACD,OAAO,EAAEtF,UAAU,CAAC;UAEpCsF,OAAO,CAAC9F,KAAK,IAAI6F,OAAO;UAExB,IAAIf,aAAa,IAAIA,aAAa,CAAC3J,QAAQ,CAACqF,UAAU,CAAC,EAAE;YACrDsF,OAAO,CAAC/F,IAAI,GAAG,IAAI;UACvB;UAEAwF,YAAY,CAAC/K,IAAI,CAACsL,OAAO,CAAC;QAC9B;MACJ;MACA,EAAExG,eAAe;;MAEjB;MACAiG,YAAY,GAAG,IAAI,CAACS,UAAU,CAACT,YAAY,CAAC,CAAClJ,GAAG,CAC5C4J,KAAK,IAAIA,KAAK,CACTC,IAAI,CAAC,CAACC,CAAC,EAAEC,CAAC,KAAKA,CAAC,CAACpG,KAAK,GAAGmG,CAAC,CAACnG,KAAK,CAAC,CAAM;MAAA,CACvCpC,KAAK,CAAC,CAAC,EAAEyB,iBAAiB,CAACgH,SAAS,CAAC,CAAE;MAChD,CAAC;;MAED;MACA9G,KAAK,GAAGgG,YAAY,CAACnJ,IAAI,CAAC,CAAC;;MAE3B;MACA,IAAIiD,iBAAiB,CAACiH,iBAAiB,EAAE;QACrCjH,iBAAiB,CAACiH,iBAAiB,CAAC/G,KAAK,CAAC;MAC9C;IACJ;;IAEA;;IAEA,MAAMgH,YAAY,GAAG,IAAI,CAACP,UAAU,CAACzG,KAAK,CAAC;IAE3C,MAAMiH,YAAY,GAAI9F,GAAG,IAAK6F,YAAY,CAAClK,GAAG,CAC1CoK,KAAK,IAAI;MACL,IAAIpH,iBAAiB,CAACqH,oBAAoB,GAAG,CAAC,EAAE;QAC5C,OAAOD,KAAK,CAAC7I,KAAK,CAAC,CAAC,EAAEyB,iBAAiB,CAACqH,oBAAoB,CAAC,CAACrK,GAAG,CAACJ,CAAC,IAAIA,CAAC,CAACyE,GAAG,CAAC,CAAC;MAClF,CAAC,MAAM;QACH,OAAO,CAAC+F,KAAK,CAAC,CAAC,CAAC,CAAC/F,GAAG,CAAC,CAAC;MAC1B;IACJ,CACJ,CAAC,CAACtE,IAAI,CAAC,CAAC,CAAC,CAAC;;IAEV,MAAMuK,SAAS,GAAGH,YAAY,CAAC,kBAAkB,CAAC,CAAC,CAAC;;IAEpD,IAAInH,iBAAiB,CAACuH,uBAAuB,EAAE;MAC3C;MACA;MACA;MACA;MACA;MACA;MACA;MACA;MACA;MACA;MACA;;MAEA,MAAMC,kBAAkB,GAAGL,YAAY,CAAC,oBAAoB,CAAC;MAC7D,MAAMM,gBAAgB,GAAGN,YAAY,CAAC,kBAAkB,CAAC;MAEzD,OAAO;QACHG,SAAS;QAETE,kBAAkB;QAClBC;MACJ,CAAC;IACL,CAAC,MAAM;MACH,OAAOH,SAAS;IACpB;EACJ;;EAEA;AACJ;AACA;AACA;AACA;AACA;EACIjB,mBAAmBA,CAACvF,IAAI,EAAE9E,MAAM,EAAE;IAC9B,IAAI,IAAI,CAACsB,MAAM,CAACiI,kBAAkB,EAAE;MAChC,IAAI,CAACvJ,MAAM,CAACyL,gBAAgB,IAAIzL,MAAM,CAACyL,gBAAgB,CAAClN,MAAM,KAAK,CAAC,EAAE;QAClE,MAAMgB,KAAK,CACP,+EAA+E,GAC/E,uFACJ,CAAC;MACL;MACA,IAAI,CAACuF,IAAI,CAAC2G,gBAAgB,EAAE;QACxB3G,IAAI,CAAC2G,gBAAgB,GAAG,EAAE;MAC9B;MACA3G,IAAI,CAAC2G,gBAAgB,CAACtM,IAAI,CAACa,MAAM,CAACyL,gBAAgB,CAAC;IACvD;IAEA,IAAI,CAACzL,MAAM,CAACwL,kBAAkB,IAAIxL,MAAM,CAACwL,kBAAkB,CAACjN,MAAM,KAAK,CAAC,EAAE;MACtE,MAAMgB,KAAK,CACP,iFAAiF,GACjF,uFACJ,CAAC;IACL;IACA,IAAI,CAACuF,IAAI,CAAC0G,kBAAkB,EAAE;MAC1B1G,IAAI,CAAC0G,kBAAkB,GAAG,EAAE;IAChC;IACA1G,IAAI,CAAC0G,kBAAkB,CAACrM,IAAI,CAACa,MAAM,CAACwL,kBAAkB,CAAC;EAC3D;;EAEA;AACJ;AACA;AACA;AACA;AACA;EACIb,UAAUA,CAACzG,KAAK,EAAE;IACd;IACA,MAAMwH,MAAM,GAAG5M,MAAM,CAACT,MAAM,CAAC,IAAI,CAAC;IAClC,KAAK,MAAMgC,GAAG,IAAI6D,KAAK,EAAE;MACrB,IAAIwH,MAAM,CAACrL,GAAG,CAACuE,EAAE,CAAC,KAAK+G,SAAS,EAAE;QAC9BD,MAAM,CAACrL,GAAG,CAACuE,EAAE,CAAC,GAAG,CAACvE,GAAG,CAAC;MAC1B,CAAC,MAAM;QACHqL,MAAM,CAACrL,GAAG,CAACuE,EAAE,CAAC,CAACzF,IAAI,CAACkB,GAAG,CAAC;MAC5B;IACJ;IAEA,OAAOvB,MAAM,CAAC8M,MAAM,CAACF,MAAM,CAAC;EAChC;;EAEA;AACJ;AACA;AACA;AACA;AACA;AACA;EACIhI,gBAAgBA,CAACF,cAAc,EAAEqI,aAAa,EAAE;IAE5C,MAAMC,IAAI,GAAGhN,MAAM,CAACT,MAAM,CAAC,IAAI,CAAC;IAEhC,KAAK,MAAMiL,IAAI,IAAI9F,cAAc,EAAE;MAC/B,IAAI8F,IAAI,CAACyC,UAAU,CAAC,SAAS,CAAC,EAAE;QAC5B,IAAIC,OAAO,GAAG1C,IAAI,CAAC2C,OAAO,CAAC,SAAS,EAAE,iBAAiB,CAAC;QAExD,IAAIJ,aAAa,IAAIvC,IAAI,CAACxJ,QAAQ,CAAC,SAAS,CAAC,EAAE;UAC3C;UACA;UACA;UACAgM,IAAI,CAACE,OAAO,CAAC,GAAGH,aAAa,CAACG,OAAO,CAAC;QAC1C,CAAC,MAAM;UACHF,IAAI,CAACE,OAAO,CAAC,GAAGxI,cAAc,CAAC8F,IAAI,CAAC;QACxC;MACJ;IACJ;IACA,OAAOwC,IAAI;EACf;;EAEA;AACJ;AACA;AACA;AACA;AACA;EACIlI,aAAaA,CAACJ,cAAc,EAAE;IAC1B,MAAMG,KAAK,GAAG7E,MAAM,CAACT,MAAM,CAAC,IAAI,CAAC;IAEjC,KAAK,MAAM6N,QAAQ,IAAI,CAAC,kBAAkB,EAAE,oBAAoB,CAAC,EAAE;MAC/D,MAAMC,MAAM,GAAG,EAAE;MACjB,KAAK,MAAM7C,IAAI,IAAI9F,cAAc,EAAE;QAC/B,IAAI8F,IAAI,CAACyC,UAAU,CAACG,QAAQ,CAAC,EAAE;UAC3B,MAAM7J,KAAK,GAAGiH,IAAI,CAAC8C,KAAK,CAAC,GAAG,CAAC,CAACC,GAAG,CAAC,CAAC;UACnCF,MAAM,CAAC9J,KAAK,CAAC,GAAGmB,cAAc,CAAC8F,IAAI,CAAC;QACxC;MACJ;MACA3F,KAAK,CAACuI,QAAQ,CAAC,GAAGC,MAAM;IAC5B;IACA,OAAOxI,KAAK;EAChB;;EAEA;AACJ;AACA;AACA;AACA;AACA;EACIJ,gBAAgBA,CAACN,YAAY,EAAE4I,aAAa,EAAE;IAC1C,IAAIA,aAAa,EAAE;MACf/M,MAAM,CAACgK,MAAM,CAAC7F,YAAY,EAAE4I,aAAa,CAAC;IAC9C,CAAC,MAAM;MACH;MACA,MAAMS,UAAU,GAAG,CAAC;;MAEpB;MACA,IAAI,IAAI,CAAChL,MAAM,CAACiI,kBAAkB,KAAK,IAAI,CAACgD,eAAe,IAAI,IAAI,CAAC,EAAE;QAClE;QACA,IAAIC,YAAY,GAAG,CAACF,UAAU,EAAE,IAAI,CAACG,iBAAiB,EAAE,CAAC,EAAE,IAAI,CAACC,cAAc,CAAC;QAC/E;QACA,IAAIC,YAAY,GAAG,CAACL,UAAU,EAAE,IAAI,CAACM,iBAAiB,EAAE,CAAC,EAAE,IAAI,CAACC,cAAc,CAAC;QAC/E;QACA,KAAK,IAAI5K,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG,IAAI,CAAC6K,kBAAkB,EAAE,EAAE7K,CAAC,EAAE;UAC9CgB,YAAY,CAAC,mBAAmBhB,CAAC,cAAc,CAAC,GAAG,IAAIrF,MAAM,CAAC,SAAS,EAAE,EAAE,EAAE4P,YAAY,CAAC;UAC1FvJ,YAAY,CAAC,mBAAmBhB,CAAC,gBAAgB,CAAC,GAAG,IAAIrF,MAAM,CAAC,SAAS,EAAE,EAAE,EAAE4P,YAAY,CAAC;UAC5FvJ,YAAY,CAAC,mBAAmBhB,CAAC,cAAc,CAAC,GAAG,IAAIrF,MAAM,CAAC,SAAS,EAAE,EAAE,EAAE+P,YAAY,CAAC;UAC1F1J,YAAY,CAAC,mBAAmBhB,CAAC,gBAAgB,CAAC,GAAG,IAAIrF,MAAM,CAAC,SAAS,EAAE,EAAE,EAAE+P,YAAY,CAAC;QAChG;MACJ,CAAC,MAAM,IAAI,IAAI,CAACrL,MAAM,CAACoG,UAAU,KAAK,QAAQ,EAAE;QAC5C;QACA;QACA,IAAI9F,IAAI,GAAG,CAAC0K,UAAU,GAAG,IAAI,CAACS,SAAS,EAAE,CAAC,EAAE,IAAI,CAACC,MAAM,CAAC;QACxD;QACA,KAAK,IAAI/K,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG,IAAI,CAACgL,UAAU,EAAE,EAAEhL,CAAC,EAAE;UACtCgB,YAAY,CAAC,mBAAmBhB,CAAC,MAAM,CAAC,GAAG,IAAIrF,MAAM,CAAC,SAAS,EAAE,EAAE,EAAEgF,IAAI,CAAC;UAC1EqB,YAAY,CAAC,mBAAmBhB,CAAC,QAAQ,CAAC,GAAG,IAAIrF,MAAM,CAAC,SAAS,EAAE,EAAE,EAAEgF,IAAI,CAAC;QAChF;MACJ,CAAC,MAAM,IAAI,IAAI,CAACN,MAAM,CAAC4L,WAAW,EAAE;QAAE;QAClC;QACA,IAAItL,IAAI,GAAG,CAAC0K,UAAU,GAAG,IAAI,CAACS,SAAS,EAAE,CAAC,EAAE,CAAC,GAAG,IAAI,CAACC,MAAM,CAAC;QAC5D;QACA,KAAK,IAAI/K,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG,IAAI,CAACgL,UAAU,EAAE,EAAEhL,CAAC,EAAE;UACtCgB,YAAY,CAAC,mBAAmBhB,CAAC,YAAY,CAAC,GAAG,IAAIrF,MAAM,CAAC,SAAS,EAAE,EAAE,EAAEgF,IAAI,CAAC;QACpF;MACJ,CAAC,MAAM,IAAI,IAAI,CAACN,MAAM,CAACoG,UAAU,KAAK,OAAO,EAAE;QAC3C;;QAEA;QACA,IAAIyF,OAAO,GAAG,CAACb,UAAU,GAAG,IAAI,CAACS,SAAS,EAAE,IAAI,CAACC,MAAM,EAAE,CAAC,CAAC,EAAC;QAC5D;QACA,IAAII,SAAS,GAAG,CAACd,UAAU,GAAG,IAAI,CAACS,SAAS,EAAE,CAAC,EAAE,IAAI,CAACC,MAAM,CAAC,EAAC;QAC9D;QACA,KAAK,IAAI/K,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG,IAAI,CAACgL,UAAU,EAAE,EAAEhL,CAAC,EAAE;UACtCgB,YAAY,CAAC,mBAAmBhB,CAAC,MAAM,CAAC,GAAG,IAAIrF,MAAM,CAAC,SAAS,EAAE,EAAE,EAAEuQ,OAAO,CAAC;UAC7ElK,YAAY,CAAC,mBAAmBhB,CAAC,QAAQ,CAAC,GAAG,IAAIrF,MAAM,CAAC,SAAS,EAAE,EAAE,EAAEwQ,SAAS,CAAC;QACrF;MACJ,CAAC,MAAM;QAAE;QACL;QACA,IAAIxL,IAAI,GAAG,CAAC0K,UAAU,EAAE,IAAI,CAACS,SAAS,EAAE,CAAC,EAAE,IAAI,CAACC,MAAM,CAAC;QACvD;QACA,KAAK,IAAI/K,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG,IAAI,CAACgL,UAAU,EAAE,EAAEhL,CAAC,EAAE;UACtCgB,YAAY,CAAC,mBAAmBhB,CAAC,MAAM,CAAC,GAAG,IAAIrF,MAAM,CAAC,SAAS,EAAE,EAAE,EAAEgF,IAAI,CAAC;UAC1EqB,YAAY,CAAC,mBAAmBhB,CAAC,QAAQ,CAAC,GAAG,IAAIrF,MAAM,CAAC,SAAS,EAAE,EAAE,EAAEgF,IAAI,CAAC;QAChF;MACJ;IACJ;EACJ;;EAEA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACIqI,aAAaA,CAAClG,aAAa,EAAEC,iBAAiB,EAAEC,eAAe,EAAEwB,qBAAqB,EAAE;IACpF,OAAO,IAAI,CAACiB,cAAc,CAAC,IAAI,EAAE3C,aAAa,EAAEC,iBAAiB,EAAEC,eAAe,EAAEwB,qBAAqB,CAAC;EAC9G;;EAEA;AACJ;AACA;AACA;AACA;AACA;EACI,MAAM0E,OAAOA,CAACrF,IAAI,EAAE;IAChB,OAAO,MAAM,IAAI,CAAC2B,QAAQ,CAAC,IAAI,EAAE3B,IAAI,CAAC;EAC1C;;EAEA;AACJ;AACA;AACA;AACA;AACA;EACI4F,UAAUA,CAAC5F,IAAI,EAAEK,UAAU,EAAE;IACzB,OAAO,IAAI,CAACwB,WAAW,CAAC7B,IAAI,EAAEK,UAAU,CAAC;EAC7C;AACJ;;AAEA;AACA;AACA,OAAO,MAAMkI,WAAW,CAAC;;AAEzB;AACA;AACA;AACA,OAAO,MAAMC,eAAe,SAASD,WAAW,CAAC;EAC7C;AACJ;AACA;AACA;AACA;AACA;EACIjH,WAAWA,CAAC;IAAEpD,iBAAiB;IAAEuK,aAAa,GAAG,IAAI;IAAEC,UAAU,GAAG;EAAK,CAAC,EAAE;IACxE,KAAK,CAAC,CAAC;IACP,IAAI,CAACxK,iBAAiB,GAAGA,iBAAiB;IAC1C,IAAI,CAACuK,aAAa,GAAGA,aAAa;IAClC,IAAI,CAACC,UAAU,GAAGA,UAAU;EAChC;AACJ;AACA;AACA;AACA,OAAO,MAAMC,mBAAmB,SAAStH,eAAe,CAAC;AACzD,OAAO,MAAMuH,SAAS,SAASD,mBAAmB,CAAC;;AAEnD;AACA;AACA;AACA,OAAO,MAAME,eAAe,SAASF,mBAAmB,CAAC;EACrD;AACJ;AACA;AACA;AACA;AACA;EACI,MAAM9F,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIgL,cAAc,CAAC,MAAM,KAAK,CAACjG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EAC9D;AACJ;;AAEA;AACA;AACA;AACA,OAAO,MAAMiL,6BAA6B,SAASJ,mBAAmB,CAAC;EACnE;AACJ;AACA;AACA;AACA;AACA;EACI,MAAM9F,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIkL,wBAAwB,CAAC,MAAM,KAAK,CAACnG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EACxE;AACJ;;AAEA;AACA;AACA;AACA,OAAO,MAAMmL,0BAA0B,SAASN,mBAAmB,CAAC;EAChE;AACJ;AACA;AACA;AACA;AACA;EACI,MAAM9F,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIoL,qBAAqB,CAAC,MAAM,KAAK,CAACrG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EACrE;AACJ;;AAEA;AACA;AACA;AACA,OAAO,MAAMqL,wBAAwB,SAASR,mBAAmB,CAAC;EAC9D;AACJ;AACA;AACA;AACA;AACA;EACI,MAAM9F,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIsL,4BAA4B,CAAC,MAAM,KAAK,CAACvG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EAC5E;AACJ;AACA;;AAEA;AACA;AACA,OAAO,MAAMuL,wBAAwB,SAAShI,eAAe,CAAC;AAC9D,OAAO,MAAMiI,cAAc,SAASD,wBAAwB,CAAC;AAC7D;;AAEA;AACA;AACA,OAAO,MAAME,uBAAuB,SAASlI,eAAe,CAAC;;AAE7D;AACA;AACA;AACA,OAAO,MAAMmI,aAAa,SAASD,uBAAuB,CAAC;;AAE3D;AACA;AACA;AACA,OAAO,MAAME,mBAAmB,SAASF,uBAAuB,CAAC;EAC7D;AACJ;AACA;AACA;AACA;AACA;EACI,MAAM1G,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIgL,cAAc,CAAC,MAAM,KAAK,CAACjG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EAC9D;AACJ;;AAEA;AACA;AACA;AACA,OAAO,MAAM4L,iCAAiC,SAASH,uBAAuB,CAAC;EAC3E;AACJ;AACA;AACA;AACA;AACA;EACI,MAAM1G,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIkL,wBAAwB,CAAC,MAAM,KAAK,CAACnG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EACxE;AACJ;;AAEA;AACA;AACA;AACA;AACA,OAAO,MAAM6L,8BAA8B,SAASJ,uBAAuB,CAAC;EACxE;AACJ;AACA;AACA;AACA;AACA;EACI,MAAM1G,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIoL,qBAAqB,CAAC,MAAM,KAAK,CAACrG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EACrE;AACJ;;AAEA;AACA;AACA;AACA;AACA,OAAO,MAAM8L,4BAA4B,SAASL,uBAAuB,CAAC;EACtE;AACJ;AACA;AACA;AACA;AACA;EACI,MAAM1G,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIsL,4BAA4B,CAAC,MAAM,KAAK,CAACvG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EAC5E;AACJ;AACA;AACA;;AAEA;AACA;AACA,OAAO,MAAM+L,uBAAuB,SAASxI,eAAe,CAAC;;AAE7D;AACA;AACA;AACA,OAAO,MAAMyI,aAAa,SAASD,uBAAuB,CAAC;;AAE3D;AACA;AACA;AACA,OAAO,MAAME,mBAAmB,SAASF,uBAAuB,CAAC;EAC7D;AACJ;AACA;AACA;AACA;AACA;EACI,MAAMhH,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIgL,cAAc,CAAC,MAAM,KAAK,CAACjG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EAC9D;AACJ;;AAEA;AACA;AACA;AACA,OAAO,MAAMkM,iCAAiC,SAASH,uBAAuB,CAAC;EAC3E;AACJ;AACA;AACA;AACA;AACA;EACI,MAAMhH,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIkL,wBAAwB,CAAC,MAAM,KAAK,CAACnG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EACxE;AACJ;;AAEA;AACA;AACA;AACA;AACA,OAAO,MAAMmM,8BAA8B,SAASJ,uBAAuB,CAAC;EACxE;AACJ;AACA;AACA;AACA;AACA;EACI,MAAMhH,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIoL,qBAAqB,CAAC,MAAM,KAAK,CAACrG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EACrE;AACJ;;AAEA;AACA;AACA;AACA;AACA,OAAO,MAAMoM,4BAA4B,SAASL,uBAAuB,CAAC;EACtE;AACJ;AACA;AACA;AACA;AACA;EACI,MAAMhH,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIsL,4BAA4B,CAAC,MAAM,KAAK,CAACvG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EAC5E;AACJ;AACA;;AAGA;AACA;AACA,OAAO,MAAMqM,sBAAsB,SAAS9I,eAAe,CAAC;;AAE5D;AACA;AACA;AACA;AACA;AACA,OAAO,MAAM+I,YAAY,SAASD,sBAAsB,CAAC;AACzD;AACA;AACA;AACA;AACA,OAAO,MAAME,kBAAkB,SAASF,sBAAsB,CAAC;EAC3D;AACJ;AACA;AACA;AACA;AACA;EACI,MAAMtH,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIgL,cAAc,CAAC,MAAM,KAAK,CAACjG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EAC9D;AACJ;;AAEA;AACA;AACA;AACA,OAAO,MAAMwM,gCAAgC,SAASH,sBAAsB,CAAC;EACzE;AACJ;AACA;AACA;AACA;AACA;EACI,MAAMtH,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIkL,wBAAwB,CAAC,MAAM,KAAK,CAACnG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EACxE;AACJ;;AAEA;AACA;AACA;AACA,OAAO,MAAMyM,6BAA6B,SAASJ,sBAAsB,CAAC;EACtE;AACJ;AACA;AACA;AACA;AACA;EACI,MAAMtH,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIoL,qBAAqB,CAAC,MAAM,KAAK,CAACrG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EACrE;AACJ;;AAEA;AACA;AACA;AACA;AACA,OAAO,MAAM0M,2BAA2B,SAASL,sBAAsB,CAAC;EACpE;AACJ;AACA;AACA;AACA;AACA;EACI,MAAMtH,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIsL,4BAA4B,CAAC,MAAM,KAAK,CAACvG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EAC5E;AACJ;AACA;;AAGA;AACA;AACA,OAAO,MAAM2M,wBAAwB,SAASpJ,eAAe,CAAC;;AAE9D;AACA;AACA;AACA,OAAO,MAAMqJ,cAAc,SAASD,wBAAwB,CAAC;;AAE7D;AACA;AACA;AACA,OAAO,MAAME,oBAAoB,SAASF,wBAAwB,CAAC;EAC/D;AACJ;AACA;AACA;AACA;AACA;EACI,MAAM5H,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIgL,cAAc,CAAC,MAAM,KAAK,CAACjG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EAC9D;AACJ;;AAEA;AACA;AACA;AACA,OAAO,MAAM8M,kCAAkC,SAASH,wBAAwB,CAAC;EAC7E;AACJ;AACA;AACA;AACA;AACA;EACI,MAAM5H,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIkL,wBAAwB,CAAC,MAAM,KAAK,CAACnG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EACxE;AACJ;;AAEA;AACA;AACA;AACA,OAAO,MAAM+M,+BAA+B,SAASJ,wBAAwB,CAAC;EAC1E;AACJ;AACA;AACA;AACA;AACA;EACI,MAAM5H,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIoL,qBAAqB,CAAC,MAAM,KAAK,CAACrG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EACrE;AACJ;;AAEA;AACA;AACA;AACA,OAAO,MAAMgN,6BAA6B,SAASL,wBAAwB,CAAC;EACxE;AACJ;AACA;AACA;AACA;AACA;EACI,MAAM5H,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIsL,4BAA4B,CAAC,MAAM,KAAK,CAACvG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EAC5E;AACJ;AACA;;AAEA;AACA;AACA,OAAO,MAAMiN,sBAAsB,SAAS1J,eAAe,CAAC;;AAE5D;AACA;AACA;AACA,OAAO,MAAM2J,YAAY,SAASD,sBAAsB,CAAC;;AAEzD;AACA;AACA;AACA,OAAO,MAAME,kBAAkB,SAASF,sBAAsB,CAAC;EAC3D;AACJ;AACA;AACA;AACA;AACA;EACI,MAAMlI,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIgL,cAAc,CAAC,MAAM,KAAK,CAACjG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EAC9D;AACJ;;AAEA;AACA;AACA;AACA,OAAO,MAAMoN,gCAAgC,SAASH,sBAAsB,CAAC;EACzE;AACJ;AACA;AACA;AACA;AACA;EACI,MAAMlI,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIkL,wBAAwB,CAAC,MAAM,KAAK,CAACnG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EACxE;AACJ;;AAEA;AACA;AACA;AACA,OAAO,MAAMqN,6BAA6B,SAASJ,sBAAsB,CAAC;EACtE;AACJ;AACA;AACA;AACA;AACA;EACI,MAAMlI,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIoL,qBAAqB,CAAC,MAAM,KAAK,CAACrG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EACrE;AACJ;;AAEA;AACA;AACA;AACA;AACA,OAAO,MAAMsN,2BAA2B,SAASL,sBAAsB,CAAC;EACpE;AACJ;AACA;AACA;AACA;AACA;EACI,MAAMlI,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIsL,4BAA4B,CAAC,MAAM,KAAK,CAACvG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EAC5E;AACJ;AACA;;AAEA;AACA;AACA,OAAO,MAAMuN,wBAAwB,SAAShK,eAAe,CAAC;;AAE9D;AACA;AACA;AACA,OAAO,MAAMiK,cAAc,SAASD,wBAAwB,CAAC;;AAE7D;AACA;AACA;AACA,OAAO,MAAME,oBAAoB,SAASF,wBAAwB,CAAC;EAC/D;AACJ;AACA;AACA;AACA;AACA;EACI,MAAMxI,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIgL,cAAc,CAAC,MAAM,KAAK,CAACjG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EAC9D;AACJ;;AAEA;AACA;AACA;AACA,OAAO,MAAM0N,kCAAkC,SAASH,wBAAwB,CAAC;EAC7E;AACJ;AACA;AACA;AACA;AACA;EACI,MAAMxI,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIkL,wBAAwB,CAAC,MAAM,KAAK,CAACnG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EACxE;AACJ;;AAEA;AACA;AACA;AACA,OAAO,MAAM2N,+BAA+B,SAASJ,wBAAwB,CAAC;EAC1E;AACJ;AACA;AACA;AACA;AACA;EACI,MAAMxI,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIoL,qBAAqB,CAAC,MAAM,KAAK,CAACrG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EACrE;AACJ;;AAEA;AACA;AACA;AACA;AACA,OAAO,MAAM4N,6BAA6B,SAASL,wBAAwB,CAAC;EACxE;AACJ;AACA;AACA;AACA;AACA;EACI,MAAMxI,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIsL,4BAA4B,CAAC,MAAM,KAAK,CAACvG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EAC5E;AACJ;AACA;;AAEA;AACA;AACA,OAAO,MAAM6N,yBAAyB,SAAStK,eAAe,CAAC;AAC/D,OAAO,MAAMuK,eAAe,SAASD,yBAAyB,CAAC;;AAE/D;AACA;AACA;AACA,OAAO,MAAME,mCAAmC,SAASF,yBAAyB,CAAC;EAC/E;AACJ;AACA;AACA;AACA;AACA;EACI,MAAM9I,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIkL,wBAAwB,CAAC,MAAM,KAAK,CAACnG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EACxE;AACJ;;AAEA;AACA;AACA;AACA,OAAO,MAAMgO,gCAAgC,SAASH,yBAAyB,CAAC;EAC5E;AACJ;AACA;AACA;AACA;AACA;EACI,MAAM9I,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIoL,qBAAqB,CAAC,MAAM,KAAK,CAACrG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EACrE;AACJ;;AAGA;AACA;AACA;AACA,OAAO,MAAMiO,8BAA8B,SAASJ,yBAAyB,CAAC;EAC1E;AACJ;AACA;AACA;AACA;AACA;EACI,MAAM9I,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIsL,4BAA4B,CAAC,MAAM,KAAK,CAACvG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EAC5E;AACJ;;AAEA;AACA;AACA;AACA,OAAO,MAAMkO,qBAAqB,SAASL,yBAAyB,CAAC;EACjE;AACJ;AACA;AACA;AACA;AACA;EACI,MAAM9I,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIgL,cAAc,CAAC,MAAM,KAAK,CAACjG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EAC9D;AACJ;AACA;;AAGA;AACA;AACA,OAAO,MAAMmO,kBAAkB,SAAS5K,eAAe,CAAC;;AAExD;AACA;AACA;AACA,OAAO,MAAM6K,QAAQ,SAASD,kBAAkB,CAAC;;AAEjD;AACA;AACA;AACA,OAAO,MAAME,cAAc,SAASF,kBAAkB,CAAC;EACnD;AACJ;AACA;AACA;AACA;AACA;EACI,MAAMpJ,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIgL,cAAc,CAAC,MAAM,KAAK,CAACjG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EAC9D;AACJ;;AAEA;AACA;AACA;AACA,OAAO,MAAMsO,4BAA4B,SAASH,kBAAkB,CAAC;EACjE;AACJ;AACA;AACA;AACA;AACA;EACI,MAAMpJ,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIkL,wBAAwB,CAAC,MAAM,KAAK,CAACnG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EACxE;AACJ;;AAEA;AACA;AACA;AACA;AACA,OAAO,MAAMuO,yBAAyB,SAASJ,kBAAkB,CAAC;EAC9D;AACJ;AACA;AACA;AACA;AACA;EACI,MAAMpJ,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIoL,qBAAqB,CAAC,MAAM,KAAK,CAACrG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EACrE;AACJ;AACA;;AAGA;AACA;AACA,OAAO,MAAMwO,yBAAyB,SAASjL,eAAe,CAAC;AAC/D,OAAO,MAAMkL,eAAe,SAASD,yBAAyB,CAAC;;AAE/D;AACA;AACA;AACA,OAAO,MAAME,qBAAqB,SAASF,yBAAyB,CAAC;EACjE;AACJ;AACA;AACA;AACA;AACA;EACI,MAAMzJ,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIgL,cAAc,CAAC,MAAM,KAAK,CAACjG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EAC9D;AACJ;;AAEA;AACA;AACA;AACA,OAAO,MAAM2O,mCAAmC,SAASH,yBAAyB,CAAC;EAC/E;AACJ;AACA;AACA;AACA;AACA;EACI,MAAMzJ,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIkL,wBAAwB,CAAC,MAAM,KAAK,CAACnG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EACxE;AACJ;;AAEA;AACA;AACA;AACA,OAAO,MAAM4O,8BAA8B,SAASJ,yBAAyB,CAAC;EAC1E;AACJ;AACA;AACA;AACA;AACA;EACI,MAAMzJ,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIsL,4BAA4B,CAAC,MAAM,KAAK,CAACvG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EAC5E;AACJ;AACA;;AAEA;AACA;AACA,OAAO,MAAM6O,oBAAoB,SAAStL,eAAe,CAAC;;AAE1D;AACA;AACA;AACA,OAAO,MAAMuL,UAAU,SAASD,oBAAoB,CAAC;;AAErD;AACA;AACA;AACA,OAAO,MAAME,gBAAgB,SAASF,oBAAoB,CAAC;EACvD;AACJ;AACA;AACA;AACA;AACA;EACI,MAAM9J,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIgL,cAAc,CAAC,MAAM,KAAK,CAACjG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EAC9D;AACJ;;AAEA;AACA;AACA;AACA,OAAO,MAAMgP,8BAA8B,SAASH,oBAAoB,CAAC;EACrE;AACJ;AACA;AACA;AACA;AACA;EACI,MAAM9J,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIkL,wBAAwB,CAAC,MAAM,KAAK,CAACnG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EACxE;AACJ;;AAEA;AACA;AACA;AACA,OAAO,MAAMiP,2BAA2B,SAASJ,oBAAoB,CAAC;EAClE;AACJ;AACA;AACA;AACA;AACA;EACI,MAAM9J,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIoL,qBAAqB,CAAC,MAAM,KAAK,CAACrG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EACrE;AACJ;;AAEA;AACA;AACA;AACA,OAAO,MAAMkP,yBAAyB,SAASL,oBAAoB,CAAC;EAChE;AACJ;AACA;AACA;AACA;AACA;EACI,MAAM9J,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIsL,4BAA4B,CAAC,MAAM,KAAK,CAACvG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EAC5E;AACJ;AACA;;AAGA;AACA;AACA,OAAO,MAAMmP,0BAA0B,SAAS5L,eAAe,CAAC;AAChE,OAAO,MAAM6L,gBAAgB,SAASD,0BAA0B,CAAC;AACjE,OAAO,MAAME,sBAAsB,SAASF,0BAA0B,CAAC;EACnE;AACJ;AACA;AACA;AACA;AACA;EACI,MAAMpK,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIgL,cAAc,CAAC,MAAM,KAAK,CAACjG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EAC9D;AACJ;AACA,OAAO,MAAMsP,oCAAoC,SAASH,0BAA0B,CAAC;EACjF;AACJ;AACA;AACA;AACA;AACA;EACI,MAAMpK,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIkL,wBAAwB,CAAC,MAAM,KAAK,CAACnG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EACxE;AACJ;AACA,OAAO,MAAMuP,+BAA+B,SAASJ,0BAA0B,CAAC;EAC5E;AACJ;AACA;AACA;AACA;AACA;EACI,MAAMpK,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIsL,4BAA4B,CAAC,MAAM,KAAK,CAACvG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EAC5E;AACJ;AACA;;AAGA;AACA;AACA,OAAO,MAAMwP,qBAAqB,SAASjM,eAAe,CAAC;AAC3D,OAAO,MAAMkM,WAAW,SAASD,qBAAqB,CAAC;AACvD,OAAO,MAAME,+BAA+B,SAASF,qBAAqB,CAAC;EACvE;AACJ;AACA;AACA;AACA;AACA;EACI,MAAMzK,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIkL,wBAAwB,CAAC,MAAM,KAAK,CAACnG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EACxE;AACJ;AACA,OAAO,MAAM2P,0BAA0B,SAASH,qBAAqB,CAAC;EAClE;AACJ;AACA;AACA;AACA;AACA;EACI,MAAMzK,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIsL,4BAA4B,CAAC,MAAM,KAAK,CAACvG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EAC5E;AACJ;AACA,OAAO,MAAM4P,iBAAiB,SAASJ,qBAAqB,CAAC;EACzD;AACJ;AACA;AACA;AACA;AACA;EACI,MAAMzK,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIgL,cAAc,CAAC,MAAM,KAAK,CAACjG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EAC9D;AACJ;AACA;;AAGA;AACA;AACA,OAAO,MAAM6P,iBAAiB,SAAStM,eAAe,CAAC;AAAG;AAE1D,OAAO,MAAMuM,OAAO,SAASD,iBAAiB,CAAC;;AAE/C;AACA;AACA;AACA,OAAO,MAAME,0BAA0B,SAASF,iBAAiB,CAAC;EAE9D;AACJ;AACA;AACA;AACA;AACA;AACA;EACIrM,WAAWA,CAAC9E,MAAM,EAAE3C,OAAO,EAAE0E,sBAAsB,EAAEW,iBAAiB,EAAE;IACpE,KAAK,CAAC1C,MAAM,EAAE3C,OAAO,CAAC;IACtB,IAAI,CAAC0E,sBAAsB,GAAGA,sBAAsB;IACpD,IAAI,CAACW,iBAAiB,GAAGA,iBAAiB;IAE1C,IAAI,CAAC8I,kBAAkB,GAAG,IAAI,CAACxL,MAAM,CAACwL,kBAAkB;IACxD,IAAI,CAACF,iBAAiB,GAAG,IAAI,CAACtL,MAAM,CAACyL,SAAS;IAC9C,IAAI,CAACF,cAAc,GAAG,IAAI,CAACvL,MAAM,CAACsR,IAAI;IAEtC,IAAI,CAACC,kBAAkB,GAAG,IAAI,CAACvR,MAAM,CAAC2L,UAAU;IAChD,IAAI,CAACR,iBAAiB,GAAG,IAAI,CAACnL,MAAM,CAACyL,SAAS;IAC9C,IAAI,CAACL,cAAc,GAAG,IAAI,CAACpL,MAAM,CAACsR,IAAI;EAC1C;AACJ;AACA;;AAGA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAME,qBAAqB,SAAS3M,eAAe,CAAC;AAAG;;AAE9D;AACA;AACA;AACA,OAAO,MAAM4M,WAAW,SAASD,qBAAqB,CAAC;;AAEvD;AACA;AACA;AACA,OAAO,MAAME,8BAA8B,SAASF,qBAAqB,CAAC;EACtE;AACJ;AACA;AACA;AACA;AACA;AACA;EACI1M,WAAWA,CAAC9E,MAAM,EAAE3C,OAAO,EAAE0E,sBAAsB,EAAEW,iBAAiB,EAAE;IACpE,KAAK,CAAC1C,MAAM,EAAE3C,OAAO,CAAC;IACtB,IAAI,CAAC0E,sBAAsB,GAAGA,sBAAsB;IACpD,IAAI,CAACW,iBAAiB,GAAGA,iBAAiB;IAE1C,IAAI,CAAC8I,kBAAkB,GAAG,IAAI,CAACxL,MAAM,CAACwL,kBAAkB;IACxD,IAAI,CAACF,iBAAiB,GAAG,IAAI,CAACtL,MAAM,CAACyL,SAAS;IAC9C,IAAI,CAACF,cAAc,GAAG,IAAI,CAACvL,MAAM,CAACsR,IAAI;IAEtC,IAAI,CAACC,kBAAkB,GAAG,IAAI,CAACvR,MAAM,CAAC2L,UAAU;IAChD,IAAI,CAACR,iBAAiB,GAAG,IAAI,CAACnL,MAAM,CAACyL,SAAS;IAC9C,IAAI,CAACL,cAAc,GAAG,IAAI,CAACpL,MAAM,CAACsR,IAAI;EAC1C;AACJ;AACA;;AAGA;AACA;AACA,OAAO,MAAMK,kBAAkB,SAAS9M,eAAe,CAAC;AAAG;AAE3D,OAAO,MAAM+M,QAAQ,SAASD,kBAAkB,CAAC;;AAEjD;AACA;AACA;AACA,OAAO,MAAME,2BAA2B,SAASF,kBAAkB,CAAC;EAEhE;AACJ;AACA;AACA;AACA;AACA;AACA;EACI7M,WAAWA,CAAC9E,MAAM,EAAE3C,OAAO,EAAE0E,sBAAsB,EAAEW,iBAAiB,EAAE;IACpE,KAAK,CAAC1C,MAAM,EAAE3C,OAAO,CAAC;IACtB,IAAI,CAAC0E,sBAAsB,GAAGA,sBAAsB;IACpD,IAAI,CAACW,iBAAiB,GAAGA,iBAAiB;IAE1C,IAAI,CAAC8I,kBAAkB,GAAG,IAAI,CAACxL,MAAM,CAACwL,kBAAkB;IACxD,IAAI,CAACF,iBAAiB,GAAG,IAAI,CAACtL,MAAM,CAACyL,SAAS;IAC9C,IAAI,CAACF,cAAc,GAAG,IAAI,CAACvL,MAAM,CAACsR,IAAI;IAEtC,IAAI,CAACC,kBAAkB,GAAG,IAAI,CAACvR,MAAM,CAAC2L,UAAU;IAChD,IAAI,CAACR,iBAAiB,GAAG,IAAI,CAACnL,MAAM,CAACyL,SAAS;IAC9C,IAAI,CAACL,cAAc,GAAG,IAAI,CAACpL,MAAM,CAACsR,IAAI;EAC1C;AACJ;AACA;;AAEA;AACA;AACA,OAAO,MAAMQ,mBAAmB,SAASjN,eAAe,CAAC;AAAG;;AAE5D;AACA;AACA;AACA,OAAO,MAAMkN,SAAS,SAASD,mBAAmB,CAAC;;AAEnD;AACA;AACA;AACA,OAAO,MAAME,4BAA4B,SAASF,mBAAmB,CAAC;EAElE;AACJ;AACA;AACA;AACA;AACA;AACA;EACIhN,WAAWA,CAAC9E,MAAM,EAAE3C,OAAO,EAAE0E,sBAAsB,EAAEW,iBAAiB,EAAE;IACpE,KAAK,CAAC1C,MAAM,EAAE3C,OAAO,CAAC;IACtB,IAAI,CAAC0E,sBAAsB,GAAGA,sBAAsB;IACpD,IAAI,CAACW,iBAAiB,GAAGA,iBAAiB;IAE1C,IAAI,CAAC8I,kBAAkB,GAAG,IAAI,CAACxL,MAAM,CAACiS,cAAc;IACpD,IAAI,CAAC3G,iBAAiB,GAAG,IAAI,CAACtL,MAAM,CAACkS,uBAAuB;IAC5D,IAAI,CAAC3G,cAAc,GAAG,IAAI,CAACvL,MAAM,CAACmS,OAAO,GAAG,IAAI,CAAC7G,iBAAiB;IAElE,IAAI,CAACiG,kBAAkB,GAAG,IAAI,CAACvR,MAAM,CAACoS,cAAc;IACpD,IAAI,CAACjH,iBAAiB,GAAG,IAAI,CAACnL,MAAM,CAACqS,uBAAuB;IAC5D,IAAI,CAACjH,cAAc,GAAG,IAAI,CAACpL,MAAM,CAACmS,OAAO,GAAG,IAAI,CAAChH,iBAAiB;EACtE;AAEJ;;AAEA;AACA;AACA;AACA,OAAO,MAAMmH,6BAA6B,SAASR,mBAAmB,CAAC;EACnE;AACJ;AACA;AACA;AACA;AACA;EACI,MAAMzL,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIkL,wBAAwB,CAAC,MAAM,KAAK,CAACnG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EACxE;AACJ;;AAEA;;AAEA;AACA;AACA,OAAO,MAAMiR,oBAAoB,SAAS1N,eAAe,CAAC;AAAG;;AAE7D;AACA;AACA;AACA,OAAO,MAAM2N,UAAU,SAASD,oBAAoB,CAAC;;AAErD;AACA;AACA;AACA,OAAO,MAAME,6BAA6B,SAASF,oBAAoB,CAAC;EAEpE;AACJ;AACA;AACA;AACA;AACA;AACA;EACIzN,WAAWA,CAAC9E,MAAM,EAAE3C,OAAO,EAAE0E,sBAAsB,EAAEW,iBAAiB,EAAE;IACpE,KAAK,CAAC1C,MAAM,EAAE3C,OAAO,CAAC;IACtB,IAAI,CAAC0E,sBAAsB,GAAGA,sBAAsB;IACpD,IAAI,CAACW,iBAAiB,GAAGA,iBAAiB;IAE1C,IAAI,CAAC8I,kBAAkB,GAAG,IAAI,CAACxL,MAAM,CAACiS,cAAc;IACpD,IAAI,CAAC3G,iBAAiB,GAAG,IAAI,CAACtL,MAAM,CAACkS,uBAAuB;IAC5D,IAAI,CAAC3G,cAAc,GAAG,IAAI,CAACvL,MAAM,CAACmS,OAAO,GAAG,IAAI,CAAC7G,iBAAiB;IAElE,IAAI,CAACiG,kBAAkB,GAAG,IAAI,CAACvR,MAAM,CAACoS,cAAc;IACpD,IAAI,CAACjH,iBAAiB,GAAG,IAAI,CAACnL,MAAM,CAACqS,uBAAuB;IAC5D,IAAI,CAACjH,cAAc,GAAG,IAAI,CAACpL,MAAM,CAACmS,OAAO,GAAG,IAAI,CAAChH,iBAAiB;EACtE;AAEJ;;AAEA;AACA;AACA;AACA,OAAO,MAAMuH,8BAA8B,SAASH,oBAAoB,CAAC;EACrE;AACJ;AACA;AACA;AACA;AACA;EACI,MAAMlM,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIkL,wBAAwB,CAAC,MAAM,KAAK,CAACnG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EACxE;AACJ;AAGA,OAAO,MAAMqR,gBAAgB,SAASJ,oBAAoB,CAAC;EACvD;AACJ;AACA;AACA;AACA;AACA;EACIzN,WAAWA,CAAC9E,MAAM,EAAE+B,sBAAsB,EAAEW,iBAAiB,EAAE;IAC3D,KAAK,CAAC1C,MAAM,EAAE+B,sBAAsB,CAAC;IACrC,IAAI,CAACW,iBAAiB,GAAGA,iBAAiB;IAE1C,IAAI,CAAC8I,kBAAkB,GAAG,IAAI,CAACxL,MAAM,CAACiS,cAAc;IACpD,IAAI,CAAC3G,iBAAiB,GAAG,IAAI,CAACtL,MAAM,CAACkS,uBAAuB;IAC5D,IAAI,CAAC3G,cAAc,GAAG,IAAI,CAACvL,MAAM,CAACmS,OAAO,GAAG,IAAI,CAAC7G,iBAAiB;IAElE,IAAI,CAACiG,kBAAkB,GAAG,IAAI,CAACvR,MAAM,CAACoS,cAAc;IACpD,IAAI,CAACjH,iBAAiB,GAAG,IAAI,CAACnL,MAAM,CAACqS,uBAAuB;IAC5D,IAAI,CAACjH,cAAc,GAAG,IAAI,CAACpL,MAAM,CAACmS,OAAO,GAAG,IAAI,CAAChH,iBAAiB;EACtE;AACJ;AACA;;AAGA;AACA;AACA,OAAO,MAAMyH,yBAAyB,SAAS/N,eAAe,CAAC;AAAG;;AAElE;AACA;AACA;AACA,OAAO,MAAMgO,eAAe,SAASD,yBAAyB,CAAC;;AAE/D;AACA;AACA;AACA,OAAO,MAAME,kCAAkC,SAASF,yBAAyB,CAAC;EAE9E;AACJ;AACA;AACA;AACA;AACA;AACA;EACI9N,WAAWA,CAAC9E,MAAM,EAAE3C,OAAO,EAAE0E,sBAAsB,EAAEW,iBAAiB,EAAE;IACpE,KAAK,CAAC1C,MAAM,EAAE3C,OAAO,CAAC;IACtB,IAAI,CAAC0E,sBAAsB,GAAGA,sBAAsB;IACpD,IAAI,CAACW,iBAAiB,GAAGA,iBAAiB;IAE1C,IAAI,CAAC8I,kBAAkB,GAAG,IAAI,CAACxL,MAAM,CAACiS,cAAc;IACpD,IAAI,CAAC3G,iBAAiB,GAAG,IAAI,CAACtL,MAAM,CAACkS,uBAAuB;IAC5D,IAAI,CAAC3G,cAAc,GAAG,IAAI,CAACvL,MAAM,CAACmS,OAAO,GAAG,IAAI,CAAC7G,iBAAiB;IAElE,IAAI,CAACiG,kBAAkB,GAAG,IAAI,CAACvR,MAAM,CAACoS,cAAc;IACpD,IAAI,CAACjH,iBAAiB,GAAG,IAAI,CAACnL,MAAM,CAACqS,uBAAuB;IAC5D,IAAI,CAACjH,cAAc,GAAG,IAAI,CAACpL,MAAM,CAACmS,OAAO,GAAG,IAAI,CAAChH,iBAAiB;EACtE;AACJ;AACA;;AAGA;AACA;AACA,OAAO,MAAM4H,8BAA8B,SAASlO,eAAe,CAAC;AAAG;;AAEvE;AACA;AACA;AACA,OAAO,MAAMmO,oBAAoB,SAASD,8BAA8B,CAAC;;AAEzE;AACA;AACA;AACA,OAAO,MAAME,uCAAuC,SAASF,8BAA8B,CAAC;EAExF;AACJ;AACA;AACA;AACA;AACA;AACA;EACIjO,WAAWA,CAAC9E,MAAM,EAAE3C,OAAO,EAAE0E,sBAAsB,EAAEW,iBAAiB,EAAE;IACpE,KAAK,CAAC1C,MAAM,EAAE3C,OAAO,CAAC;IACtB,IAAI,CAAC0E,sBAAsB,GAAGA,sBAAsB;IACpD,IAAI,CAACW,iBAAiB,GAAGA,iBAAiB;IAE1C,IAAI,CAAC8I,kBAAkB,GAAG,IAAI,CAACxL,MAAM,CAACiS,cAAc;IACpD,IAAI,CAAC3G,iBAAiB,GAAG,IAAI,CAACtL,MAAM,CAACkS,uBAAuB;IAC5D,IAAI,CAAC3G,cAAc,GAAG,IAAI,CAACvL,MAAM,CAACmS,OAAO,GAAG,IAAI,CAAC7G,iBAAiB;IAElE,IAAI,CAACiG,kBAAkB,GAAG,IAAI,CAACvR,MAAM,CAACoS,cAAc;IACpD,IAAI,CAACjH,iBAAiB,GAAG,IAAI,CAACnL,MAAM,CAACqS,uBAAuB;IAC5D,IAAI,CAACjH,cAAc,GAAG,IAAI,CAACpL,MAAM,CAACmS,OAAO,GAAG,IAAI,CAAChH,iBAAiB;EACtE;AACJ;AACA;;AAGA;AACA;AACA,OAAO,MAAM+H,sBAAsB,SAASrO,eAAe,CAAC;AAC5D,OAAO,MAAMsO,YAAY,SAASD,sBAAsB,CAAC;;AAEzD;AACA;AACA;AACA,OAAO,MAAME,kBAAkB,SAASF,sBAAsB,CAAC;EAC3D;AACJ;AACA;AACA;AACA;AACA;EACI,MAAM7M,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIgL,cAAc,CAAC,MAAM,KAAK,CAACjG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EAC9D;AACJ;;AAEA;AACA;AACA;AACA,OAAO,MAAM+R,gCAAgC,SAASH,sBAAsB,CAAC;EACzE;AACJ;AACA;AACA;AACA;AACA;EACI,MAAM7M,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIkL,wBAAwB,CAAC,MAAM,KAAK,CAACnG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EACxE;AACJ;;AAEA;AACA;AACA;AACA,OAAO,MAAMgS,6BAA6B,SAASJ,sBAAsB,CAAC;EACtE;AACJ;AACA;AACA;AACA;AACA;EACI,MAAM7M,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIoL,qBAAqB,CAAC,MAAM,KAAK,CAACrG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EACrE;AACJ;;AAEA;AACA;AACA;AACA,OAAO,MAAMiS,2BAA2B,SAASL,sBAAsB,CAAC;EACpE;AACJ;AACA;AACA;AACA;AACA;EACI,MAAM7M,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIsL,4BAA4B,CAAC,MAAM,KAAK,CAACvG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EAC5E;AACJ;AACA;;AAGA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMkS,kBAAkB,SAAS3O,eAAe,CAAC;;AAExD;AACA;AACA;AACA,OAAO,MAAM4O,QAAQ,SAASD,kBAAkB,CAAC;;AAEjD;AACA;AACA;AACA,OAAO,MAAME,kBAAkB,SAASF,kBAAkB,CAAC;EACvD;AACJ;AACA;AACA;AACA;AACA;EACI,MAAMnN,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIgL,cAAc,CAAC,MAAM,KAAK,CAACjG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EAC9D;AACJ;;AAEA;AACA;AACA;AACA,OAAO,MAAMqS,4BAA4B,SAASH,kBAAkB,CAAC;EACjE;AACJ;AACA;AACA;AACA;AACA;EACI,MAAMnN,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIkL,wBAAwB,CAAC,MAAM,KAAK,CAACnG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EACxE;AACJ;;AAEA;AACA;AACA;AACA,OAAO,MAAMsS,yBAAyB,SAASJ,kBAAkB,CAAC;EAC9D;AACJ;AACA;AACA;AACA;AACA;EACI,MAAMnN,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIoL,qBAAqB,CAAC,MAAM,KAAK,CAACrG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EACrE;AACJ;;AAEA;AACA;AACA;AACA,OAAO,MAAMuS,uBAAuB,SAASL,kBAAkB,CAAC;EAC5D;AACJ;AACA;AACA;AACA;AACA;EACI,MAAMnN,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIsL,4BAA4B,CAAC,MAAM,KAAK,CAACvG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EAC5E;AACJ;AACA;;AAEA;AACA;AACA,OAAO,MAAMwS,yBAAyB,SAASjP,eAAe,CAAC;AAC/D,OAAO,MAAMkP,eAAe,SAASD,yBAAyB,CAAC;;AAE/D;AACA;AACA;AACA,OAAO,MAAME,qBAAqB,SAASF,yBAAyB,CAAC;EACjE;AACJ;AACA;AACA;AACA;AACA;EACI,MAAMzN,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIgL,cAAc,CAAC,MAAM,KAAK,CAACjG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EAC9D;AACJ;;AAEA;AACA;AACA;AACA,OAAO,MAAM2S,mCAAmC,SAASH,yBAAyB,CAAC;EAC/E;AACJ;AACA;AACA;AACA;AACA;EACI,MAAMzN,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIkL,wBAAwB,CAAC,MAAM,KAAK,CAACnG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EACxE;AACJ;;AAEA;AACA;AACA;AACA,OAAO,MAAM4S,gCAAgC,SAASJ,yBAAyB,CAAC;EAC5E;AACJ;AACA;AACA;AACA;AACA;EACI,MAAMzN,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIoL,qBAAqB,CAAC,MAAM,KAAK,CAACrG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EACrE;AACJ;;AAEA;AACA;AACA;AACA,OAAO,MAAM6S,8BAA8B,SAASL,yBAAyB,CAAC;EAC1E;AACJ;AACA;AACA;AACA;AACA;EACI,MAAMzN,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIsL,4BAA4B,CAAC,MAAM,KAAK,CAACvG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EAC5E;AACJ;AACA;;AAEA;AACA;AACA,OAAO,MAAM8S,kBAAkB,SAASvP,eAAe,CAAC;AAAG;;AAE3D;AACA;AACA;AACA,OAAO,MAAMwP,QAAQ,SAASD,kBAAkB,CAAC;;AAEjD;AACA;AACA;AACA;AACA,OAAO,MAAME,yBAAyB,SAASF,kBAAkB,CAAC;AAClE;;AAEA;AACA;AACA,OAAO,MAAMG,sBAAsB,SAAS1P,eAAe,CAAC;AAAG;;AAE/D;AACA;AACA;AACA,OAAO,MAAM2P,YAAY,SAASD,sBAAsB,CAAC;;AAEzD;AACA;AACA;AACA,OAAO,MAAME,+BAA+B,SAASF,sBAAsB,CAAC;EAExEzR,uBAAuB,GAAG,KAAK;EAC/BY,eAAe,GAAG,gBAAgB;;EAElC;AACJ;AACA;AACA;AACA;AACA;AACA;EACIoB,WAAWA,CAAC9E,MAAM,EAAE3C,OAAO,EAAE0E,sBAAsB,EAAEW,iBAAiB,EAAE;IACpE,KAAK,CAAC1C,MAAM,EAAE3C,OAAO,CAAC;IACtB,IAAI,CAAC0E,sBAAsB,GAAGA,sBAAsB;IACpD,IAAI,CAACW,iBAAiB,GAAGA,iBAAiB;IAE1C,IAAI,CAAC8I,kBAAkB,GAAG,IAAI,CAACxL,MAAM,CAACiS,cAAc;IACpD,IAAI,CAAC3G,iBAAiB,GAAG,IAAI,CAACtL,MAAM,CAACkS,uBAAuB;IAC5D,IAAI,CAAC3G,cAAc,GAAG,IAAI,CAACvL,MAAM,CAACmS,OAAO,GAAG,IAAI,CAAC7G,iBAAiB;IAElE,IAAI,CAACiG,kBAAkB,GAAG,IAAI,CAACvR,MAAM,CAACoS,cAAc;IACpD,IAAI,CAACjH,iBAAiB,GAAG,IAAI,CAACnL,MAAM,CAACqS,uBAAuB;IAC5D,IAAI,CAACjH,cAAc,GAAG,IAAI,CAACpL,MAAM,CAACmS,OAAO,GAAG,IAAI,CAAChH,iBAAiB;EACtE;;EAEA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;EAEI;AACJ;AACA;AACA;AACA;AACA;AACA;EACI,MAAM1D,QAAQA,CACVnK,MAAM,EACNoF,iBAAiB,GAAG,IAAI,EACxB8D,gBAAgB,GAAG;EACnB;EACA;EACA;EACA;EACA;EACA;EAAA,EACF;IACE;IACA9D,iBAAiB,GAAG,IAAI,CAAC4E,sBAAsB,CAAC5E,iBAAiB,CAAC;;IAGlE;IACAA,iBAAiB,CAACgS,iBAAiB,KAAK,KAAK;;IAE7C;;IAEA,IAAIhS,iBAAiB,CAACgS,iBAAiB,EAAE;MACrClO,gBAAgB,GAAG,CAAC,IAAI/L,+BAA+B,CAACiI,iBAAiB,CAAC,CAAC;IAC/E;IAEA,IAAIA,iBAAiB,CAACiS,uBAAuB,EAAE;MAC3CjS,iBAAiB,CAACoG,iBAAiB,GAAG,IAAI;MAC1CpG,iBAAiB,CAACuH,uBAAuB,GAAG,IAAI;MAEhD,IAAIvH,iBAAiB,CAACkS,IAAI,KAAK,WAAW,EAAE;QACxC1X,OAAO,CAACC,IAAI,CAAC,kEAAkE,CAAC;MACpF;MAEA,IAAI,CAACuF,iBAAiB,CAACmS,eAAe,EAAE;QACpC,MAAM,IAAI5W,KAAK,CACX,0FAA0F,GAC1F,6HACJ,CAAC;MACL;IACJ;IAEA,MAAM6W,OAAO,GAAG,MAAM,KAAK,CAACrN,QAAQ,CAACnK,MAAM,EAAEoF,iBAAiB,EAAE8D,gBAAgB,CAAC;IAEjF,IAAI9D,iBAAiB,CAACiS,uBAAuB,IAAIjS,iBAAiB,CAACmS,eAAe,EAAE;MAChFC,OAAO,CAAC,kBAAkB,CAAC,GAAG,IAAI,CAACC,yBAAyB,CACxDD,OAAO,EACPpS,iBAAiB,CAACmS,eAAe,EACjCnS,iBAAiB,CAACsS,UACtB,CAAC;IACL;IAEA,OAAOF,OAAO;EAClB;;EAEA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACIC,yBAAyBA,CAACE,gBAAgB,EAAEJ,eAAe,EAAEG,UAAU,GAAG,IAAI,EAAEE,cAAc,GAAG,IAAI,EAAE;IACnG,IAAI,CAACD,gBAAgB,CAAC9K,gBAAgB,EAAE;MACpC,MAAM,IAAIlM,KAAK,CACX,qEAAqE,GACrE,uFACJ,CAAC;IACL;IAEA,IAAIkX,mBAAmB,GAAG,IAAI,CAACnV,MAAM,CAACmV,mBAAmB;IACzD,IAAIA,mBAAmB,KAAK9K,SAAS,EAAE;MACnCnN,OAAO,CAACC,IAAI,CAAC,sEAAsE,CAAC;MACpFgY,mBAAmB,GAAG,CAAC;IAC3B;IAEA,MAAMC,eAAe,GAAGH,gBAAgB,CAAC9K,gBAAgB,CAACzK,GAAG,CAACoK,KAAK,IAAI;MACnE;MACA;MACA,IAAIK,gBAAgB,GAAGhL,KAAK,CAACK,IAAI,CAAC;QAAEvC,MAAM,EAAE,IAAI,CAAC+C,MAAM,CAACiS;MAAe,CAAC,EACpE,CAACoD,CAAC,EAAE1U,CAAC,KAAK3F,GAAG,CAAC8O,KAAK,CAACpK,GAAG,CAACJ,CAAC,IAAIA,CAAC,CAACqB,CAAC,CAAC,CAAC,EAAE,CAAC,CACzC,CAAC;MAED,IAAI2U,OAAO,GAAGla,KAAK,CAACyZ,eAAe,CAACnV,GAAG,CAAC,CAAC,CAAC6V,CAAC,EAAEC,CAAC,CAAC,KAAK;QAChD,OAAOR,UAAU,GACX7K,gBAAgB,CAACoL,CAAC,CAAC,CAACtU,KAAK,CAAC,IAAI,EAAEuU,CAAC,EAAE,IAAI,EAAE,CAAC,CAAC,EAAER,UAAU,CAAC,CAAC,GACzD7K,gBAAgB,CAACoL,CAAC,CAAC,CAACtU,KAAK,CAAC,IAAI,EAAEuU,CAAC,CAAC;MAC5C,CAAC,CAAC,CAAC;MACHF,OAAO,GAAGA,OAAO,CAACG,SAAS,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC;MAEvC,IAAI,CAACC,GAAG,EAAEC,cAAc,CAAC,GAAGta,QAAQ,CAACia,OAAO,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,IAAI,CAAC;;MAE1D;MACA,IAAIM,eAAe,GAAGN,OAAO,CAACtX,KAAK,CAAC,CAAC,CAAC,CAAC;;MAEvC,KAAK,IAAIwL,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGoM,eAAe,CAACtV,IAAI,CAAC,CAAC,CAAC,EAAE,EAAEkJ,CAAC,EAAE;QAC9C,IAAIqM,OAAO,GAAGD,eAAe,CAACpM,CAAC,CAAC,CAAC,CAAC;;QAElC,KAAK,IAAIC,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGoM,OAAO,CAACvV,IAAI,CAAC,CAAC,CAAC,EAAE,EAAEmJ,CAAC,EAAE;UACtC,IAAIqM,OAAO,GAAGD,OAAO,CAACpM,CAAC,CAAC,CAAC,CAAC;;UAE1B,MAAMsM,SAAS,GAAGL,GAAG,CAAClM,CAAC,CAAC,CAACC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;UAChC,MAAMuM,UAAU,GAAGL,cAAc,CAACnM,CAAC,CAAC,CAACC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;;UAE5C,KAAK,IAAIwM,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGH,OAAO,CAACxV,IAAI,CAAC,CAAC,CAAC,EAAE,EAAE2V,CAAC,EAAE;YAEtC,IAAIC,OAAO,GAAGJ,OAAO,CAACG,CAAC,CAAC,CAAC,CAAC;YAC1B,KAAK,IAAIE,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGD,OAAO,CAAC7V,IAAI,CAACpD,MAAM,EAAE,EAAEkZ,CAAC,EAAE;cAC1CD,OAAO,CAAC7V,IAAI,CAAC8V,CAAC,CAAC,GAAG,CAACD,OAAO,CAAC7V,IAAI,CAAC8V,CAAC,CAAC,GAAGH,UAAU,CAAC3V,IAAI,CAAC8V,CAAC,CAAC,IAAIJ,SAAS,CAAC1V,IAAI,CAAC8V,CAAC,CAAC;YAChF;;YAEA;YACAD,OAAO,CAAC7V,IAAI,CAAC+V,GAAG,CAAC3a,YAAY,CAACya,OAAO,CAAC7V,IAAI,EAAE8U,mBAAmB,CAAC,CAAC;UACrE;QACJ;MACJ;;MAEA;MACA,MAAMkB,MAAM,GAAGnb,IAAI,CAAC0a,eAAe,EAAE,CAAC,CAAC;MACvC,OAAOS,MAAM;IACjB,CAAC,CAAC;IAEF,MAAMC,eAAe,GAAG,CAACrB,gBAAgB,CAACjL,SAAS,CAAC/M,MAAM,EAAEgY,gBAAgB,CAACjL,SAAS,CAAC,CAAC,CAAC,CAAC/M,MAAM,CAAC;IAEjG,MAAMsZ,UAAU,GAAG,IAAIjb,MAAM,CACzB,SAAS,EACT,IAAIkb,YAAY,CAACF,eAAe,CAAC,CAAC,CAAC,GAAGA,eAAe,CAAC,CAAC,CAAC,CAAC,EACzDA,eACJ,CAAC;;IAED;IACA,KAAK,IAAIG,SAAS,GAAG,CAAC,EAAEA,SAAS,GAAGH,eAAe,CAAC,CAAC,CAAC,EAAE,EAAEG,SAAS,EAAE;MACjE;MACA;MACA,MAAMJ,MAAM,GAAGjB,eAAe,CAACqB,SAAS,CAAC,CAACC,GAAG,CAAC,CAAC,CAACC,QAAQ,CAAC,CAAC,CAAC;MAC3D,IAAI,CAACC,YAAY,EAAEC,YAAY,CAAC,GAAG5b,kBAAkB,CAACob,MAAM,CAAC;MAE7D,IAAIS,KAAK,GAAG3X,KAAK,CAACK,IAAI,CAAC;QAAEvC,MAAM,EAAE2Z,YAAY,CAAC3Z,MAAM,GAAG;MAAE,CAAC,EAAE,CAAC8Z,CAAC,EAAEpW,CAAC,KAAKiW,YAAY,CAACjW,CAAC,GAAG,CAAC,CAAC,GAAGiW,YAAY,CAACjW,CAAC,CAAC,CAAC;MAC5G,IAAIqW,KAAK,GAAGhd,WAAW,CAAC,CAAC,CAAC,CAAC,EAAE8c,KAAK,CAAC,CAACpX,GAAG,CAACJ,CAAC,IAAI,CAAC,CAACA,CAAC,CAAC,CAAC,CAAC;;MAEnD,IAAI2X,UAAU,GAAG,EAAE;MACnB,KAAK,IAAItW,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGqW,KAAK,CAAC/Z,MAAM,EAAE,EAAE0D,CAAC,EAAE;QACnC,IAAIqW,KAAK,CAACrW,CAAC,CAAC,EAAE;UACVsW,UAAU,CAACpZ,IAAI,CAACgZ,YAAY,CAAClW,CAAC,CAAC,GAAGuU,cAAc,CAAC;UACjD;QACJ;MACJ;MACAqB,UAAU,CAACE,SAAS,CAAC,CAACpW,IAAI,CAAC+V,GAAG,CAACa,UAAU,EAAE,CAAC,CAAC;IACjD;IAEA,OAAOV,UAAU;EACrB;AACJ;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO,MAAMW,yBAAyB,SAASrS,eAAe,CAAC;EAC3DnB,eAAe,GAAG,cAAc;;EAEhC;AACJ;AACA;AACA;AACA;AACA;AACA;EACIoB,WAAWA,CAAC9E,MAAM,EAAE3C,OAAO,EAAE0E,sBAAsB,EAAEW,iBAAiB,EAAE;IACpE,KAAK,CAAC1C,MAAM,EAAE3C,OAAO,CAAC;IACtB,IAAI,CAAC0E,sBAAsB,GAAGA,sBAAsB;IACpD,IAAI,CAACW,iBAAiB,GAAGA,iBAAiB;;IAE1C;IACA,MAAMyU,aAAa,GAAG,IAAI,CAACnX,MAAM,CAACoX,OAAO;IACzC,MAAMC,aAAa,GAAG,IAAI,CAACrX,MAAM,CAACsX,OAAO;;IAEzC;IACA,MAAMC,gBAAgB,GAAGJ,aAAa,CAAC/Q,UAAU;IACjD,MAAMoR,YAAY,GACdC,gCAAgC,CAACzS,GAAG,CAACuS,gBAAgB,CAAC,IACnDG,mCAAmC,CAAC1S,GAAG,CAACuS,gBAAgB,CAAC;IAChE,IAAI,CAACC,YAAY,EAAE;MACfta,OAAO,CAACC,IAAI,CAAC,2BAA2Boa,gBAAgB,qIAAqI,CAAC;IAClM;;IAEA;IACA,MAAMI,YAAY,GAAG/P,gCAAgC,CAAC5C,GAAG,CAACqS,aAAa,CAACjR,UAAU,CAAC;IACnF,IAAI,CAACuR,YAAY,EAAE;MACf,MAAM,IAAI1Z,KAAK,CAAC,6EAA6E,IAAI,CAAC+B,MAAM,CAACsX,OAAO,CAAClR,UAAU,GAAG,CAAC;IACnI;;IAEA;IACA,MAAMwR,iBAAiB,GAAGD,YAAY,CAAC,CAAC,CAAC;IACzC;IACA,MAAML,OAAO,GAAG,IAAIM,iBAAiB,CAACP,aAAa,EAAEtV,sBAAsB,EAAEW,iBAAiB,CAAC;IAE/F,IAAI,CAACuI,eAAe,GAAG,oBAAoB,IAAIqM,OAAO;IACtD,IAAI,IAAI,CAACrM,eAAe,EAAE;MACtB;MACA,IAAI,CAACO,kBAAkB,GAAG8L,OAAO,CAAC9L,kBAAkB;MACpD,IAAI,CAACF,iBAAiB,GAAGgM,OAAO,CAAChM,iBAAiB;MAClD,IAAI,CAACC,cAAc,GAAG+L,OAAO,CAAC/L,cAAc;MAE5C,IAAI,CAACgG,kBAAkB,GAAG+F,OAAO,CAAC/F,kBAAkB;MACpD,IAAI,CAACpG,iBAAiB,GAAGmM,OAAO,CAACnM,iBAAiB;MAClD,IAAI,CAACC,cAAc,GAAGkM,OAAO,CAAClM,cAAc;IAEhD,CAAC,MAAM;MACH;MACA,IAAI,CAACO,UAAU,GAAG2L,OAAO,CAAC3L,UAAU;MACpC,IAAI,CAACF,SAAS,GAAG6L,OAAO,CAAC7L,SAAS;MAClC,IAAI,CAACC,MAAM,GAAG4L,OAAO,CAAC5L,MAAM;IAChC;EACJ;AACJ;AACA;;AAEA;AACA;AACA,OAAO,MAAMmM,mBAAmB,SAAShT,eAAe,CAAC;;AAEzD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMiT,SAAS,SAASD,mBAAmB,CAAC;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAME,2BAA2B,SAASF,mBAAmB,CAAC;EAEjE;EACA,aAAahS,eAAeA,CAACpJ,6BAA6B,EAAEE,OAAO,GAAG,CAAC,CAAC,EAAE;IACtE;IACAA,OAAO,CAACuJ,eAAe,KAAK,YAAY;IACxC,OAAO,KAAK,CAACL,eAAe,CAACpJ,6BAA6B,EAAEE,OAAO,CAAC;EACxE;AACJ;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMqb,6BAA6B,SAASH,mBAAmB,CAAC;EACnE;EACA,aAAahS,eAAeA,CAACpJ,6BAA6B,EAAEE,OAAO,GAAG,CAAC,CAAC,EAAE;IACtE;IACAA,OAAO,CAACuJ,eAAe,KAAK,cAAc;IAC1C,OAAO,KAAK,CAACL,eAAe,CAACpJ,6BAA6B,EAAEE,OAAO,CAAC;EACxE;AACJ;AACA;;AAGA;AACA;AACA,OAAO,MAAMsb,qBAAqB,SAASpT,eAAe,CAAC;;AAE3D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMqT,WAAW,SAASD,qBAAqB,CAAC;;AAEvD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAME,eAAe,SAASF,qBAAqB,CAAC;EAEvD;EACA,aAAapS,eAAeA,CAACpJ,6BAA6B,EAAEE,OAAO,GAAG,CAAC,CAAC,EAAE;IACtE;IACAA,OAAO,CAACuJ,eAAe,KAAK,YAAY;IACxC,OAAO,KAAK,CAACL,eAAe,CAACpJ,6BAA6B,EAAEE,OAAO,CAAC;EACxE;AACJ;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMyb,iBAAiB,SAASP,mBAAmB,CAAC;EACvD;EACA,aAAahS,eAAeA,CAACpJ,6BAA6B,EAAEE,OAAO,GAAG,CAAC,CAAC,EAAE;IACtE;IACAA,OAAO,CAACuJ,eAAe,KAAK,cAAc;IAC1C,OAAO,KAAK,CAACL,eAAe,CAACpJ,6BAA6B,EAAEE,OAAO,CAAC;EACxE;AACJ;AACA;AACA;AACA,OAAO,MAAM0b,0BAA0B,SAASxT,eAAe,CAAC;AAEhE,OAAO,MAAMyT,gBAAgB,SAASD,0BAA0B,CAAC;AACjE;;AAGA;AACA;AACA,OAAO,MAAME,sBAAsB,SAAS1T,eAAe,CAAC;AAE5D,OAAO,MAAM2T,YAAY,SAASD,sBAAsB,CAAC;;AAEzD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAME,2BAA2B,SAASF,sBAAsB,CAAC;AACxE;;AAGA;AACA;AACA,OAAO,MAAMG,mBAAmB,SAAS7T,eAAe,CAAC;EACrD;AACJ;AACA;AACA;AACA;AACA;EACIC,WAAWA,CAAC9E,MAAM,EAAE3C,OAAO,EAAEqF,iBAAiB,EAAE;IAC5C,KAAK,CAAC1C,MAAM,EAAE3C,OAAO,CAAC;IACtB,IAAI,CAACqF,iBAAiB,GAAGA,iBAAiB;;IAE1C;IACA,IAAI,CAAC1C,MAAM,CAACD,YAAY,GAAG,IAAI,CAACC,MAAM,CAACC,YAAY;IAEnD,IAAI,CAACwL,SAAS,GAAG,IAAI,CAACzL,MAAM,CAAC2Y,MAAM;IACnC,IAAI,CAAChN,UAAU,GAAG,IAAI,CAAC3L,MAAM,CAAC4Y,OAAO;IACrC,IAAI,CAAClN,MAAM,GAAG,IAAI,CAAC1L,MAAM,CAAC6Y,MAAM,GAAG,IAAI,CAACpN,SAAS;EACrD;AACJ;AAEA,OAAO,MAAMqN,SAAS,SAASJ,mBAAmB,CAAC;;AAEnD;AACA;AACA;AACA,OAAO,MAAMK,eAAe,SAASL,mBAAmB,CAAC;AACzD;AACA;AACA;AACA;;AAEA;AACA;AACA,OAAO,MAAMM,qBAAqB,SAASnU,eAAe,CAAC;EACvD;AACJ;AACA;AACA;AACA;AACA;EACIC,WAAWA,CAAC9E,MAAM,EAAE3C,OAAO,EAAEqF,iBAAiB,EAAE;IAC5C,KAAK,CAAC1C,MAAM,EAAE3C,OAAO,CAAC;IACtB,IAAI,CAACqF,iBAAiB,GAAGA,iBAAiB;;IAE1C;IACA,IAAI,CAAC1C,MAAM,CAACD,YAAY,GAAG,IAAI,CAACC,MAAM,CAACC,YAAY;IAEnD,IAAI,CAACwL,SAAS,GAAG,IAAI,CAACzL,MAAM,CAACyL,SAAS;IACtC,IAAI,CAACE,UAAU,GAAG,IAAI,CAAC3L,MAAM,CAAC2L,UAAU;IACxC,IAAI,CAACD,MAAM,GAAG,IAAI,CAAC1L,MAAM,CAACiZ,WAAW,GAAG,IAAI,CAACxN,SAAS;EAC1D;AACJ;AACA,OAAO,MAAMyN,WAAW,SAASF,qBAAqB,CAAC;AAEvD,OAAO,MAAMG,iBAAiB,SAASH,qBAAqB,CAAC;AAC7D;;AAEA;AACA;AACA,OAAO,MAAMI,sBAAsB,SAASvU,eAAe,CAAC;EACxD;AACJ;AACA;AACA;AACA;AACA;EACIC,WAAWA,CAAC9E,MAAM,EAAE3C,OAAO,EAAEqF,iBAAiB,EAAE;IAC5C,KAAK,CAAC1C,MAAM,EAAE3C,OAAO,CAAC;IACtB,IAAI,CAACqF,iBAAiB,GAAGA,iBAAiB;;IAE1C;IACA,IAAI,CAAC1C,MAAM,CAACD,YAAY,GAAG,IAAI,CAACC,MAAM,CAACC,YAAY;IAEnD,IAAI,CAACwL,SAAS,GAAG,IAAI,CAACzL,MAAM,CAACqZ,mBAAmB;IAChD,IAAI,CAAC1N,UAAU,GAAG,IAAI,CAAC3L,MAAM,CAACsZ,iBAAiB;IAC/C,IAAI,CAAC5N,MAAM,GAAG,IAAI,CAAC1L,MAAM,CAACiZ,WAAW,GAAG,IAAI,CAACxN,SAAS;EAC1D;AACJ;AACA,OAAO,MAAM8N,YAAY,SAASH,sBAAsB,CAAC;AAEzD,OAAO,MAAMI,kBAAkB,SAASJ,sBAAsB,CAAC;AAC/D;;AAGA;AACA;AACA,OAAO,MAAMK,mBAAmB,SAAS5U,eAAe,CAAC;EACrD;AACJ;AACA;AACA;AACA;AACA;EACIC,WAAWA,CAAC9E,MAAM,EAAE3C,OAAO,EAAEqF,iBAAiB,EAAE;IAC5C,KAAK,CAAC1C,MAAM,EAAE3C,OAAO,CAAC;IACtB,IAAI,CAACqF,iBAAiB,GAAGA,iBAAiB;;IAE1C;IACA,IAAI,CAAC1C,MAAM,CAACD,YAAY,GAAG,IAAI,CAACC,MAAM,CAACC,YAAY;IAEnD,IAAI,CAACwL,SAAS,GAAG,IAAI,CAACzL,MAAM,CAAC2Y,MAAM;IACnC,IAAI,CAAChN,UAAU,GAAG,IAAI,CAAC3L,MAAM,CAAC4Y,OAAO;IACrC,IAAI,CAAClN,MAAM,GAAG,IAAI,CAAC1L,MAAM,CAAC6Y,MAAM,GAAG,IAAI,CAACpN,SAAS;EACrD;AACJ;AAEA,OAAO,MAAMiO,SAAS,SAASD,mBAAmB,CAAC;AAEnD,OAAO,MAAME,eAAe,SAASF,mBAAmB,CAAC;AACzD;;AAGA;AACA;AACA,OAAO,MAAMG,yBAAyB,SAAS/U,eAAe,CAAC;EAC3D;AACJ;AACA;AACA;AACA;AACA;EACIC,WAAWA,CAAC9E,MAAM,EAAE3C,OAAO,EAAEqF,iBAAiB,EAAE;IAC5C,KAAK,CAAC1C,MAAM,EAAE3C,OAAO,CAAC;IACtB,IAAI,CAACqF,iBAAiB,GAAGA,iBAAiB;;IAE1C;IACA,IAAI,CAAC1C,MAAM,CAACD,YAAY,GAAG,IAAI,CAACC,MAAM,CAACC,YAAY;IAEnD,IAAI,CAACwL,SAAS,GAAG,IAAI,CAACzL,MAAM,CAAC2Y,MAAM;IACnC,IAAI,CAAChN,UAAU,GAAG,IAAI,CAAC3L,MAAM,CAAC4Y,OAAO;IACrC,IAAI,CAAClN,MAAM,GAAG,IAAI,CAAC1L,MAAM,CAAC6Y,MAAM,GAAG,IAAI,CAACpN,SAAS;EACrD;AACJ;AAEA,OAAO,MAAMoO,eAAe,SAASD,yBAAyB,CAAC;AAE/D,OAAO,MAAME,qBAAqB,SAASF,yBAAyB,CAAC;AACrE;;AAEA;AACA;AACA,OAAO,MAAMG,sBAAsB,SAASlV,eAAe,CAAC;EACxD;AACJ;AACA;AACA;AACA;AACA;EACIC,WAAWA,CAAC9E,MAAM,EAAE3C,OAAO,EAAEqF,iBAAiB,EAAE;IAC5C,KAAK,CAAC1C,MAAM,EAAE3C,OAAO,CAAC;IACtB,IAAI,CAACqF,iBAAiB,GAAGA,iBAAiB;;IAE1C;IACA,IAAI,CAAC1C,MAAM,CAACD,YAAY,GAAG,IAAI,CAACC,MAAM,CAACC,YAAY;IAEnD,IAAI,CAACwL,SAAS,GAAG,IAAI,CAACzL,MAAM,CAAC2Y,MAAM;IACnC,IAAI,CAAChN,UAAU,GAAG,IAAI,CAAC3L,MAAM,CAAC4Y,OAAO;IACrC,IAAI,CAAClN,MAAM,GAAG,IAAI,CAAC1L,MAAM,CAAC6Y,MAAM,GAAG,IAAI,CAACpN,SAAS;EACrD;AACJ;AACA;AACA;AACA;AACA,OAAO,MAAMuO,YAAY,SAASD,sBAAsB,CAAC;;AAEzD;AACA;AACA;AACA,OAAO,MAAME,kBAAkB,SAASF,sBAAsB,CAAC;AAC/D;;AAGA;AACA;;AAEA;AACA;AACA;AACA,OAAO,MAAMG,oBAAoB,SAASrV,eAAe,CAAC;EACtD;AACJ;AACA;AACA;AACA;AACA;EACIC,WAAWA,CAAC9E,MAAM,EAAE3C,OAAO,EAAEqF,iBAAiB,EAAE;IAC5C,KAAK,CAAC1C,MAAM,EAAE3C,OAAO,CAAC;IACtB,IAAI,CAACqF,iBAAiB,GAAGA,iBAAiB;;IAE1C;IACA,IAAI,CAAC1C,MAAM,CAACD,YAAY,GAAG,IAAI,CAACC,MAAM,CAACC,YAAY;IAEnD,IAAI,CAACwL,SAAS,GAAG,IAAI,CAACzL,MAAM,CAACma,mBAAmB,IAAI,IAAI,CAACna,MAAM,CAACqZ,mBAAmB;IACnF,IAAI,CAAC1N,UAAU,GAAG,IAAI,CAAC3L,MAAM,CAACsZ,iBAAiB;IAC/C,IAAI,CAAC5N,MAAM,GAAG,IAAI,CAAC1L,MAAM,CAACiZ,WAAW,GAAG,IAAI,CAACjZ,MAAM,CAACqZ,mBAAmB;EAC3E;AACJ;AACA;AACA;AACA;AACA,OAAO,MAAMe,UAAU,SAASF,oBAAoB,CAAC;AAErD,OAAO,MAAMG,gBAAgB,SAASH,oBAAoB,CAAC;AAC3D;;AAEA;AACA;;AAEA;AACA;AACA;AACA,OAAO,MAAMI,oBAAoB,SAASzV,eAAe,CAAC;EACtD;AACJ;AACA;AACA;AACA;AACA;EACIC,WAAWA,CAAC9E,MAAM,EAAE3C,OAAO,EAAEqF,iBAAiB,EAAE;IAC5C,KAAK,CAAC1C,MAAM,EAAE3C,OAAO,CAAC;IACtB,IAAI,CAACqF,iBAAiB,GAAGA,iBAAiB;;IAE1C;IACA,IAAI,CAAC1C,MAAM,CAACD,YAAY,GAAG,IAAI,CAACC,MAAM,CAACC,YAAY;IAEnD,IAAI,CAACwL,SAAS,GAAG,IAAI,CAACzL,MAAM,CAACma,mBAAmB,IAAI,IAAI,CAACna,MAAM,CAACqZ,mBAAmB;IACnF,IAAI,CAAC1N,UAAU,GAAG,IAAI,CAAC3L,MAAM,CAACsZ,iBAAiB;IAC/C,IAAI,CAAC5N,MAAM,GAAG,IAAI,CAAC1L,MAAM,CAACiZ,WAAW,GAAG,IAAI,CAACjZ,MAAM,CAACqZ,mBAAmB;EAC3E;AACJ;AACA;AACA;AACA;AACA,OAAO,MAAMkB,UAAU,SAASD,oBAAoB,CAAC;AAErD,OAAO,MAAME,gBAAgB,SAASF,oBAAoB,CAAC;AAC3D;;AAGA;AACA;;AAEA,OAAO,MAAMG,kBAAkB,SAAS5V,eAAe,CAAC;EACpD;AACJ;AACA;AACA;AACA;AACA;EACIC,WAAWA,CAAC9E,MAAM,EAAE3C,OAAO,EAAEqF,iBAAiB,EAAE;IAC5C,KAAK,CAAC1C,MAAM,EAAE3C,OAAO,CAAC;IACtB,IAAI,CAACqF,iBAAiB,GAAGA,iBAAiB;;IAE1C;IACA,IAAI,CAAC1C,MAAM,CAACD,YAAY,GAAG,IAAI,CAACC,MAAM,CAACC,YAAY;IAEnD,IAAI,CAACwL,SAAS,GAAG,IAAI,CAACzL,MAAM,CAACqZ,mBAAmB;IAChD,IAAI,CAAC1N,UAAU,GAAG,IAAI,CAAC3L,MAAM,CAACsZ,iBAAiB;IAC/C,IAAI,CAAC5N,MAAM,GAAG,IAAI,CAAC1L,MAAM,CAACiZ,WAAW,GAAG,IAAI,CAACxN,SAAS;EAC1D;AACJ;AACA;AACA;AACA;AACA,OAAO,MAAMiP,QAAQ,SAASD,kBAAkB,CAAC;AAEjD,OAAO,MAAME,cAAc,SAASF,kBAAkB,CAAC;AACvD;;AAGA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMG,oBAAoB,SAAS/V,eAAe,CAAC;EACtD;AACJ;AACA;AACA;AACA;AACA;EACIC,WAAWA,CAAC9E,MAAM,EAAE3C,OAAO,EAAEqF,iBAAiB,EAAE;IAC5C,KAAK,CAAC1C,MAAM,EAAE3C,OAAO,CAAC;IACtB,IAAI,CAACqF,iBAAiB,GAAGA,iBAAiB;;IAE1C;IACA,IAAI,CAAC1C,MAAM,CAACD,YAAY,GAAG,IAAI,CAACC,MAAM,CAACC,YAAY;IAEnD,IAAI,CAACwL,SAAS,GAAG,IAAI,CAACzL,MAAM,CAAC2Y,MAAM;IACnC,IAAI,CAAChN,UAAU,GAAG,IAAI,CAAC3L,MAAM,CAAC4Y,OAAO;IACrC,IAAI,CAAClN,MAAM,GAAG,IAAI,CAAC1L,MAAM,CAACiZ,WAAW,GAAG,IAAI,CAACxN,SAAS;EAC1D;AACJ;;AAEA;AACA;AACA;AACA,OAAO,MAAMoP,UAAU,SAASD,oBAAoB,CAAC;;AAErD;AACA;AACA;AACA,OAAO,MAAME,gBAAgB,SAASF,oBAAoB,CAAC;AAC3D;;AAEA;AACA;AACA,OAAO,MAAMG,kBAAkB,SAASlW,eAAe,CAAC;EACpD;AACJ;AACA;AACA;AACA;AACA;EACIC,WAAWA,CAAC9E,MAAM,EAAE3C,OAAO,EAAEqF,iBAAiB,EAAE;IAC5C,KAAK,CAAC1C,MAAM,EAAE3C,OAAO,CAAC;IACtB,IAAI,CAACqF,iBAAiB,GAAGA,iBAAiB;;IAE1C;IACA,IAAI,CAAC1C,MAAM,CAACD,YAAY,GAAG,IAAI,CAACC,MAAM,CAACC,YAAY;IAEnD,IAAI,CAACwL,SAAS,GAAG,IAAI,CAACzL,MAAM,CAACgb,OAAO;IACpC,IAAI,CAACrP,UAAU,GAAG,IAAI,CAAC3L,MAAM,CAACib,QAAQ;IACtC,IAAI,CAACvP,MAAM,GAAG,IAAI,CAAC1L,MAAM,CAACmS,OAAO,GAAG,IAAI,CAAC1G,SAAS;EACtD;AACJ;;AAEA;AACA;AACA;AACA,OAAO,MAAMyP,QAAQ,SAASH,kBAAkB,CAAC;;AAEjD;AACA;AACA;AACA,OAAO,MAAMI,cAAc,SAASJ,kBAAkB,CAAC;AACvD;;AAGA;AACA;AACA,OAAO,MAAMK,kBAAkB,SAASvW,eAAe,CAAC;EACpD;AACJ;AACA;AACA;AACA;AACA;EACIC,WAAWA,CAAC9E,MAAM,EAAE3C,OAAO,EAAEqF,iBAAiB,EAAE;IAC5C,KAAK,CAAC1C,MAAM,EAAE3C,OAAO,CAAC;IACtB,IAAI,CAACqF,iBAAiB,GAAGA,iBAAiB;;IAE1C;IACA,IAAI,CAAC1C,MAAM,CAACD,YAAY,GAAG,IAAI,CAACC,MAAM,CAACC,YAAY;IAEnD,IAAI,CAACwL,SAAS,GAAG,IAAI,CAACzL,MAAM,CAACqZ,mBAAmB;IAChD,IAAI,CAAC1N,UAAU,GAAG,IAAI,CAAC3L,MAAM,CAACsZ,iBAAiB;IAC/C,IAAI,CAAC5N,MAAM,GAAG,IAAI,CAAC1L,MAAM,CAACiZ,WAAW,GAAG,IAAI,CAACxN,SAAS;EAC1D;AACJ;;AAEA;AACA;AACA;AACA,OAAO,MAAM4P,QAAQ,SAASD,kBAAkB,CAAC;;AAEjD;AACA;AACA;AACA,OAAO,MAAME,cAAc,SAASF,kBAAkB,CAAC;AACvD;;AAEA;AACA,OAAO,MAAMG,kBAAkB,SAAS1W,eAAe,CAAC;AACxD,OAAO,MAAM2W,QAAQ,SAASD,kBAAkB,CAAC;AACjD,OAAO,MAAME,yBAAyB,SAASF,kBAAkB,CAAC;EAC9D;AACJ;AACA;EACI,MAAMlV,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIkL,wBAAwB,CAAC,MAAM,KAAK,CAACnG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EACxE;AACJ;AACA;;AAGA;AACA,OAAO,MAAMoa,sBAAsB,SAAS7W,eAAe,CAAC;AAC5D,OAAO,MAAM8W,YAAY,SAASD,sBAAsB,CAAC;AACzD,OAAO,MAAME,6BAA6B,SAASF,sBAAsB,CAAC;EACtE;AACJ;AACA;EACI,MAAMrV,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIkL,wBAAwB,CAAC,MAAM,KAAK,CAACnG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EACxE;AACJ;AACA;;AAEA;AACA,OAAO,MAAMua,uBAAuB,SAAShX,eAAe,CAAC;;AAE7D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMiX,uBAAuB,SAASD,uBAAuB,CAAC;EACjE;AACJ;AACA;EACI,MAAMxV,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIya,kBAAkB,CAAC,MAAM,KAAK,CAAC1V,KAAK,CAAC/E,YAAY,CAAC,CAAC;EAClE;AACJ;AACA;;AAEA;AACA,OAAO,MAAM0a,wBAAwB,SAASnX,eAAe,CAAC;AAC9D,OAAO,MAAMoX,cAAc,SAASD,wBAAwB,CAAC;AAC7D,OAAO,MAAME,+BAA+B,SAASF,wBAAwB,CAAC;EAC1E;AACJ;AACA;EACI,MAAM3V,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIkL,wBAAwB,CAAC,MAAM,KAAK,CAACnG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EACxE;AACJ;AACA;;AAEA;;AAEA;AACA,OAAO,MAAM6a,0BAA0B,SAAStX,eAAe,CAAC;AAChE,OAAO,MAAMuX,gBAAgB,SAASD,0BAA0B,CAAC;AACjE,OAAO,MAAME,iCAAiC,SAASF,0BAA0B,CAAC;EAC9E;AACJ;AACA;EACI,MAAM9V,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIkL,wBAAwB,CAAC,MAAM,KAAK,CAACnG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EACxE;AACJ;AACA;;AAEA;;AAEA;AACA,OAAO,MAAMgb,qBAAqB,SAASzX,eAAe,CAAC;AAC3D,OAAO,MAAM0X,WAAW,SAASD,qBAAqB,CAAC;AACvD,OAAO,MAAME,wBAAwB,SAASF,qBAAqB,CAAC;AACpE;;AAEA;AACA,OAAO,MAAMG,oBAAoB,SAAS5X,eAAe,CAAC;AAC1D,OAAO,MAAM6X,UAAU,SAASD,oBAAoB,CAAC;AACrD,OAAO,MAAME,uBAAuB,SAASF,oBAAoB,CAAC;AAClE;;AAEA;AACA;AACA,OAAO,MAAMG,mBAAmB,SAAS/X,eAAe,CAAC;AACzD,OAAO,MAAMgY,SAAS,SAASD,mBAAmB,CAAC;AACnD,OAAO,MAAME,0BAA0B,SAASF,mBAAmB,CAAC;EAChE;AACJ;AACA;EACI,MAAMvW,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIkL,wBAAwB,CAAC,MAAM,KAAK,CAACnG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EACxE;AACJ;AACA;;AAGA;AACA,OAAO,MAAMyb,mBAAmB,SAASlY,eAAe,CAAC;AACzD,OAAO,MAAMmY,SAAS,SAASD,mBAAmB,CAAC;AACnD,OAAO,MAAME,sBAAsB,SAASF,mBAAmB,CAAC;EAC5D;AACJ;AACA;EACI,MAAM1W,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAI4b,yBAAyB,CAAC,MAAM,KAAK,CAAC7W,KAAK,CAAC/E,YAAY,CAAC,CAAC;EACzE;AACJ;AAEA,OAAO,MAAM6b,mBAAmB,SAASJ,mBAAmB,CAAC;EACzD;AACJ;AACA;AACA;AACA;EACI,MAAM1W,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAI8b,sBAAsB,CAAC,MAAM,KAAK,CAAC/W,KAAK,CAAC/E,YAAY,CAAC,CAAC;EACtE;AACJ;AAEA,OAAO,MAAM4b,yBAAyB,SAASnR,WAAW,CAAC;EACvD;AACJ;AACA;AACA;AACA;AACA;EACIjH,WAAWA,CAAC;IAAE3C,MAAM;IAAEkb;EAAW,CAAC,EAAE;IAChC,KAAK,CAAC,CAAC;IACP,IAAI,CAAClb,MAAM,GAAGA,MAAM;IACpB,IAAI,CAACkb,UAAU,GAAGA,UAAU;EAChC;AACJ;AAEA,OAAO,MAAMD,sBAAsB,SAASrR,WAAW,CAAC;EACpD;AACJ;AACA;AACA;AACA;AACA;EACIjH,WAAWA,CAAC;IAAE3C,MAAM;IAAEkb,UAAU;IAAEC;EAAW,CAAC,EAAE;IAC5C,KAAK,CAAC,CAAC;IACP,IAAI,CAACnb,MAAM,GAAGA,MAAM;IACpB,IAAI,CAACkb,UAAU,GAAGA,UAAU;IAC5B,IAAI,CAACC,UAAU,GAAGA,UAAU;EAChC;AACJ;AACA;;AAEA;AACA,OAAO,MAAMC,+BAA+B,SAAS1Y,eAAe,CAAC;;AAErE;AACA;AACA;AACA;AACA,OAAO,MAAM2Y,qBAAqB,SAASD,+BAA+B,CAAC;;AAE3E;AACA;AACA;AACA;AACA,OAAO,MAAME,kCAAkC,SAASF,+BAA+B,CAAC;EACpF;AACJ;AACA;EACI,MAAMlX,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIoc,qCAAqC,CAAC,MAAM,KAAK,CAACrX,KAAK,CAAC/E,YAAY,CAAC,CAAC;EACrF;AACJ;AACA,OAAO,MAAMoc,qCAAqC,SAASR,yBAAyB,CAAC;AACrF;;AAGA;AACA,OAAO,MAAMS,mBAAmB,SAAS9Y,eAAe,CAAC;AACzD,OAAO,MAAM+Y,SAAS,SAASD,mBAAmB,CAAC;AACnD,OAAO,MAAME,0BAA0B,SAASF,mBAAmB,CAAC;EAChE;AACJ;AACA;EACI,MAAMtX,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIkL,wBAAwB,CAAC,MAAM,KAAK,CAACnG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EACxE;AACJ;AACA;;AAGA;AACA;AACA;AACA;AACA,OAAO,MAAMwc,qBAAqB,SAASjZ,eAAe,CAAC;;AAE3D;AACA;AACA;AACA,OAAO,MAAMkZ,WAAW,SAASD,qBAAqB,CAAC;;AAEvD;AACA;AACA;AACA,OAAO,MAAME,4BAA4B,SAASF,qBAAqB,CAAC;EACpE;AACJ;AACA;EACI,MAAMzX,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIkL,wBAAwB,CAAC,MAAM,KAAK,CAACnG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EACxE;AACJ;AACA;;AAGA;AACA,OAAO,MAAM2c,mBAAmB,SAASpZ,eAAe,CAAC;AACzD,OAAO,MAAMqZ,SAAS,SAASD,mBAAmB,CAAC;AACnD,OAAO,MAAME,0BAA0B,SAASF,mBAAmB,CAAC;EAChE;AACJ;AACA;EACI,MAAM5X,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIkL,wBAAwB,CAAC,MAAM,KAAK,CAACnG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EACxE;AACJ;AACA;;AAEA;AACA,OAAO,MAAM8c,sBAAsB,SAASvZ,eAAe,CAAC;;AAE5D;AACA;AACA;AACA,OAAO,MAAMwZ,YAAY,SAASD,sBAAsB,CAAC;;AAEzD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAME,8BAA8B,SAASF,sBAAsB,CAAC;AAC3E;;AAEA;AACA,OAAO,MAAMG,kBAAkB,SAAS1Z,eAAe,CAAC;;AAExD;AACA;AACA;AACA,OAAO,MAAM2Z,QAAQ,SAASD,kBAAkB,CAAC;;AAEjD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAME,qBAAqB,SAASF,kBAAkB,CAAC;AAC9D;;AAEA;AACA,OAAO,MAAMG,4BAA4B,SAAS7Z,eAAe,CAAC;;AAElE;AACA;AACA;AACA,OAAO,MAAM8Z,+BAA+B,SAASD,4BAA4B,CAAC;AAClF;;AAGA;AACA,OAAO,MAAME,mBAAmB,SAAS/Z,eAAe,CAAC;;AAEzD;AACA;AACA;AACA,OAAO,MAAMga,SAAS,SAASD,mBAAmB,CAAC;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAME,sBAAsB,SAASF,mBAAmB,CAAC;AAChE;;AAEA;AACA,OAAO,MAAMG,wBAAwB,SAASla,eAAe,CAAC;;AAE9D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMma,cAAc,SAASD,wBAAwB,CAAC;AAC7D;;AAGA;AACA,OAAO,MAAME,uBAAuB,SAASpa,eAAe,CAAC;;AAE7D;AACA;AACA;AACA,OAAO,MAAMqa,aAAa,SAASD,uBAAuB,CAAC;;AAE3D;AACA;AACA;AACA,OAAO,MAAME,8BAA8B,SAASF,uBAAuB,CAAC;EACxE;AACJ;AACA;EACI,MAAM5Y,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIkL,wBAAwB,CAAC,MAAM,KAAK,CAACnG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EACxE;AACJ;AACA;;AAGA;AACA,OAAO,MAAM8d,yBAAyB,SAASva,eAAe,CAAC;;AAE/D;AACA;AACA;AACA,OAAO,MAAMwa,eAAe,SAASD,yBAAyB,CAAC;;AAE/D;AACA;AACA;AACA,OAAO,MAAME,gCAAgC,SAASF,yBAAyB,CAAC;EAC5E;AACJ;AACA;EACI,MAAM/Y,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIkL,wBAAwB,CAAC,MAAM,KAAK,CAACnG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EACxE;AACJ;AACA;;AAEA;AACA,OAAO,MAAMie,qBAAqB,SAAS1a,eAAe,CAAC;;AAE3D;AACA;AACA;AACA,OAAO,MAAM2a,WAAW,SAASD,qBAAqB,CAAC;;AAEvD;AACA;AACA;AACA,OAAO,MAAME,4BAA4B,SAASF,qBAAqB,CAAC;EACpE;AACJ;AACA;EACI,MAAMlZ,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIkL,wBAAwB,CAAC,MAAM,KAAK,CAACnG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EACxE;AACJ;AACA;;AAGA;AACA,OAAO,MAAMoe,oBAAoB,SAAS7a,eAAe,CAAC;AAC1D,OAAO,MAAM8a,UAAU,SAASD,oBAAoB,CAAC;AACrD,OAAO,MAAME,uBAAuB,SAASF,oBAAoB,CAAC;EAC9D;AACJ;AACA;EACI,MAAMrZ,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIue,0BAA0B,CAAC,MAAM,KAAK,CAACxZ,KAAK,CAAC/E,YAAY,CAAC,CAAC;EAC1E;AACJ;AAEA,OAAO,MAAMue,0BAA0B,SAAS9T,WAAW,CAAC;EACxD;AACJ;AACA;AACA;AACA;AACA;EACIjH,WAAWA,CAAC;IAAE3C,MAAM;IAAEkb;EAAW,CAAC,EAAE;IAChC,KAAK,CAAC,CAAC;IACP,IAAI,CAAClb,MAAM,GAAGA,MAAM;IACpB,IAAI,CAACkb,UAAU,GAAGA,UAAU;EAChC;AACJ;AACA;;AAGA;AACA,OAAO,MAAMyC,kBAAkB,SAASjb,eAAe,CAAC;;AAExD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMkb,QAAQ,SAASD,kBAAkB,CAAC;EAC7C;AACJ;AACA;AACA;AACA;AACA;EACIhb,WAAWA,CAAC9E,MAAM,EAAEggB,cAAc,EAAEC,2BAA2B,EAAE;IAC7D,KAAK,CAACjgB,MAAM,EAAEggB,cAAc,CAAC;IAC7B,IAAI,CAACC,2BAA2B,GAAGA,2BAA2B;EAClE;;EAEA;AACJ;AACA;AACA;AACA;AACA;EACI,MAAMC,oBAAoBA,CAAC;IAAEC;EAAa,CAAC,EAAE;IACzC;IACA;IACA;IACA;IACA;IACA;IACA,OAAO,MAAM1e,cAAc,CAAC,IAAI,EAAE;MAAE0e;IAAa,CAAC,CAAC;EACvD;;EAEA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;EAEI;AACJ;AACA;AACA;EACI,MAAMxc,OAAOA,CAACrC,YAAY,EAAE;IACxB,IAAI,CAACA,YAAY,CAAC8e,gBAAgB,IAAI,CAAC9e,YAAY,CAAC+e,2BAA2B,EAAE;MAC7E;MACA/e,YAAY,GAAG;QACX,GAAGA,YAAY;QACf,IAAI,MAAM,IAAI,CAAC4e,oBAAoB,CAAC5e,YAAY,CAAC;MACrD,CAAC;IACL;IAEA,IAAI,CAACA,YAAY,CAACgf,YAAY,EAAE;MAC5B;MACA,MAAMC,KAAK,GAAGjf,YAAY,CAACkf,YAAY,CAAClgB,IAAI,CAACW,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;MACzD,MAAMwf,WAAW,GAAGF,KAAK,CAACG,MAAM,CAAC,CAAClX,CAAC,EAAEC,CAAC,KAAKD,CAAC,GAAGC,CAAC,EAAE,CAAC,CAAC;MACpDnI,YAAY,CAACgf,YAAY,GAAG,IAAIhlB,MAAM,CAClC,OAAO,EACP,IAAIiE,aAAa,CAACkhB,WAAW,CAAC,CAAC9b,IAAI,CAAC,EAAE,CAAC,EACvC4b,KACJ,CAAC;IACL;;IAEA;IACA;IACA;IACA,OAAO,MAAM9hB,UAAU,CAAC,IAAI,CAACwhB,2BAA2B,EAAE;MACtDO,YAAY,EAAElf,YAAY,CAACkf,YAAY;MACvCF,YAAY,EAAEhf,YAAY,CAACgf,YAAY;MACvCF,gBAAgB,EAAE9e,YAAY,CAAC8e,gBAAgB;MAC/CC,2BAA2B,EAAE/e,YAAY,CAAC+e;IAC9C,CAAC,CAAC;EACN;;EAEA;AACJ;AACA;AACA;AACA;EACI,MAAMha,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIqf,0BAA0B,CAAC,MAAM,KAAK,CAACta,KAAK,CAAC/E,YAAY,CAAC,CAAC;EAC1E;AACJ;;AAGA;AACA;AACA;AACA,OAAO,MAAMqf,0BAA0B,SAAS5U,WAAW,CAAC;EACxD;AACJ;AACA;AACA;AACA;EACIjH,WAAWA,CAAC;IAAE8b,UAAU;IAAEtD;EAAW,CAAC,EAAE;IACpC,KAAK,CAAC,CAAC;IACP,IAAI,CAACsD,UAAU,GAAGA,UAAU;IAC5B,IAAI,CAACtD,UAAU,GAAGA,UAAU;EAChC;AACJ;AACA;;AAGA;AACA;AACA,OAAO,MAAMuD,qBAAqB,SAAShc,eAAe,CAAC;AAAG;AAE9D,OAAO,MAAMic,WAAW,SAASD,qBAAqB,CAAC;AAEvD,OAAO,MAAME,aAAa,SAASF,qBAAqB,CAAC;EAErD;AACJ;AACA;AACA;AACA;AACA;AACA;EACI/b,WAAWA,CAAC9E,MAAM,EAAE3C,OAAO,EAAE0E,sBAAsB,EAAEW,iBAAiB,EAAE;IACpE,KAAK,CAAC1C,MAAM,EAAE3C,OAAO,CAAC;IACtB,IAAI,CAAC0E,sBAAsB,GAAGA,sBAAsB;IACpD,IAAI,CAACW,iBAAiB,GAAGA,iBAAiB;IAE1C,IAAI,CAAC8I,kBAAkB,GAAG,IAAI,CAACxL,MAAM,CAACiS,cAAc;IACpD,IAAI,CAAC3G,iBAAiB,GAAG,IAAI,CAACtL,MAAM,CAACkS,uBAAuB;IAC5D,IAAI,CAAC3G,cAAc,GAAG,IAAI,CAACvL,MAAM,CAACmS,OAAO,GAAG,IAAI,CAAC7G,iBAAiB;IAElE,IAAI,CAACiG,kBAAkB,GAAG,IAAI,CAACvR,MAAM,CAACoS,cAAc;IACpD,IAAI,CAACjH,iBAAiB,GAAG,IAAI,CAACnL,MAAM,CAACqS,uBAAuB;IAC5D,IAAI,CAACjH,cAAc,GAAG,IAAI,CAACpL,MAAM,CAACmS,OAAO,GAAG,IAAI,CAAChH,iBAAiB;EACtE;AACJ;AACA;;AAEA;AACA;AACA,OAAO,MAAM6V,qBAAqB,SAASnc,eAAe,CAAC;AAAG;AAE9D,OAAO,MAAMoc,WAAW,SAASD,qBAAqB,CAAC;AAEvD,OAAO,MAAME,8BAA8B,SAASF,qBAAqB,CAAC;EAEtE;AACJ;AACA;AACA;AACA;AACA;AACA;EACIlc,WAAWA,CAAC9E,MAAM,EAAE3C,OAAO,EAAE0E,sBAAsB,EAAEW,iBAAiB,EAAE;IACpE,KAAK,CAAC1C,MAAM,EAAE3C,OAAO,CAAC;IACtB,IAAI,CAAC0E,sBAAsB,GAAGA,sBAAsB;IACpD,IAAI,CAACW,iBAAiB,GAAGA,iBAAiB;IAE1C,IAAI,CAAC8I,kBAAkB,GAAG,IAAI,CAACxL,MAAM,CAACiS,cAAc;IACpD,IAAI,CAAC3G,iBAAiB,GAAG,IAAI,CAACtL,MAAM,CAACkS,uBAAuB;IAC5D,IAAI,CAAC3G,cAAc,GAAG,IAAI,CAACvL,MAAM,CAACmS,OAAO,GAAG,IAAI,CAAC7G,iBAAiB;IAElE,IAAI,CAACiG,kBAAkB,GAAG,IAAI,CAACvR,MAAM,CAACoS,cAAc;IACpD,IAAI,CAACjH,iBAAiB,GAAG,IAAI,CAACnL,MAAM,CAACqS,uBAAuB;IAC5D,IAAI,CAACjH,cAAc,GAAG,IAAI,CAACpL,MAAM,CAACmS,OAAO,GAAG,IAAI,CAAChH,iBAAiB;EACtE;AAEJ;AACA;;AAEA;AACA;AACA,OAAO,MAAMgW,uBAAuB,SAAStc,eAAe,CAAC;AAAG;;AAEhE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMuc,aAAa,SAASD,uBAAuB,CAAC;AAE3D,OAAO,MAAME,cAAc,SAASF,uBAAuB,CAAC;EACxD;AACJ;AACA;AACA;AACA;EACI,MAAM9a,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIggB,cAAc,CAAC,MAAM,KAAK,CAACjb,KAAK,CAAC/E,YAAY,CAAC,CAAC;EAC9D;AACJ;AAEA,OAAO,MAAMigB,iCAAiC,SAASJ,uBAAuB,CAAC;EAC3E;AACJ;AACA;AACA;AACA;EACI,MAAM9a,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIkL,wBAAwB,CAAC,MAAM,KAAK,CAACnG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EACxE;AACJ;;AAEA;AACA;AACA;AACA,OAAO,MAAMkgB,mCAAmC,SAASL,uBAAuB,CAAC;EAC7E;AACJ;AACA;AACA;AACA;EACI,MAAM9a,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIoL,qBAAqB,CAAC,MAAM,KAAK,CAACrG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EACrE;AACJ;AACA;;AAEA;AACA;AACA,OAAO,MAAMmgB,wBAAwB,SAAS5c,eAAe,CAAC;AAAG;;AAEjE;AACA;AACA;AACA,OAAO,MAAM6c,cAAc,SAASD,wBAAwB,CAAC;;AAE7D;AACA;AACA;AACA,OAAO,MAAME,eAAe,SAASF,wBAAwB,CAAC;EAC1D;AACJ;AACA;AACA;AACA;EACI,MAAMpb,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIggB,cAAc,CAAC,MAAM,KAAK,CAACjb,KAAK,CAAC/E,YAAY,CAAC,CAAC;EAC9D;AACJ;;AAEA;AACA;AACA;AACA,OAAO,MAAMsgB,kCAAkC,SAASH,wBAAwB,CAAC;EAC7E;AACJ;AACA;AACA;AACA;EACI,MAAMpb,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIkL,wBAAwB,CAAC,MAAM,KAAK,CAACnG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EACxE;AACJ;AACA;;AAEA;AACA;AACA,OAAO,MAAMugB,2BAA2B,SAAShd,eAAe,CAAC;AAAG;;AAEpE;AACA;AACA;AACA,OAAO,MAAMid,iBAAiB,SAASD,2BAA2B,CAAC;;AAEnE;AACA;AACA;AACA,OAAO,MAAME,kBAAkB,SAASF,2BAA2B,CAAC;EAChE;AACJ;AACA;AACA;AACA;EACI,MAAMxb,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIggB,cAAc,CAAC,MAAM,KAAK,CAACjb,KAAK,CAAC/E,YAAY,CAAC,CAAC;EAC9D;AACJ;;AAEA;AACA;AACA;AACA,OAAO,MAAM0gB,qCAAqC,SAASH,2BAA2B,CAAC;EACnF;AACJ;AACA;AACA;AACA;EACI,MAAMxb,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIkL,wBAAwB,CAAC,MAAM,KAAK,CAACnG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EACxE;AACJ;;AAEA;AACA;AACA;AACA,OAAO,MAAM2gB,uCAAuC,SAASJ,2BAA2B,CAAC;EACrF;AACJ;AACA;AACA;AACA;EACI,MAAMxb,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIoL,qBAAqB,CAAC,MAAM,KAAK,CAACrG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EACrE;AACJ;AACA;;AAEA;AACA;AACA,OAAO,MAAM4gB,2BAA2B,SAASrd,eAAe,CAAC;AAAG;;AAEpE;AACA;AACA;AACA,OAAO,MAAMsd,iBAAiB,SAASD,2BAA2B,CAAC;;AAEnE;AACA;AACA;AACA,OAAO,MAAME,kBAAkB,SAASF,2BAA2B,CAAC;EAChE;AACJ;AACA;AACA;AACA;EACI,MAAM7b,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIggB,cAAc,CAAC,MAAM,KAAK,CAACjb,KAAK,CAAC/E,YAAY,CAAC,CAAC;EAC9D;AACJ;;AAEA;AACA;AACA;AACA,OAAO,MAAM+gB,qCAAqC,SAASH,2BAA2B,CAAC;EACnF;AACJ;AACA;AACA;AACA;EACI,MAAM7b,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIkL,wBAAwB,CAAC,MAAM,KAAK,CAACnG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EACxE;AACJ;AACA;;AAEA;AACA;AACA,OAAO,MAAMghB,qBAAqB,SAASzd,eAAe,CAAC;;AAE3D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAM0d,WAAW,SAASpB,uBAAuB,CAAC;;AAEzD;AACA;AACA;AACA,OAAO,MAAMqB,YAAY,SAASrB,uBAAuB,CAAC;EACtD;AACJ;AACA;AACA;AACA;EACI,MAAM9a,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIggB,cAAc,CAAC,MAAM,KAAK,CAACjb,KAAK,CAAC/E,YAAY,CAAC,CAAC;EAC9D;AACJ;;AAEA;AACA;AACA;AACA,OAAO,MAAMmhB,+BAA+B,SAAStB,uBAAuB,CAAC;EACzE;AACJ;AACA;AACA;AACA;EACI,MAAM9a,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIkL,wBAAwB,CAAC,MAAM,KAAK,CAACnG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EACxE;AACJ;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMohB,oBAAoB,SAAS7d,eAAe,CAAC;AAAG;;AAE7D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAM8d,UAAU,SAASD,oBAAoB,CAAC;;AAErD;AACA;AACA;AACA,OAAO,MAAME,WAAW,SAASF,oBAAoB,CAAC;EAClD;AACJ;AACA;AACA;AACA;EACI,MAAMrc,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIggB,cAAc,CAAC,MAAM,KAAK,CAACjb,KAAK,CAAC/E,YAAY,CAAC,CAAC;EAC9D;AACJ;;AAEA;AACA;AACA;AACA,OAAO,MAAMuhB,8BAA8B,SAASH,oBAAoB,CAAC;EACrE;AACJ;AACA;AACA;AACA;EACI,MAAMrc,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIkL,wBAAwB,CAAC,MAAM,KAAK,CAACnG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EACxE;AACJ;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMwhB,eAAe,SAASJ,oBAAoB,CAAC;EACtD;AACJ;AACA;AACA;AACA;EACI,MAAMrc,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIyhB,aAAa,CAAC,MAAM,KAAK,CAAC1c,KAAK,CAAC/E,YAAY,CAAC,CAAC;EAC7D;AACJ;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAM0hB,gCAAgC,SAASN,oBAAoB,CAAC;EACvE;AACJ;AACA;AACA;AACA;EACI,MAAMrc,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIoL,qBAAqB,CAAC,MAAM,KAAK,CAACrG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EACrE;AACJ;;AAEA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAM2hB,uBAAuB,SAASpe,eAAe,CAAC;AAAG;;AAEhE;AACA;AACA;AACA,OAAO,MAAMqe,aAAa,SAASD,uBAAuB,CAAC;AAAG;;AAE9D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAME,uBAAuB,SAASF,uBAAuB,CAAC;;AAErE;AACA;AACA;AACA,OAAO,MAAMG,uBAAuB,SAASH,uBAAuB,CAAC;EAEjE;AACJ;AACA;AACA;AACA;AACA;AACA;EACIne,WAAWA,CAAC9E,MAAM,EAAE3C,OAAO,EAAE0E,sBAAsB,EAAEW,iBAAiB,EAAE;IACpE,KAAK,CAAC1C,MAAM,EAAE3C,OAAO,CAAC;IACtB,IAAI,CAAC0E,sBAAsB,GAAGA,sBAAsB;IACpD,IAAI,CAACW,iBAAiB,GAAGA,iBAAiB;IAE1C,IAAI,CAAC8I,kBAAkB,GAAG,IAAI,CAACxL,MAAM,CAACiS,cAAc;IACpD,IAAI,CAAC3G,iBAAiB,GAAG,IAAI,CAACtL,MAAM,CAACkS,uBAAuB;IAC5D,IAAI,CAAC3G,cAAc,GAAG,IAAI,CAACvL,MAAM,CAACiZ,WAAW,GAAG,IAAI,CAAC3N,iBAAiB;IAEtE,IAAI,CAACiG,kBAAkB,GAAG,IAAI,CAACvR,MAAM,CAACoS,cAAc;IACpD,IAAI,CAACjH,iBAAiB,GAAG,IAAI,CAACnL,MAAM,CAACqS,uBAAuB;IAC5D,IAAI,CAACjH,cAAc,GAAG,IAAI,CAACpL,MAAM,CAACiZ,WAAW,GAAG,IAAI,CAAC9N,iBAAiB;EAC1E;;EAEA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;;EAEI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACI,MAAMkY,eAAeA,CAACC,YAAY,EAAEC,kBAAkB,EAAE;IACpDC,SAAS,GAAG,GAAG;IACfC,WAAW,GAAG,GAAG;IACjBC,WAAW,GAAG,IAAI;IAClBC,OAAO,GAAG;IACV;EACJ,CAAC,GAAG,CAAC,CAAC,EAAE;IAEJ,MAAMriB,YAAY,GAAG;MACjBM,SAAS,EAAE0hB;IACf,CAAC;IAED,MAAM;MAAE/hB,eAAe;MAAES;IAAuB,CAAC,GAAG,MAAMP,cAAc,CAAC,IAAI,EAAEH,YAAY,CAAC;IAE5F,MAAMsiB,CAAC,GAAGriB,eAAe,CAACjB,IAAI,CAAC,CAAC,CAAC,GAAG,IAAI,CAACN,MAAM,CAAC6jB,gBAAgB;IAChE,MAAMC,MAAM,GAAGC,IAAI,CAACC,KAAK,CAACJ,CAAC,GAAGF,WAAW,CAAC;IAC1C,MAAMO,MAAM,GAAGF,IAAI,CAACC,KAAK,CAACJ,CAAC,GAAGH,WAAW,CAAC;IAE1C,MAAMS,YAAY,GAAG,IAAI,CAAClkB,MAAM,CAACkkB,YAAY;IAE7C,IAAIC,gBAAgB,GAAG,EAAE;IACzB,IAAI3iB,eAAe,GAAG,IAAI;IAC1B,IAAI4iB,eAAe,GAAG,IAAI;IAC1B,IAAIC,GAAG,GAAG,CAAC;IAEX,OAAO,IAAI,EAAE;MACT,EAAEA,GAAG;MAEL,MAAM5jB,gBAAgB,GAAGU,UAAU,CAAC,CAAC,CAACijB,eAAe,CAAC;MACtD,IAAIE,eAAe;MACnB,IAAIF,eAAe,EAAE;QACjBE,eAAe,GAAGF,eAAe,CAACG,mBAAmB;MACzD,CAAC,MAAM;QACHD,eAAe,GAAG,IAAIhpB,MAAM,CACxB,SAAS,EACT,IAAIkb,YAAY,CAAC0N,YAAY,CAAC,EAC9B,CAAC,CAAC,EAAE,CAAC,EAAEA,YAAY,CACvB,CAAC;MACL;MACA,IAAIviB,YAAY,GAAG;QACflB,gBAAgB;QAChB6jB,eAAe;QACftiB,sBAAsB,EAAEA,sBAAsB;QAC9CuhB,kBAAkB,EAAEA,kBAAkB;QACtCzhB,qBAAqB,EAAEP;MAC3B,CAAC;MAED,IAAI,CAACU,gBAAgB,CAACN,YAAY,EAAEH,eAAe,CAAC;MACpD4iB,eAAe,GAAG,MAAM3lB,UAAU,CAAC,IAAI,CAACsD,sBAAsB,EAAEJ,YAAY,CAAC;MAC7EH,eAAe,GAAG,IAAI,CAACY,gBAAgB,CAACgiB,eAAe,EAAE5iB,eAAe,CAAC;MAEzE,MAAM;QAAEgjB,IAAI;QAAEC;MAAS,CAAC,GAAGL,eAAe;MAC1CD,gBAAgB,CAACtmB,IAAI,CAAC4mB,QAAQ,CAAC;MAE/B,IAAIJ,GAAG,IAAIJ,MAAM;MACb;MACA9kB,KAAK,CAACK,IAAI,CAACglB,IAAI,CAACnkB,IAAI,CAAC,CAAC9B,MAAM,CAACmmB,CAAC,IAAIA,CAAC,IAAIlB,SAAS,CAAC,CAACvmB,MAAM,GAAG,CAAC,IAAIonB,GAAG,IAAIP,MAAM,CAChF,EAAE;QACC;MACJ;IACJ;IAEA,MAAMa,WAAW,GAAG3pB,GAAG,CAACmpB,gBAAgB,CAAC;IACzC,MAAM;MAAES;IAAS,CAAC,GAAG,MAAMnmB,UAAU,CAACklB,OAAO,CAACtmB,OAAO,EAAE;MAAEsnB;IAAY,CAAC,CAAC;IAEvE,OAAO;MACHA,WAAW;MACXC;MACA;IACJ,CAAC;EACL;AACJ;;AAEA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMC,eAAe,SAAShgB,eAAe,CAAC;EACjDnB,eAAe,GAAG,aAAa;AACnC;AACA;;AAGA;AACA;AACA,OAAO,MAAMohB,oBAAoB,SAASjgB,eAAe,CAAC;EACtD;AACJ;AACA;AACA;AACA;AACA;EACIC,WAAWA,CAAC9E,MAAM,EAAE3C,OAAO,EAAEqF,iBAAiB,EAAE;IAC5C,KAAK,CAAC1C,MAAM,EAAE3C,OAAO,CAAC;IACtB,IAAI,CAACqF,iBAAiB,GAAGA,iBAAiB;;IAE1C;IACA,IAAI,CAAC1C,MAAM,CAACD,YAAY,GAAG,IAAI,CAACC,MAAM,CAACC,YAAY;IAEnD,IAAI,CAACsR,kBAAkB,GAAG,IAAI,CAAC/F,kBAAkB,GAAG,IAAI,CAACxL,MAAM,CAACiS,cAAc;IAC9E,IAAI,CAAC9G,iBAAiB,GAAG,IAAI,CAACG,iBAAiB,GAAG,IAAI,CAACtL,MAAM,CAACkS,uBAAuB;IACrF,IAAI,CAAC9G,cAAc,GAAG,IAAI,CAACG,cAAc,GAAG,IAAI,CAACvL,MAAM,CAACmS,OAAO,GAAG,IAAI,CAAC7G,iBAAiB;EAC5F;AACJ;;AAEA;AACA;AACA;AACA,OAAO,MAAMyZ,gBAAgB,SAASD,oBAAoB,CAAC;;AAE3D;;AAGA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAME,sBAAsB,SAASngB,eAAe,CAAC;EACxD;AACJ;AACA;AACA;AACA;AACA;EACIC,WAAWA,CAAC9E,MAAM,EAAE3C,OAAO,EAAEqF,iBAAiB,EAAE;IAC5C,KAAK,CAAC1C,MAAM,EAAE3C,OAAO,CAAC;IACtB,IAAI,CAACqF,iBAAiB,GAAGA,iBAAiB;;IAE1C;IACA,IAAI,CAAC1C,MAAM,CAACD,YAAY,GAAG,IAAI,CAACC,MAAM,CAACC,YAAY;IAEnD,IAAI,CAACwL,SAAS,GAAG,IAAI,CAACzL,MAAM,CAACma,mBAAmB;IAChD,IAAI,CAACxO,UAAU,GAAG,IAAI,CAAC3L,MAAM,CAACsZ,iBAAiB;IAC/C,IAAI,CAAC5N,MAAM,GAAG,IAAI,CAAC1L,MAAM,CAACiZ,WAAW,GAAG,IAAI,CAACjZ,MAAM,CAACqZ,mBAAmB;EAC3E;AACJ;AAEA,OAAO,MAAM4L,YAAY,SAASD,sBAAsB,CAAC;AAEzD,OAAO,MAAME,kBAAkB,SAASF,sBAAsB,CAAC;AAC/D;;AAGA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMG,yBAAyB,SAAStgB,eAAe,CAAC;EAC3D;AACJ;AACA;AACA;AACA;AACA;EACIC,WAAWA,CAAC9E,MAAM,EAAE3C,OAAO,EAAEqF,iBAAiB,EAAE;IAC5C,KAAK,CAAC1C,MAAM,EAAE3C,OAAO,CAAC;IACtB,IAAI,CAACqF,iBAAiB,GAAGA,iBAAiB;;IAE1C;IACA,IAAI,CAAC1C,MAAM,CAACD,YAAY,GAAG,IAAI,CAACC,MAAM,CAACC,YAAY;IAEnD,IAAI,CAACwL,SAAS,GAAG,IAAI,CAACzL,MAAM,CAACma,mBAAmB;IAChD,IAAI,CAACxO,UAAU,GAAG,IAAI,CAAC3L,MAAM,CAACsZ,iBAAiB;IAC/C,IAAI,CAAC5N,MAAM,GAAG,IAAI,CAAC1L,MAAM,CAACiZ,WAAW,GAAG,IAAI,CAACjZ,MAAM,CAACqZ,mBAAmB;EAC3E;AACJ;AAEA,OAAO,MAAM+L,eAAe,SAASD,yBAAyB,CAAC;AAE/D,OAAO,MAAME,qBAAqB,SAASF,yBAAyB,CAAC;AACrE;;AAGA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMG,qBAAqB,SAASzgB,eAAe,CAAC;EACvD;AACJ;AACA;AACA;AACA;AACA;EACIC,WAAWA,CAAC9E,MAAM,EAAE3C,OAAO,EAAEqF,iBAAiB,EAAE;IAC5C,KAAK,CAAC1C,MAAM,EAAE3C,OAAO,CAAC;IACtB,IAAI,CAACqF,iBAAiB,GAAGA,iBAAiB;;IAE1C;IACA,IAAI,CAAC1C,MAAM,CAACD,YAAY,GAAG,IAAI,CAACC,MAAM,CAACC,YAAY;IAEnD,IAAI,CAACwL,SAAS,GAAG,IAAI,CAACzL,MAAM,CAACqZ,mBAAmB;IAChD,IAAI,CAAC1N,UAAU,GAAG,IAAI,CAAC3L,MAAM,CAACsZ,iBAAiB;IAC/C,IAAI,CAAC5N,MAAM,GAAG,IAAI,CAAC1L,MAAM,CAACiZ,WAAW,GAAG,IAAI,CAACjZ,MAAM,CAACqZ,mBAAmB;EAC3E;AACJ;AAEA,OAAO,MAAMkM,WAAW,SAASD,qBAAqB,CAAC;AAEvD,OAAO,MAAME,iBAAiB,SAASF,qBAAqB,CAAC;AAC7D;;AAGA;AACA;AACA,OAAO,MAAMG,mBAAmB,SAAS5gB,eAAe,CAAC;AAEzD,OAAO,MAAM6gB,SAAS,SAASD,mBAAmB,CAAC;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAME,2BAA2B,SAASF,mBAAmB,CAAC;EAEjE;EACA,aAAa5f,eAAeA,CAACpJ,6BAA6B,EAAEE,OAAO,GAAG,CAAC,CAAC,EAAE;IACtE;IACAA,OAAO,CAACuJ,eAAe,KAAK,YAAY;IACxC,OAAO,KAAK,CAACL,eAAe,CAACpJ,6BAA6B,EAAEE,OAAO,CAAC;EACxE;AACJ;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMipB,4BAA4B,SAASH,mBAAmB,CAAC;EAClE;EACA,aAAa5f,eAAeA,CAACpJ,6BAA6B,EAAEE,OAAO,GAAG,CAAC,CAAC,EAAE;IACtE;IACAA,OAAO,CAACuJ,eAAe,KAAK,aAAa;IACzC,OAAO,KAAK,CAACL,eAAe,CAACpJ,6BAA6B,EAAEE,OAAO,CAAC;EACxE;AACJ;AACA;;AAGA;AACA;AACA,OAAO,MAAMkpB,mBAAmB,SAAShhB,eAAe,CAAC;;AAEzD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMihB,SAAS,SAASD,mBAAmB,CAAC;EAC/C;AACJ;AACA;AACA;AACA;EACI,MAAMxf,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIykB,eAAe,CAAC,MAAM,KAAK,CAAC1f,KAAK,CAAC/E,YAAY,CAAC,CAAC;EAC/D;AACJ;AACA;;AAEA;AACA;AACA,OAAO,MAAM0kB,wBAAwB,SAASnhB,eAAe,CAAC;;AAE9D;AACA;AACA;AACA,OAAO,MAAMohB,cAAc,SAASD,wBAAwB,CAAC;;AAE7D;AACA;AACA;AACA,OAAO,MAAME,+BAA+B,SAASF,wBAAwB,CAAC;;AAE9E;AACA;AACA;AACA,OAAO,MAAMG,gCAAgC,SAASH,wBAAwB,CAAC;;AAE/E;;AAEA;AACA;AACA,OAAO,MAAMI,uBAAuB,SAASvhB,eAAe,CAAC;EACzD;AACJ;AACA;AACA;AACA;AACA;EACIC,WAAWA,CAAC9E,MAAM,EAAE3C,OAAO,EAAEqF,iBAAiB,EAAE;IAC5C,KAAK,CAAC1C,MAAM,EAAE3C,OAAO,CAAC;IACtB,IAAI,CAACqF,iBAAiB,GAAGA,iBAAiB;;IAE1C;IACA,IAAI,CAAC1C,MAAM,CAACD,YAAY,GAAG,IAAI,CAACC,MAAM,CAACC,YAAY;IAEnD,IAAI,CAACwL,SAAS,GAAG,IAAI,CAACzL,MAAM,CAACqZ,mBAAmB;IAChD,IAAI,CAAC1N,UAAU,GAAG,IAAI,CAAC3L,MAAM,CAACsZ,iBAAiB;IAC/C,IAAI,CAAC5N,MAAM,GAAG,IAAI,CAAC1L,MAAM,CAACiZ,WAAW,GAAG,IAAI,CAACxN,SAAS;EAC1D;AACJ;;AAEA;AACA;AACA;AACA,OAAO,MAAM4a,aAAa,SAASD,uBAAuB,CAAC;;AAE3D;AACA;AACA;AACA,OAAO,MAAME,mBAAmB,SAASF,uBAAuB,CAAC;AACjE;;AAGA;AACA,OAAO,MAAMG,2BAA2B,SAAS1hB,eAAe,CAAC;;AAEjE;AACA;AACA;AACA,OAAO,MAAM2hB,iBAAiB,SAASD,2BAA2B,CAAC;;AAEnE;AACA;AACA;AACA,OAAO,MAAME,kCAAkC,SAASF,2BAA2B,CAAC;EAChF;AACJ;AACA;EACI,MAAMlgB,KAAKA,CAAC/E,YAAY,EAAE;IACtB,OAAO,IAAIkL,wBAAwB,CAAC,MAAM,KAAK,CAACnG,KAAK,CAAC/E,YAAY,CAAC,CAAC;EACxE;AACJ;AACA;;AAGA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO,MAAMolB,eAAe,CAAC;EACzB;AACJ;AACA;AACA;EACI,OAAOC,oBAAoB,GAAG,IAAI;;EAElC;AACJ;AACA;AACA;EACI,OAAOC,YAAY,GAAG,KAAK;;EAG3B;EACA,aAAa/gB,eAAeA,CAACpJ,6BAA6B,EAAE;IACxDI,SAAS,GAAG,IAAI;IAChBiJ,iBAAiB,GAAG,IAAI;IACxB9F,MAAM,GAAG,IAAI;IACb+F,SAAS,GAAG,IAAI;IAChBC,gBAAgB,GAAG,KAAK;IACxBC,QAAQ,GAAG,MAAM;IACjBC,eAAe,GAAG;EACtB,CAAC,GAAG,CAAC,CAAC,EAAE;IAEJ,IAAIvJ,OAAO,GAAG;MACVE,SAAS;MACTiJ,iBAAiB;MACjB9F,MAAM;MACN+F,SAAS;MACTC,gBAAgB;MAChBC,QAAQ;MACRC;IACJ,CAAC;IACDlG,MAAM,GAAG,MAAMpG,UAAU,CAACiM,eAAe,CAACpJ,6BAA6B,EAAEE,OAAO,CAAC;IACjF,IAAI,CAACA,OAAO,CAACqD,MAAM,EAAE;MACjB;MACArD,OAAO,CAACqD,MAAM,GAAGA,MAAM;IAC3B;IAEA,IAAI,CAAC,IAAI,CAAC2mB,oBAAoB,EAAE;MAC5B,MAAM,IAAI1oB,KAAK,CAAC,uEAAuE,GAAG,IAAI,CAAC+J,IAAI,CAAC;IACxG;IAEA,KAAK,IAAI6e,mBAAmB,IAAI,IAAI,CAACF,oBAAoB,EAAE;MACvD,MAAMG,SAAS,GAAGD,mBAAmB,CAAC7hB,GAAG,CAAChF,MAAM,CAACoG,UAAU,CAAC;MAC5D,IAAI,CAAC0gB,SAAS,EAAE;QACZ,SAAS,CAAC;MACd;MACA,OAAO,MAAMA,SAAS,CAAC,CAAC,CAAC,CAACjhB,eAAe,CAACpJ,6BAA6B,EAAEE,OAAO,CAAC;IACrF;IAEA,IAAI,IAAI,CAACiqB,YAAY,EAAE;MACnB1pB,OAAO,CAACC,IAAI,CAAC,wBAAwB6C,MAAM,CAACoG,UAAU,6CAA6C,CAAC;MACpG,OAAO,MAAMvB,eAAe,CAACgB,eAAe,CAACpJ,6BAA6B,EAAEE,OAAO,CAAC;IACxF,CAAC,MAAM;MACH,MAAMsB,KAAK,CAAC,2BAA2B+B,MAAM,CAACoG,UAAU,EAAE,CAAC;IAC/D;EACJ;AACJ;AAEA,MAAMqR,gCAAgC,GAAG,IAAIpb,GAAG,CAAC,CAC7C,CAAC,MAAM,EAAE,CAAC,WAAW,EAAE+P,SAAS,CAAC,CAAC,EAClC,CAAC,YAAY,EAAE,CAAC,gBAAgB,EAAEU,cAAc,CAAC,CAAC,EAClD,CAAC,UAAU,EAAE,CAAC,eAAe,EAAEE,aAAa,CAAC,CAAC,EAC9C,CAAC,SAAS,EAAE,CAAC,cAAc,EAAEY,YAAY,CAAC,CAAC,EAC3C,CAAC,KAAK,EAAE,CAAC,UAAU,EAAE8B,QAAQ,CAAC,CAAC,EAC/B,CAAC,UAAU,EAAE,CAAC,eAAe,EAAEpC,aAAa,CAAC,CAAC,EAC9C,CAAC,WAAW,EAAE,CAAC,gBAAgB,EAAEY,cAAc,CAAC,CAAC,EACjD,CAAC,SAAS,EAAE,CAAC,cAAc,EAAEM,YAAY,CAAC,CAAC,EAC3C,CAAC,YAAY,EAAE,CAAC,gBAAgB,EAAEM,cAAc,CAAC,CAAC,EAClD,CAAC,OAAO,EAAE,CAAC,YAAY,EAAEsB,UAAU,CAAC,CAAC,EACrC,CAAC,QAAQ,EAAE,CAAC,aAAa,EAAEW,WAAW,CAAC,CAAC,EACxC,CAAC,YAAY,EAAE,CAAC,iBAAiB,EAAE3B,eAAe,CAAC,CAAC,EACpD,CAAC,SAAS,EAAE,CAAC,cAAc,EAAE+D,YAAY,CAAC,CAAC,EAC3C,CAAC,KAAK,EAAE,CAAC,UAAU,EAAEM,QAAQ,CAAC,CAAC,EAC/B,CAAC,aAAa,EAAE,CAAC,iBAAiB,EAAEM,eAAe,CAAC,CAAC,EACrD,CAAC,MAAM,EAAE,CAAC,WAAW,EAAE2R,SAAS,CAAC,CAAC,EAClC,CAAC,MAAM,EAAE,CAAC,WAAW,EAAE5N,SAAS,CAAC,CAAC,EAClC,CAAC,SAAS,EAAE,CAAC,cAAc,EAAEU,YAAY,CAAC,CAAC,EAC3C,CAAC,cAAc,EAAE,CAAC,kBAAkB,EAAEF,gBAAgB,CAAC,CAAC,EACxD,CAAC,QAAQ,EAAE,CAAC,aAAa,EAAEJ,WAAW,CAAC,CAAC,EACxC,CAAC,YAAY,EAAE,CAAC,iBAAiB,EAAEnI,eAAe,CAAC,CAAC,EACpD,CAAC,aAAa,EAAE,CAAC,kBAAkB,EAAEW,gBAAgB,CAAC,CAAC,EACvD,CAAC,UAAU,EAAE,CAAC,eAAe,EAAE0Q,aAAa,CAAC,CAAC,EAC9C,CAAC,eAAe,EAAE,CAAC,mBAAmB,EAAEe,iBAAiB,CAAC,CAAC,EAC3D,CAAC,WAAW,EAAE,CAAC,gBAAgB,EAAET,cAAc,CAAC,CAAC,EACjD,CAAC,eAAe,EAAE,CAAC,mBAAmB,EAAEI,iBAAiB,CAAC,CAAC,EAC3D,CAAC,QAAQ,EAAE,CAAC,aAAa,EAAES,WAAW,CAAC,CAAC,EACxC,CAAC,OAAO,EAAE,CAAC,YAAY,EAAEI,UAAU,CAAC,CAAC,EACrC,CAAC,+BAA+B,EAAE,CAAC,UAAU,EAAEtO,QAAQ,CAAC,CAAC,EACzD,CAAC,MAAM,EAAE,CAAC,WAAW,EAAEyR,SAAS,CAAC,CAAC,EAElC,CAAC,MAAM,EAAE,CAAC,WAAW,EAAE9I,SAAS,CAAC,CAAC,EAClC,CAAC,mBAAmB,EAAE,CAAC,uBAAuB,EAAEQ,qBAAqB,CAAC,CAAC,EACvE,CAAC,KAAK,EAAE,CAAC,UAAU,EAAEhC,QAAQ,CAAC,CAAC,EAC/B,CAAC,SAAS,EAAE,CAAC,cAAc,EAAEG,YAAY,CAAC,CAAC,EAC3C,CAAC,WAAW,EAAE,CAAC,gBAAgB,EAAEM,cAAc,CAAC,CAAC,EACjD,CAAC,aAAa,EAAE,CAAC,kBAAkB,EAAEG,gBAAgB,CAAC,CAAC,EACvD,CAAC,QAAQ,EAAE,CAAC,aAAa,EAAEG,WAAW,CAAC,CAAC,EACxC,CAAC,OAAO,EAAE,CAAC,YAAY,EAAEG,UAAU,CAAC,CAAC,EACrC,CAAC,MAAM,EAAE,CAAC,WAAW,EAAEG,SAAS,CAAC,CAAC,EAClC,CAAC,MAAM,EAAE,CAAC,WAAW,EAAEe,SAAS,CAAC,CAAC,EAClC,CAAC,UAAU,EAAE,CAAC,eAAe,EAAEsB,aAAa,CAAC,CAAC,EAC9C,CAAC,YAAY,EAAE,CAAC,iBAAiB,EAAEG,eAAe,CAAC,CAAC,EACpD,CAAC,QAAQ,EAAE,CAAC,aAAa,EAAEG,WAAW,CAAC,CAAC,EACxC,CAAC,QAAQ,EAAE,CAAC,aAAa,EAAEzB,WAAW,CAAC,CAAC,EACxC,CAAC,MAAM,EAAE,CAAC,WAAW,EAAEG,SAAS,CAAC,CAAC,EAClC,CAAC,SAAS,EAAE,CAAC,cAAc,EAAEG,YAAY,CAAC,CAAC,EAC3C,CAAC,YAAY,EAAE,CAAC,gBAAgB,EAAEW,cAAc,CAAC,CAAC,EAClD,CAAC,OAAO,EAAE,CAAC,YAAY,EAAEW,UAAU,CAAC,CAAC,EACrC,CAAC,KAAK,EAAE,CAAC,UAAU,EAAEnB,QAAQ,CAAC,CAAC,EAC/B,CAAC,MAAM,EAAE,CAAC,WAAW,EAAEK,SAAS,CAAC,CAAC,EAElC,CAAC,SAAS,EAAE,CAAC,iBAAiB,EAAEgG,eAAe,CAAC,CAAC,EACjD,CAAC,cAAc,EAAE,CAAC,mBAAmB,EAAE2B,iBAAiB,CAAC,CAAC,CAE7D,CAAC;AAEF,MAAM9O,mCAAmC,GAAG,IAAIrb,GAAG,CAAC,CAChD,CAAC,IAAI,EAAE,CAAC,SAAS,EAAE+U,OAAO,CAAC,CAAC,EAC5B,CAAC,QAAQ,EAAE,CAAC,aAAa,EAAEK,WAAW,CAAC,CAAC,EACxC,CAAC,KAAK,EAAE,CAAC,UAAU,EAAEG,QAAQ,CAAC,CAAC,EAC/B,CAAC,MAAM,EAAE,CAAC,WAAW,EAAEG,SAAS,CAAC,CAAC,EAClC,CAAC,OAAO,EAAE,CAAC,YAAY,EAAES,UAAU,CAAC,CAAC,EACrC,CAAC,QAAQ,EAAE,CAAC,aAAa,EAAEsO,WAAW,CAAC,CAAC,EACxC,CAAC,SAAS,EAAE,CAAC,cAAc,EAAEtM,YAAY,CAAC,CAAC,EAC3C,CAAC,SAAS,EAAE,CAAC,aAAa,EAAEyM,WAAW,CAAC,CAAC,EACzC,CAAC,YAAY,EAAE,CAAC,iBAAiB,EAAEpO,eAAe,CAAC,CAAC,EACpD,CAAC,kBAAkB,EAAE,CAAC,sBAAsB,EAAEG,oBAAoB,CAAC,CAAC,CACvE,CAAC;AAGF,MAAM+T,gCAAgC,GAAG,IAAI1qB,GAAG,CAAC,CAC7C,CAAC,OAAO,EAAE,CAAC,YAAY,EAAEwe,UAAU,CAAC,CAAC,EACrC,CAAC,MAAM,EAAE,CAAC,WAAW,EAAE/B,SAAS,CAAC,CAAC,EAClC,CAAC,MAAM,EAAE,CAAC,WAAW,EAAEY,SAAS,CAAC,CAAC,EAClC,CAAC,aAAa,EAAE,CAAC,iBAAiB,EAAEG,eAAe,CAAC,CAAC,EACrD,CAAC,SAAS,EAAE,CAAC,aAAa,EAAEX,WAAW,CAAC,CAAC,EACzC,CAAC,UAAU,EAAE,CAAC,cAAc,EAAEK,YAAY,CAAC,CAAC,EAC5C,CAAC,SAAS,EAAE,CAAC,cAAc,EAAES,YAAY,CAAC,CAAC,EAC3C,CAAC,OAAO,EAAE,CAAC,YAAY,EAAEI,UAAU,CAAC,CAAC,EACrC,CAAC,OAAO,EAAE,CAAC,YAAY,EAAEG,UAAU,CAAC,CAAC,EACrC,CAAC,KAAK,EAAE,CAAC,UAAU,EAAEG,QAAQ,CAAC,CAAC,EAC/B,CAAC,KAAK,EAAE,CAAC,UAAU,EAAEQ,QAAQ,CAAC,CAAC,EAC/B,CAAC,KAAK,EAAE,CAAC,UAAU,EAAEG,QAAQ,CAAC,CAAC,EAC/B,CAAC,SAAS,EAAE,CAAC,cAAc,EAAE4J,YAAY,CAAC,CAAC,EAC3C,CAAC,YAAY,EAAE,CAAC,iBAAiB,EAAEG,eAAe,CAAC,CAAC,EACpD,CAAC,QAAQ,EAAE,CAAC,aAAa,EAAEG,WAAW,CAAC,CAAC,CAC3C,CAAC;AAEF,MAAMzd,wCAAwC,GAAG,IAAIzL,GAAG,CAAC,CACrD,CAAC,UAAU,EAAE,CAAC,yBAAyB,EAAE8mB,uBAAuB,CAAC,CAAC,EAClE,CAAC,SAAS,EAAE,CAAC,iCAAiC,EAAE1O,+BAA+B,CAAC,CAAC,CACpF,CAAC;AAEF,MAAMuS,2CAA2C,GAAG,IAAI3qB,GAAG,CAAC,CACxD,CAAC,UAAU,EAAE,CAAC,yBAAyB,EAAE+mB,uBAAuB,CAAC,CAAC,CACrE,CAAC;AAEF,MAAM6D,wCAAwC,GAAG,IAAI5qB,GAAG,CAAC,CACrD,CAAC,MAAM,EAAE,CAAC,WAAW,EAAEypB,SAAS,CAAC,CAAC,CACrC,CAAC;AAEF,MAAMoB,+CAA+C,GAAG,IAAI7qB,GAAG,CAAC,CAC5D,CAAC,MAAM,EAAE,CAAC,+BAA+B,EAAEkQ,6BAA6B,CAAC,CAAC,EAC1E,CAAC,UAAU,EAAE,CAAC,mCAAmC,EAAEW,iCAAiC,CAAC,CAAC,EACtF,CAAC,SAAS,EAAE,CAAC,kCAAkC,EAAEY,gCAAgC,CAAC,CAAC,EACnF,CAAC,KAAK,EAAE,CAAC,8BAA8B,EAAE8B,4BAA4B,CAAC,CAAC,EACvE,CAAC,UAAU,EAAE,CAAC,mCAAmC,EAAEpC,iCAAiC,CAAC,CAAC,EACtF,CAAC,WAAW,EAAE,CAAC,oCAAoC,EAAEY,kCAAkC,CAAC,CAAC,EACzF,CAAC,SAAS,EAAE,CAAC,kCAAkC,EAAEM,gCAAgC,CAAC,CAAC,EACnF,CAAC,YAAY,EAAE,CAAC,oCAAoC,EAAEM,kCAAkC,CAAC,CAAC,EAC1F,CAAC,OAAO,EAAE,CAAC,gCAAgC,EAAEsB,8BAA8B,CAAC,CAAC,EAC7E,CAAC,QAAQ,EAAE,CAAC,iCAAiC,EAAEU,+BAA+B,CAAC,CAAC,EAChF,CAAC,YAAY,EAAE,CAAC,qCAAqC,EAAE3B,mCAAmC,CAAC,CAAC,EAC5F,CAAC,SAAS,EAAE,CAAC,kCAAkC,EAAEgE,gCAAgC,CAAC,CAAC,EACnF,CAAC,KAAK,EAAE,CAAC,8BAA8B,EAAEM,4BAA4B,CAAC,CAAC,EACvE,CAAC,aAAa,EAAE,CAAC,qCAAqC,EAAEM,mCAAmC,CAAC,CAAC,EAC7F,CAAC,MAAM,EAAE,CAAC,+BAA+B,EAAE3B,6BAA6B,CAAC,CAAC,EAC1E,CAAC,OAAO,EAAE,CAAC,gCAAgC,EAAEI,8BAA8B,CAAC,CAAC,EAC7E,CAAC,YAAY,EAAE,CAAC,qCAAqC,EAAEzC,mCAAmC,CAAC,CAAC,EAC5F,CAAC,aAAa,EAAE,CAAC,sCAAsC,EAAEW,oCAAoC,CAAC,CAAC,CAClG,CAAC;AAEF,MAAMuW,4CAA4C,GAAG,IAAI9qB,GAAG,CAAC,CACzD,CAAC,MAAM,EAAE,CAAC,4BAA4B,EAAEoQ,0BAA0B,CAAC,CAAC,EACpE,CAAC,UAAU,EAAE,CAAC,gCAAgC,EAAEU,8BAA8B,CAAC,CAAC,EAChF,CAAC,SAAS,EAAE,CAAC,+BAA+B,EAAEY,6BAA6B,CAAC,CAAC,EAC7E,CAAC,KAAK,EAAE,CAAC,2BAA2B,EAAE8B,yBAAyB,CAAC,CAAC,EACjE,CAAC,UAAU,EAAE,CAAC,gCAAgC,EAAEpC,8BAA8B,CAAC,CAAC,EAChF,CAAC,WAAW,EAAE,CAAC,iCAAiC,EAAEY,+BAA+B,CAAC,CAAC,EACnF,CAAC,SAAS,EAAE,CAAC,+BAA+B,EAAEM,6BAA6B,CAAC,CAAC,EAC7E,CAAC,YAAY,EAAE,CAAC,iCAAiC,EAAEM,+BAA+B,CAAC,CAAC,EACpF,CAAC,OAAO,EAAE,CAAC,6BAA6B,EAAEsB,2BAA2B,CAAC,CAAC,EACvE,CAAC,YAAY,EAAE,CAAC,kCAAkC,EAAEjB,gCAAgC,CAAC,CAAC,EACtF,CAAC,SAAS,EAAE,CAAC,+BAA+B,EAAEgE,6BAA6B,CAAC,CAAC,EAC7E,CAAC,KAAK,EAAE,CAAC,2BAA2B,EAAEM,yBAAyB,CAAC,CAAC,EACjE,CAAC,aAAa,EAAE,CAAC,kCAAkC,EAAEM,gCAAgC,CAAC,CAAC,CAC1F,CAAC;AAEF,MAAMrM,4CAA4C,GAAG,IAAIxL,GAAG,CAAC,CACzD,CAAC,IAAI,EAAE,CAAC,4BAA4B,EAAEgV,0BAA0B,CAAC,CAAC,EAClE,CAAC,QAAQ,EAAE,CAAC,gCAAgC,EAAEK,8BAA8B,CAAC,CAAC,EAC9E,CAAC,KAAK,EAAE,CAAC,6BAA6B,EAAEG,2BAA2B,CAAC,CAAC,EACrE,CAAC,MAAM,EAAE,CAAC,8BAA8B,EAAEG,4BAA4B,CAAC,CAAC,EACxE,CAAC,OAAO,EAAE,CAAC,+BAA+B,EAAES,6BAA6B,CAAC,CAAC,EAC3E,CAAC,QAAQ,EAAE,CAAC,eAAe,EAAEsO,aAAa,CAAC,CAAC,EAC5C,CAAC,SAAS,EAAE,CAAC,gCAAgC,EAAEG,8BAA8B,CAAC,CAAC,EAC/E,CAAC,YAAY,EAAE,CAAC,oCAAoC,EAAEpO,kCAAkC,CAAC,CAAC,EAC1F,CAAC,kBAAkB,EAAE,CAAC,yCAAyC,EAAEG,uCAAuC,CAAC,CAAC,CAC7G,CAAC;AAEF,MAAMrL,gCAAgC,GAAG,IAAIvL,GAAG,CAAC,CAC7C,CAAC,OAAO,EAAE,CAAC,kBAAkB,EAAEye,gBAAgB,CAAC,CAAC,EACjD,CAAC,MAAM,EAAE,CAAC,iBAAiB,EAAE/B,eAAe,CAAC,CAAC,EAC9C,CAAC,MAAM,EAAE,CAAC,iBAAiB,EAAEY,eAAe,CAAC,CAAC,EAC9C,CAAC,aAAa,EAAE,CAAC,uBAAuB,EAAEG,qBAAqB,CAAC,CAAC,EACjE,CAAC,SAAS,EAAE,CAAC,mBAAmB,EAAEX,iBAAiB,CAAC,CAAC,EACrD,CAAC,UAAU,EAAE,CAAC,oBAAoB,EAAEK,kBAAkB,CAAC,CAAC,EACxD,CAAC,SAAS,EAAE,CAAC,oBAAoB,EAAES,kBAAkB,CAAC,CAAC,EACvD,CAAC,OAAO,EAAE,CAAC,kBAAkB,EAAEI,gBAAgB,CAAC,CAAC,EACjD,CAAC,OAAO,EAAE,CAAC,kBAAkB,EAAEG,gBAAgB,CAAC,CAAC,EACjD,CAAC,KAAK,EAAE,CAAC,gBAAgB,EAAEG,cAAc,CAAC,CAAC,EAC3C,CAAC,KAAK,EAAE,CAAC,gBAAgB,EAAEQ,cAAc,CAAC,CAAC,EAC3C,CAAC,KAAK,EAAE,CAAC,gBAAgB,EAAEG,cAAc,CAAC,CAAC,EAC3C,CAAC,OAAO,EAAE,CAAC,kBAAkB,EAAE3I,gBAAgB,CAAC,CAAC,EACjD,CAAC,SAAS,EAAE,CAAC,oBAAoB,EAAEuS,kBAAkB,CAAC,CAAC,EACvD,CAAC,YAAY,EAAE,CAAC,uBAAuB,EAAEG,qBAAqB,CAAC,CAAC,EAChE,CAAC,QAAQ,EAAE,CAAC,mBAAmB,EAAEG,iBAAiB,CAAC,CAAC,EACpD,CAAC,OAAO,EAAE,CAAC,kBAAkB,EAAET,gBAAgB,CAAC,CAAC,EACjD,CAAC,UAAU,EAAE,CAAC,qBAAqB,EAAEuB,mBAAmB,CAAC,CAAC,CAC7D,CAAC;AAEF,MAAMc,iCAAiC,GAAG,IAAI/qB,GAAG,CAAC,CAC9C,CAAC,MAAM,EAAE,CAAC,iBAAiB,EAAEgQ,eAAe,CAAC,CAAC,EAC9C,CAAC,UAAU,EAAE,CAAC,qBAAqB,EAAEY,mBAAmB,CAAC,CAAC,EAC1D,CAAC,SAAS,EAAE,CAAC,oBAAoB,EAAEY,kBAAkB,CAAC,CAAC,EACvD,CAAC,KAAK,EAAE,CAAC,gBAAgB,EAAE8B,cAAc,CAAC,CAAC,EAC3C,CAAC,UAAU,EAAE,CAAC,qBAAqB,EAAEpC,mBAAmB,CAAC,CAAC,EAC1D,CAAC,WAAW,EAAE,CAAC,sBAAsB,EAAEY,oBAAoB,CAAC,CAAC,EAC7D,CAAC,SAAS,EAAE,CAAC,oBAAoB,EAAEM,kBAAkB,CAAC,CAAC,EACvD,CAAC,YAAY,EAAE,CAAC,sBAAsB,EAAEM,oBAAoB,CAAC,CAAC,EAC9D,CAAC,OAAO,EAAE,CAAC,kBAAkB,EAAEsB,gBAAgB,CAAC,CAAC,EACjD,CAAC,QAAQ,EAAE,CAAC,mBAAmB,EAAEa,iBAAiB,CAAC,CAAC,EACpD,CAAC,YAAY,EAAE,CAAC,uBAAuB,EAAE1B,qBAAqB,CAAC,CAAC,EAChE,CAAC,SAAS,EAAE,CAAC,oBAAoB,EAAE4D,kBAAkB,CAAC,CAAC,EACvD,CAAC,KAAK,EAAE,CAAC,oBAAoB,EAAEM,kBAAkB,CAAC,CAAC,EACnD,CAAC,aAAa,EAAE,CAAC,uBAAuB,EAAEM,qBAAqB,CAAC,CAAC,EACjE,CAAC,YAAY,EAAE,CAAC,uBAAuB,EAAEhE,qBAAqB,CAAC,CAAC,EAChE,CAAC,aAAa,EAAE,CAAC,wBAAwB,EAAEW,sBAAsB,CAAC,CAAC,CACtE,CAAC;AAEF,MAAM0W,0CAA0C,GAAG,IAAIhrB,GAAG,CAAC,CACvD,CAAC,MAAM,EAAE,CAAC,0BAA0B,EAAEsQ,wBAAwB,CAAC,CAAC,EAChE,CAAC,UAAU,EAAE,CAAC,8BAA8B,EAAES,4BAA4B,CAAC,CAAC,EAC5E,CAAC,SAAS,EAAE,CAAC,6BAA6B,EAAEY,2BAA2B,CAAC,CAAC,EACzE,CAAC,UAAU,EAAE,CAAC,8BAA8B,EAAEN,4BAA4B,CAAC,CAAC,EAC5E,CAAC,WAAW,EAAE,CAAC,+BAA+B,EAAEY,6BAA6B,CAAC,CAAC,EAC/E,CAAC,SAAS,EAAE,CAAC,6BAA6B,EAAEM,2BAA2B,CAAC,CAAC,EACzE,CAAC,YAAY,EAAE,CAAC,+BAA+B,EAAEM,6BAA6B,CAAC,CAAC,EAChF,CAAC,OAAO,EAAE,CAAC,2BAA2B,EAAEsB,yBAAyB,CAAC,CAAC,EACnE,CAAC,QAAQ,EAAE,CAAC,4BAA4B,EAAES,0BAA0B,CAAC,CAAC,EACtE,CAAC,YAAY,EAAE,CAAC,gCAAgC,EAAE1B,8BAA8B,CAAC,CAAC,EAClF,CAAC,SAAS,EAAE,CAAC,6BAA6B,EAAEgE,2BAA2B,CAAC,CAAC,EACzE,CAAC,KAAK,EAAE,CAAC,yBAAyB,EAAEM,uBAAuB,CAAC,CAAC,EAC7D,CAAC,aAAa,EAAE,CAAC,gCAAgC,EAAEM,8BAA8B,CAAC,CAAC,EACnF,CAAC,YAAY,EAAE,CAAC,gCAAgC,EAAEjE,8BAA8B,CAAC,CAAC,EAClF,CAAC,aAAa,EAAE,CAAC,iCAAiC,EAAEW,+BAA+B,CAAC,CAAC,CACxF,CAAC;AAEF,MAAM9I,oCAAoC,GAAG,IAAI1L,GAAG,CAAC,CACjD,CAAC,wBAAwB,EAAE,CAAC,2BAA2B,EAAE6a,yBAAyB,CAAC,CAAC,CACvF,CAAC;AAEF,MAAMoQ,mDAAmD,GAAG,IAAIjrB,GAAG,CAAC,CAChE,CAAC,wBAAwB,EAAE,CAAC,2BAA2B,EAAE6a,yBAAyB,CAAC,CAAC,CACvF,CAAC;AAEF,MAAMqQ,4CAA4C,GAAG,IAAIlrB,GAAG,CAAC,CACzD,CAAC,KAAK,EAAE,CAAC,2BAA2B,EAAEof,yBAAyB,CAAC,CAAC,EACjE,CAAC,SAAS,EAAE,CAAC,+BAA+B,EAAEG,6BAA6B,CAAC,CAAC,EAC7E,CAAC,WAAW,EAAE,CAAC,iCAAiC,EAAEM,+BAA+B,CAAC,CAAC,EACnF,CAAC,aAAa,EAAE,CAAC,mCAAmC,EAAEG,iCAAiC,CAAC,CAAC,EACzF,CAAC,MAAM,EAAE,CAAC,4BAA4B,EAAES,0BAA0B,CAAC,CAAC,EACpE,CAAC,MAAM,EAAE,CAAC,4BAA4B,EAAEe,0BAA0B,CAAC,CAAC,EACpE,CAAC,UAAU,EAAE,CAAC,gCAAgC,EAAEsB,8BAA8B,CAAC,CAAC,EAChF,CAAC,YAAY,EAAE,CAAC,kCAAkC,EAAEG,gCAAgC,CAAC,CAAC,EACtF,CAAC,QAAQ,EAAE,CAAC,8BAA8B,EAAEG,4BAA4B,CAAC,CAAC,EAC1E,CAAC,QAAQ,EAAE,CAAC,8BAA8B,EAAEzB,4BAA4B,CAAC,CAAC,EAC1E,CAAC,MAAM,EAAE,CAAC,4BAA4B,EAAEG,0BAA0B,CAAC,CAAC,EACpE,CAAC,WAAW,EAAE,CAAC,iCAAiC,EAAE+H,+BAA+B,CAAC,CAAC,EACnF,CAAC,cAAc,EAAE,CAAC,oCAAoC,EAAEO,kCAAkC,CAAC,CAAC,CAC/F,CAAC;AAEF,MAAMe,wCAAwC,GAAG,IAAInrB,GAAG,CAAC,CACrD,CAAC,MAAM,EAAE,CAAC,wBAAwB,EAAE4gB,sBAAsB,CAAC,CAAC,EAC5D,CAAC,mBAAmB,EAAE,CAAC,oCAAoC,EAAEQ,kCAAkC,CAAC,CAAC,EACjG,CAAC,OAAO,EAAE,CAAC,yBAAyB,EAAEmC,uBAAuB,CAAC,CAAC,CAClE,CAAC;AAEF,MAAM6H,kDAAkD,GAAG,IAAIprB,GAAG,CAAC,CAC/D,CAAC,QAAQ,EAAE,CAAC,0BAA0B,EAAEmgB,wBAAwB,CAAC,CAAC,EAClE,CAAC,OAAO,EAAE,CAAC,yBAAyB,EAAEG,uBAAuB,CAAC,CAAC,CAClE,CAAC;AAEF,MAAM+K,0CAA0C,GAAG,IAAIrrB,GAAG,CAAC,CACvD,CAAC,MAAM,EAAE,CAAC,qBAAqB,EAAE8gB,mBAAmB,CAAC,CAAC,EACtD,CAAC,SAAS,EAAE,CAAC,6BAA6B,EAAE1E,2BAA2B,CAAC,CAAC,CAC5E,CAAC;AAEF,MAAMkP,6CAA6C,GAAG,IAAItrB,GAAG,CAAC,CAC1D,CAAC,WAAW,EAAE,CAAC,kCAAkC,EAAE8pB,gCAAgC,CAAC,CAAC,CACxF,CAAC;AAEF,MAAMyB,uCAAuC,GAAG,IAAIvrB,GAAG,CAAC,CACpD,CAAC,KAAK,EAAE,CAAC,UAAU,EAAE0jB,QAAQ,CAAC,CAAC,CAClC,CAAC;AAEF,MAAM8H,2BAA2B,GAAG,IAAIxrB,GAAG,CAAC,CACxC,CAAC,UAAU,EAAE,CAAC,gBAAgB,EAAEglB,cAAc,CAAC,CAAC,EAChD,CAAC,eAAe,EAAE,CAAC,oBAAoB,EAAEe,kBAAkB,CAAC,CAAC,EAC7D,CAAC,WAAW,EAAE,CAAC,iBAAiB,EAAET,eAAe,CAAC,CAAC,EACnD,CAAC,eAAe,EAAE,CAAC,oBAAoB,EAAEI,kBAAkB,CAAC,CAAC,EAC7D,CAAC,OAAO,EAAE,CAAC,aAAa,EAAEa,WAAW,CAAC,CAAC,EACvC,CAAC,QAAQ,EAAE,CAAC,cAAc,EAAEJ,YAAY,CAAC,CAAC,CAC7C,CAAC;AAEF,MAAMsF,4CAA4C,GAAG,IAAIzrB,GAAG,CAAC,CACzD,CAAC,UAAU,EAAE,CAAC,mCAAmC,EAAEklB,iCAAiC,CAAC,CAAC,EACtF,CAAC,eAAe,EAAE,CAAC,uCAAuC,EAAEc,qCAAqC,CAAC,CAAC,EACnG,CAAC,WAAW,EAAE,CAAC,oCAAoC,EAAET,kCAAkC,CAAC,CAAC,EACzF,CAAC,eAAe,EAAE,CAAC,uCAAuC,EAAEI,qCAAqC,CAAC,CAAC,EACnG,CAAC,OAAO,EAAE,CAAC,gCAAgC,EAAEa,8BAA8B,CAAC,CAAC,EAC7E,CAAC,QAAQ,EAAE,CAAC,iCAAiC,EAAEJ,+BAA+B,CAAC,CAAC,EAChF,CAAC,+BAA+B,EAAE,CAAC,2BAA2B,EAAEnO,yBAAyB,CAAC,CAAC,CAC9F,CAAC;AAEF,MAAMyT,qCAAqC,GAAG,IAAI1rB,GAAG,CAAC,CAClD,CAAC,OAAO,EAAE,CAAC,iBAAiB,EAAEymB,eAAe,CAAC,CAAC,CAClD,CAAC;AAEF,MAAMkF,kDAAkD,GAAG,IAAI3rB,GAAG,CAAC,CAC/D,CAAC,eAAe,EAAE,CAAC,yCAAyC,EAAE4lB,uCAAuC,CAAC,CAAC,EACvG,CAAC,OAAO,EAAE,CAAC,kCAAkC,EAAEe,gCAAgC,CAAC,CAAC,EACjF,CAAC,UAAU,EAAE,CAAC,qCAAqC,EAAExB,mCAAmC,CAAC,CAAC,CAC7F,CAAC;AAEF,MAAMyG,qCAAqC,GAAG,IAAI5rB,GAAG,CAAC,CAClD,CAAC,UAAU,EAAE,CAAC,yBAAyB,EAAEyf,uBAAuB,CAAC,CAAC,CACrE,CAAC;AAEF,MAAMoM,sCAAsC,GAAG,IAAI7rB,GAAG,CAAC,CACnD,CAAC,SAAS,EAAE,CAAC,gCAAgC,EAAEiiB,8BAA8B,CAAC,CAAC,CAClF,CAAC;AAEF,MAAM6J,wCAAwC,GAAG,IAAI9rB,GAAG,CAAC,CACrD,CAAC,KAAK,EAAE,CAAC,uBAAuB,EAAEoiB,qBAAqB,CAAC,CAAC,EACzD,CAAC,gBAAgB,EAAE,CAAC,iCAAiC,EAAEE,+BAA+B,CAAC,CAAC,EACxF,CAAC,MAAM,EAAE,CAAC,wBAAwB,EAAEG,sBAAsB,CAAC,CAAC,CAC/D,CAAC;;AAEF;AACA;AACA,MAAMsJ,gDAAgD,GAAG,IAAI/rB,GAAG,CAAC,CAC7D,CAAC,MAAM,EAAE,CAAC,+BAA+B,EAAE2b,6BAA6B,CAAC,CAAC,EAC1E,CAAC,QAAQ,EAAE,CAAC,mBAAmB,EAAEI,iBAAiB,CAAC,CAAC,CACvD,CAAC;AAEF,MAAMiQ,wBAAwB,GAAG,CAC7B,CAAC5Q,gCAAgC,EAAE5b,WAAW,CAACC,WAAW,CAAC,EAC3D,CAAC4b,mCAAmC,EAAE7b,WAAW,CAACE,cAAc,CAAC,EACjE,CAACgrB,gCAAgC,EAAElrB,WAAW,CAACK,WAAW,CAAC,EAC3D,CAACgrB,+CAA+C,EAAErrB,WAAW,CAACC,WAAW,CAAC,EAC1E,CAACqrB,4CAA4C,EAAEtrB,WAAW,CAACC,WAAW,CAAC,EACvE,CAAC+L,4CAA4C,EAAEhM,WAAW,CAACG,OAAO,CAAC,EACnE,CAAC8L,wCAAwC,EAAEjM,WAAW,CAACG,OAAO,CAAC,EAC/D,CAAC4L,gCAAgC,EAAE/L,WAAW,CAACK,WAAW,CAAC,EAC3D,CAACkrB,iCAAiC,EAAEvrB,WAAW,CAACC,WAAW,CAAC,EAC5D,CAACurB,0CAA0C,EAAExrB,WAAW,CAACC,WAAW,CAAC,EACrE,CAACiM,oCAAoC,EAAElM,WAAW,CAACI,UAAU,CAAC,EAC9D,CAACsrB,4CAA4C,EAAE1rB,WAAW,CAACC,WAAW,CAAC,EACvE,CAAC4rB,0CAA0C,EAAE7rB,WAAW,CAACC,WAAW,CAAC,EACrE,CAAC6rB,6CAA6C,EAAE9rB,WAAW,CAACC,WAAW,CAAC,EACxE,CAACmsB,qCAAqC,EAAEpsB,WAAW,CAACC,WAAW,CAAC,EAChE,CAACosB,sCAAsC,EAAErsB,WAAW,CAACC,WAAW,CAAC,EACjE,CAACqsB,wCAAwC,EAAEtsB,WAAW,CAACC,WAAW,CAAC,EACnE,CAAC0rB,wCAAwC,EAAE3rB,WAAW,CAACC,WAAW,CAAC,EACnE,CAAC2rB,kDAAkD,EAAE5rB,WAAW,CAACC,WAAW,CAAC,EAC7E,CAAC8rB,uCAAuC,EAAE/rB,WAAW,CAACM,cAAc,CAAC,EACrE,CAAC0rB,2BAA2B,EAAEhsB,WAAW,CAACC,WAAW,CAAC,EACtD,CAACgsB,4CAA4C,EAAEjsB,WAAW,CAACC,WAAW,CAAC,EACvE,CAACkrB,2CAA2C,EAAEnrB,WAAW,CAACG,OAAO,CAAC,EAClE,CAACirB,wCAAwC,EAAEprB,WAAW,CAACC,WAAW,CAAC,EACnE,CAACisB,qCAAqC,EAAElsB,WAAW,CAACC,WAAW,CAAC,EAChE,CAACksB,kDAAkD,EAAEnsB,WAAW,CAACC,WAAW,CAAC;AAE7E;AACA,CAACssB,gDAAgD,EAAEvsB,WAAW,CAACC,WAAW,CAAC,CAC9E;AAED,KAAK,MAAM,CAACwsB,QAAQ,EAAEC,IAAI,CAAC,IAAIF,wBAAwB,EAAE;EACrD;EACA,KAAK,MAAM,CAACrgB,IAAI,EAAEwgB,KAAK,CAAC,IAAIF,QAAQ,CAAChe,MAAM,CAAC,CAAC,EAAE;IAC3ClO,kBAAkB,CAACga,GAAG,CAACpO,IAAI,EAAEugB,IAAI,CAAC;IAClChsB,2BAA2B,CAAC6Z,GAAG,CAACoS,KAAK,EAAExgB,IAAI,CAAC;IAC5C1L,2BAA2B,CAAC8Z,GAAG,CAACpO,IAAI,EAAEwgB,KAAK,CAAC;EAChD;AACJ;AAEA,MAAMC,cAAc,GAAG,CACnB,CAAC,6BAA6B,EAAE1Q,2BAA2B,EAAElc,WAAW,CAACC,WAAW,CAAC,EACrF,CAAC,iBAAiB,EAAEqc,eAAe,EAAEtc,WAAW,CAACC,WAAW,CAAC,EAC7D,CAAC,6BAA6B,EAAE6pB,2BAA2B,EAAE9pB,WAAW,CAACC,WAAW,CAAC,EACrF,CAAC,8BAA8B,EAAE8pB,4BAA4B,EAAE/pB,WAAW,CAACC,WAAW,CAAC,CAC1F;AACD,KAAK,MAAM,CAACkM,IAAI,EAAEwgB,KAAK,EAAED,IAAI,CAAC,IAAIE,cAAc,EAAE;EAC9CrsB,kBAAkB,CAACga,GAAG,CAACpO,IAAI,EAAEugB,IAAI,CAAC;EAClChsB,2BAA2B,CAAC6Z,GAAG,CAACoS,KAAK,EAAExgB,IAAI,CAAC;EAC5C1L,2BAA2B,CAAC8Z,GAAG,CAACpO,IAAI,EAAEwgB,KAAK,CAAC;AAChD;;AAGA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAME,SAAS,SAAShC,eAAe,CAAC;EAC3C;EACA;EACA,OAAOC,oBAAoB,GAAG0B,wBAAwB,CAAC3oB,GAAG,CAACJ,CAAC,IAAIA,CAAC,CAAC,CAAC,CAAC,CAAC;EACrE,OAAOsnB,YAAY,GAAG,IAAI;AAC9B;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAM+B,kCAAkC,SAASjC,eAAe,CAAC;EACpE,OAAOC,oBAAoB,GAAG,CAACO,+CAA+C,CAAC;AACnF;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAM0B,+BAA+B,SAASlC,eAAe,CAAC;EACjE,OAAOC,oBAAoB,GAAG,CAACQ,4CAA4C,CAAC;AAChF;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAM0B,qBAAqB,SAASnC,eAAe,CAAC;EACvD,OAAOC,oBAAoB,GAAG,CAAC9e,4CAA4C,CAAC;AAChF;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMihB,yBAAyB,SAASpC,eAAe,CAAC;EAC3D,OAAOC,oBAAoB,GAAG,CAAC7e,wCAAwC,CAAC;AAC5E;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMihB,6BAA6B,SAASrC,eAAe,CAAC;EAC/D,OAAOC,oBAAoB,GAAG,CAACK,2CAA2C,CAAC;AAC/E;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMgC,0BAA0B,SAAStC,eAAe,CAAC;EAC5D,OAAOC,oBAAoB,GAAG,CAACM,wCAAwC,CAAC;AAC5E;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMgC,oBAAoB,SAASvC,eAAe,CAAC;EACtD,OAAOC,oBAAoB,GAAG,CAAC/e,gCAAgC,CAAC;AACpE;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMshB,oBAAoB,SAASxC,eAAe,CAAC;EACtD,OAAOC,oBAAoB,GAAG,CAACS,iCAAiC,CAAC;AACrE;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAM+B,6BAA6B,SAASzC,eAAe,CAAC;EAC/D,OAAOC,oBAAoB,GAAG,CAACU,0CAA0C,CAAC;AAC9E;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAM+B,sBAAsB,SAAS1C,eAAe,CAAC;EACxD,OAAOC,oBAAoB,GAAG,CAAC5e,oCAAoC,CAAC;AACxE;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMshB,+BAA+B,SAAS3C,eAAe,CAAC;EACjE,OAAOC,oBAAoB,GAAG,CAACY,4CAA4C,CAAC;AAChF;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAM+B,6BAA6B,SAAS5C,eAAe,CAAC;EAC/D,OAAOC,oBAAoB,GAAG,CAACe,0CAA0C,CAAC;AAC9E;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAM6B,gCAAgC,SAAS7C,eAAe,CAAC;EAClE,OAAOC,oBAAoB,GAAG,CAACgB,6CAA6C,CAAC;AACjF;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAM6B,2BAA2B,SAAS9C,eAAe,CAAC;EAC7D,OAAOC,oBAAoB,GAAG,CAACa,wCAAwC,CAAC;AAC5E;AAEA,OAAO,MAAMiC,mCAAmC,SAAS/C,eAAe,CAAC;EACrE,OAAOC,oBAAoB,GAAG,CAACc,kDAAkD,CAAC;AACtF;;AAGA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMiC,0BAA0B,SAAShD,eAAe,CAAC;EAC5D,OAAOC,oBAAoB,GAAG,CAACiB,uCAAuC,CAAC;AAC3E;AAEA,OAAO,MAAM+B,eAAe,SAASjD,eAAe,CAAC;EACjD,OAAOC,oBAAoB,GAAG,CAACkB,2BAA2B,CAAC;AAC/D;AAEA,OAAO,MAAM+B,+BAA+B,SAASlD,eAAe,CAAC;EACjE,OAAOC,oBAAoB,GAAG,CAACmB,4CAA4C,CAAC;AAChF;AAEA,OAAO,MAAM+B,mBAAmB,SAASnD,eAAe,CAAC;EACrD,OAAOC,oBAAoB,GAAG,CAACoB,qCAAqC,CAAC;AACzE;AAEA,OAAO,MAAM+B,oCAAoC,SAASpD,eAAe,CAAC;EACtE,OAAOC,oBAAoB,GAAG,CAACqB,kDAAkD,CAAC;AACtF;AAEA,OAAO,MAAM+B,qCAAqC,SAASrD,eAAe,CAAC;EACvE,OAAOC,oBAAoB,GAAG,CAACW,mDAAmD,CAAC;AACvF;AAEA,OAAO,MAAM0C,wBAAwB,SAAStD,eAAe,CAAC;EAC1D,OAAOC,oBAAoB,GAAG,CAACsB,qCAAqC,CAAC;AACzE;AAEA,OAAO,MAAMgC,wBAAwB,SAASvD,eAAe,CAAC;EAC1D,OAAOC,oBAAoB,GAAG,CAACuB,sCAAsC,CAAC;AAC1E;AAEA,OAAO,MAAMgC,2BAA2B,SAASxD,eAAe,CAAC;EAC7D,OAAOC,oBAAoB,GAAG,CAACwB,wCAAwC,CAAC;AAC5E;AAEA,OAAO,MAAMgC,kCAAkC,SAASzD,eAAe,CAAC;EACpE,OAAOC,oBAAoB,GAAG,CAACyB,gDAAgD,CAAC;AACpF;;AAEA;;AAEA;AACA,OAAO,MAAM7lB,eAAe,SAASwJ,WAAW,CAAC;EAC7C;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;EACIjH,WAAWA,CAAC;IAAE3C,MAAM;IAAEX,eAAe;IAAED,eAAe;IAAE2I,kBAAkB,GAAG,IAAI;IAAEC,gBAAgB,GAAG;EAAK,CAAC,EAAE;IAC1G,KAAK,CAAC,CAAC;IACP,IAAI,CAAChI,MAAM,GAAGA,MAAM;IACpB,IAAI,CAACX,eAAe,GAAGA,eAAe;IACtC,IAAI,CAACD,eAAe,GAAGA,eAAe;IACtC,IAAI,CAAC2I,kBAAkB,GAAGA,kBAAkB;IAC5C,IAAI,CAACC,gBAAgB,GAAGA,gBAAgB;EAC5C;AACJ;;AAEA;AACA;AACA;AACA,OAAO,MAAMqC,wBAAwB,SAAST,WAAW,CAAC;EACtD;AACJ;AACA;AACA;EACIjH,WAAWA,CAAC;IAAE3C;EAAO,CAAC,EAAE;IACpB,KAAK,CAAC,CAAC;IACP,IAAI,CAACA,MAAM,GAAGA,MAAM;EACxB;AACJ;;AAEA;AACA;AACA;AACA,OAAO,MAAM4gB,aAAa,SAAShX,WAAW,CAAC;EAC3C;AACJ;AACA;AACA;AACA;EACIjH,WAAWA,CAAC;IAAE3C,MAAM;IAAEioB;EAAW,CAAC,EAAE;IAChC,KAAK,CAAC,CAAC;IACP,IAAI,CAACjoB,MAAM,GAAGA,MAAM;IACpB,IAAI,CAACioB,UAAU,GAAGA,UAAU;EAChC;AACJ;;AAEA;AACA;AACA;AACA,OAAO,MAAM1d,qBAAqB,SAASX,WAAW,CAAC;EACnD;AACJ;AACA;AACA;EACIjH,WAAWA,CAAC;IAAE3C;EAAO,CAAC,EAAE;IACpB,KAAK,CAAC,CAAC;IACP,IAAI,CAACA,MAAM,GAAGA,MAAM;EACxB;AACJ;;AAEA;AACA;AACA;AACA,OAAO,MAAMmK,cAAc,SAASP,WAAW,CAAC;EAC5C;AACJ;AACA;AACA;EACIjH,WAAWA,CAAC;IAAE3C;EAAO,CAAC,EAAE;IACpB,KAAK,CAAC,CAAC;IACP,IAAI,CAACA,MAAM,GAAGA,MAAM;EACxB;AACJ;;AAEA;AACA;AACA;AACA,OAAO,MAAMyK,4BAA4B,SAASb,WAAW,CAAC;EAC1D;AACJ;AACA;AACA;AACA;EACIjH,WAAWA,CAAC;IAAEulB,YAAY;IAAEC;EAAW,CAAC,EAAE;IACtC,KAAK,CAAC,CAAC;IACP,IAAI,CAACD,YAAY,GAAGA,YAAY;IAChC,IAAI,CAACC,UAAU,GAAGA,UAAU;EAChC;AACJ;;AAGA;AACA;AACA;AACA,OAAO,MAAMhJ,cAAc,SAASvV,WAAW,CAAC;EAC5C;AACJ;AACA;AACA;EACIjH,WAAWA,CAAC;IAAE3C;EAAO,CAAC,EAAE;IACpB,KAAK,CAAC,CAAC;IACP,IAAI,CAACA,MAAM,GAAGA,MAAM;EACxB;AACJ;;AAEA;AACA;AACA;AACA,OAAO,MAAMooB,sBAAsB,SAASxe,WAAW,CAAC;EACpD;AACJ;AACA;AACA;AACA;AACA;EACIjH,WAAWA,CAAC;IAAE3C,MAAM;IAAEX;EAAgB,CAAC,EAAE;IACrC,KAAK,CAAC,CAAC;IACP,IAAI,CAACW,MAAM,GAAGA,MAAM;IACpB,IAAI,CAACX,eAAe,GAAGA,eAAe;EAC1C;AACJ;AAEA,OAAO,MAAMua,kBAAkB,SAAShQ,WAAW,CAAC;EAChD;AACJ;AACA;AACA;EACIjH,WAAWA,CAAC;IAAE0lB;EAAO,CAAC,EAAE;IACpB,KAAK,CAAC,CAAC;IACP,IAAI,CAACA,MAAM,GAAGA,MAAM;EACxB;AACJ;;AAEA;AACA;AACA;AACA,OAAO,MAAMzE,eAAe,SAASha,WAAW,CAAC;EAC7C;AACJ;AACA;AACA;AACA;AACA;EACIjH,WAAWA,CAAC;IAAE8f,QAAQ;IAAED;EAAY,CAAC,EAAE;IACnC,KAAK,CAAC,CAAC;IACP,IAAI,CAACC,QAAQ,GAAGA,QAAQ;IACxB,IAAI,CAACD,WAAW,GAAGA,WAAW;EAClC;AACJ","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}
{"ast":null,"code":"/**\n * @file Helper module for `Tensor` processing.\n * \n * These functions and classes are only used internally, \n * meaning an end-user shouldn't need to access anything here.\n * \n * @module utils/tensor\n */\n\nimport { ONNX } from '../backends/onnx.js';\nimport { interpolate_data, permute_data } from './maths.js';\nconst DataTypeMap = Object.freeze({\n  float32: Float32Array,\n  float64: Float64Array,\n  string: Array,\n  // string[]\n  int8: Int8Array,\n  uint8: Uint8Array,\n  int16: Int16Array,\n  uint16: Uint16Array,\n  int32: Int32Array,\n  uint32: Uint32Array,\n  int64: BigInt64Array,\n  uint64: BigUint64Array,\n  bool: Uint8Array\n});\n\n/**\n * @typedef {keyof typeof DataTypeMap} DataType\n * @typedef {import('./maths.js').AnyTypedArray | any[]} DataArray\n */\n\nconst ONNXTensor = ONNX.Tensor;\nexport class Tensor {\n  /** @type {number[]} Dimensions of the tensor. */\n  dims;\n\n  /** @type {DataType} Type of the tensor. */\n  type;\n\n  /** @type {DataArray} The data stored in the tensor. */\n  data;\n\n  /** @type {number} The number of elements in the tensor. */\n  size;\n\n  /**\n   * Create a new Tensor or copy an existing Tensor.\n   * @param {[DataType, DataArray, number[]]|[import('onnxruntime-common').Tensor]} args\n   */\n  constructor(...args) {\n    if (args[0] instanceof ONNXTensor) {\n      // Create shallow copy\n      Object.assign(this, args[0]);\n    } else {\n      // Create new tensor\n      Object.assign(this, new ONNXTensor(/** @type {DataType} */args[0], /** @type {Exclude<import('./maths.js').AnyTypedArray, Uint8ClampedArray>} */args[1], args[2]));\n    }\n    return new Proxy(this, {\n      get: (obj, key) => {\n        if (typeof key === 'string') {\n          let index = Number(key);\n          if (Number.isInteger(index)) {\n            // key is an integer (i.e., index)\n            return obj._getitem(index);\n          }\n        }\n        // @ts-ignore\n        return obj[key];\n      },\n      set: (obj, key, value) => {\n        // TODO allow setting of data\n\n        // @ts-ignore\n        return obj[key] = value;\n      }\n    });\n  }\n\n  /**\n   * Returns an iterator object for iterating over the tensor data in row-major order.\n   * If the tensor has more than one dimension, the iterator will yield subarrays.\n   * @returns {Iterator} An iterator object for iterating over the tensor data in row-major order.\n   */\n  *[Symbol.iterator]() {\n    const [iterLength, ...iterDims] = this.dims;\n    if (iterDims.length > 0) {\n      const iterSize = iterDims.reduce((a, b) => a * b);\n      for (let i = 0; i < iterLength; ++i) {\n        yield this._subarray(i, iterSize, iterDims);\n      }\n    } else {\n      yield* this.data;\n    }\n  }\n\n  /**\n   * Index into a Tensor object.\n   * @param {number} index The index to access.\n   * @returns {Tensor} The data at the specified index.\n   */\n  _getitem(index) {\n    const [iterLength, ...iterDims] = this.dims;\n    index = safeIndex(index, iterLength);\n    if (iterDims.length > 0) {\n      const iterSize = iterDims.reduce((a, b) => a * b);\n      return this._subarray(index, iterSize, iterDims);\n    } else {\n      return new Tensor(this.type, [this.data[index]], iterDims);\n    }\n  }\n\n  /**\n   * @param {number|bigint} item The item to search for in the tensor\n   * @returns {number} The index of the first occurrence of item in the tensor data.\n   */\n  indexOf(item) {\n    for (let index = 0; index < this.data.length; ++index) {\n      // Note: == instead of === so we can match Ints with BigInts\n      if (this.data[index] == item) {\n        return index;\n      }\n    }\n    return -1;\n  }\n\n  /**\n   * @param {number} index \n   * @param {number} iterSize \n   * @param {any} iterDims \n   * @returns {Tensor}\n   */\n  _subarray(index, iterSize, iterDims) {\n    const o1 = index * iterSize;\n    const o2 = (index + 1) * iterSize;\n\n    // We use subarray if available (typed array), otherwise we use slice (normal array)\n    const data = 'subarray' in this.data ? this.data.subarray(o1, o2) : this.data.slice(o1, o2);\n    return new Tensor(this.type, data, iterDims);\n  }\n\n  /**\n   * Returns the value of this tensor as a standard JavaScript Number. This only works\n   * for tensors with one element. For other cases, see `Tensor.tolist()`.\n   * @returns {number|bigint} The value of this tensor as a standard JavaScript Number.\n   * @throws {Error} If the tensor has more than one element.\n   */\n  item() {\n    if (this.data.length !== 1) {\n      throw new Error(`a Tensor with ${this.data.length} elements cannot be converted to Scalar`);\n    }\n    return this.data[0];\n  }\n\n  /**\n   * Convert tensor data to a n-dimensional JS list\n   * @returns {Array}\n   */\n  tolist() {\n    return reshape(this.data, this.dims);\n  }\n\n  /**\n   * Return a new Tensor with the sigmoid function applied to each element.\n   * @returns {Tensor} The tensor with the sigmoid function applied.\n   */\n  sigmoid() {\n    return this.clone().sigmoid_();\n  }\n\n  /**\n   * Applies the sigmoid function to the tensor in place.\n   * @returns {Tensor} Returns `this`.\n   */\n  sigmoid_() {\n    for (let i = 0; i < this.data.length; ++i) {\n      this.data[i] = 1 / (1 + Math.exp(-this.data[i]));\n    }\n    return this;\n  }\n\n  /**\n   * Return a new Tensor with every element multiplied by a constant.\n   * @param {number} val The value to multiply by.\n   * @returns {Tensor} The new tensor.\n   */\n  mul(val) {\n    return this.clone().mul_(val);\n  }\n\n  /**\n   * Multiply the tensor by a constant in place.\n   * @param {number} val The value to multiply by.\n   * @returns {Tensor} Returns `this`.\n   */\n  mul_(val) {\n    for (let i = 0; i < this.data.length; ++i) {\n      this.data[i] *= val;\n    }\n    return this;\n  }\n\n  /**\n   * Return a new Tensor with every element added by a constant.\n   * @param {number} val The value to add by.\n   * @returns {Tensor} The new tensor.\n   */\n  add(val) {\n    return this.clone().add_(val);\n  }\n\n  /**\n   * Add the tensor by a constant in place.\n   * @param {number} val The value to add by.\n   * @returns {Tensor} Returns `this`.\n   */\n  add_(val) {\n    for (let i = 0; i < this.data.length; ++i) {\n      this.data[i] += val;\n    }\n    return this;\n  }\n  clone() {\n    return new Tensor(this.type, this.data.slice(), this.dims.slice());\n  }\n  slice(...slices) {\n    // This allows for slicing with ranges and numbers\n    let newTensorDims = [];\n    let newOffsets = [];\n\n    // slices is an array of numbers or arrays of numbers\n    // e.g., slices = [0, [1, 3], null, [0, 3]]\n    for (let sliceIndex = 0; sliceIndex < this.dims.length; ++sliceIndex) {\n      let slice = slices[sliceIndex];\n      if (slice === null || slice === undefined) {\n        // null or undefined means take the whole dimension\n        newOffsets.push([0, this.dims[sliceIndex]]);\n        newTensorDims.push(this.dims[sliceIndex]);\n      } else if (typeof slice === 'number') {\n        slice = safeIndex(slice, this.dims[sliceIndex], sliceIndex);\n\n        // A number means take a single element\n        newOffsets.push([slice, slice + 1]);\n      } else if (Array.isArray(slice) && slice.length === 2) {\n        // An array of length 2 means take a range of elements\n\n        if (slice[0] > slice[1]) {\n          throw new Error(`Invalid slice: ${slice}`);\n        }\n        let offsets = [Math.max(slice[0], 0), Math.min(slice[1], this.dims[sliceIndex])];\n        newOffsets.push(offsets);\n        newTensorDims.push(offsets[1] - offsets[0]);\n      } else {\n        throw new Error(`Invalid slice: ${slice}`);\n      }\n    }\n    let newDims = newOffsets.map(([start, end]) => end - start);\n    let newBufferSize = newDims.reduce((a, b) => a * b);\n\n    // Allocate memory\n    // @ts-ignore\n    let data = new this.data.constructor(newBufferSize);\n\n    // Precompute strides\n    const stride = this.stride();\n    for (let i = 0; i < newBufferSize; ++i) {\n      let originalIndex = 0;\n      for (let j = newDims.length - 1, num = i; j >= 0; --j) {\n        const size = newDims[j];\n        originalIndex += (num % size + newOffsets[j][0]) * stride[j];\n        num = Math.floor(num / size);\n      }\n      data[i] = this.data[originalIndex];\n    }\n    return new Tensor(this.type, data, newTensorDims);\n  }\n\n  /**\n   * Return a permuted version of this Tensor, according to the provided dimensions.\n   * @param  {...number} dims Dimensions to permute.\n   * @returns {Tensor} The permuted tensor.\n   */\n  permute(...dims) {\n    return permute(this, dims);\n  }\n\n  // TODO: implement transpose. For now (backwards compatibility), it's just an alias for permute()\n  transpose(...dims) {\n    return this.permute(...dims);\n  }\n\n  // TODO add .max() and .min() methods\n\n  /**\n   * Returns the sum of each row of the input tensor in the given dimension dim.\n   * \n   * @param {number} [dim=null] The dimension or dimensions to reduce. If `null`, all dimensions are reduced.\n   * @param {boolean} keepdim Whether the output tensor has `dim` retained or not.\n   * @returns The summed tensor\n   */\n  sum(dim = null, keepdim = false) {\n    return this.norm(1, dim, keepdim);\n  }\n\n  /**\n   * Returns the matrix norm or vector norm of a given tensor.\n   * @param {number|string} [p='fro'] The order of norm\n   * @param {number} [dim=null] Specifies which dimension of the tensor to calculate the norm across.\n   * If dim is None, the norm will be calculated across all dimensions of input.\n   * @param {boolean} [keepdim=false] Whether the output tensors have dim retained or not.\n   * @returns {Tensor} The norm of the tensor.\n   */\n  norm(p = 'fro', dim = null, keepdim = false) {\n    if (p === 'fro') {\n      // NOTE: Since we only support integer dims, Frobenius norm produces the same result as p=2.\n      p = 2;\n    } else if (typeof p === 'string') {\n      throw Error(`Unsupported norm: ${p}`);\n    }\n    if (dim === null) {\n      // @ts-ignore\n      let val = this.data.reduce((a, b) => a + b ** p, 0) ** (1 / p);\n      return new Tensor(this.type, [val], []);\n    }\n\n    // Negative indexing\n    dim = safeIndex(dim, this.dims.length);\n\n    // Calculate the shape of the resulting array after summation\n    const resultDims = this.dims.slice(); // Copy the original dimensions\n    resultDims[dim] = 1; // Remove the specified axis\n\n    // Create a new array to store the accumulated values\n    // @ts-ignore\n    const result = new this.data.constructor(this.data.length / this.dims[dim]);\n\n    // Iterate over the data array\n    for (let i = 0; i < this.data.length; ++i) {\n      // Calculate the index in the resulting array\n      let resultIndex = 0;\n      for (let j = this.dims.length - 1, num = i, resultMultiplier = 1; j >= 0; --j) {\n        const size = this.dims[j];\n        if (j !== dim) {\n          const index = num % size;\n          resultIndex += index * resultMultiplier;\n          resultMultiplier *= resultDims[j];\n        }\n        num = Math.floor(num / size);\n      }\n\n      // Accumulate the value at the current index\n      result[resultIndex] += this.data[i] ** p;\n    }\n    if (p !== 1) {\n      for (let i = 0; i < result.length; ++i) {\n        result[i] = result[i] ** (1 / p);\n      }\n    }\n    if (!keepdim) {\n      resultDims.splice(dim, 1);\n    }\n    return new Tensor(this.type, result, resultDims);\n  }\n\n  /**\n   * Performs `L_p` normalization of inputs over specified dimension. Operates in place.\n   * @param {number} [p=2] The exponent value in the norm formulation\n   * @param {number} [dim=1] The dimension to reduce\n   * @returns {Tensor} `this` for operation chaining.\n   */\n  normalize_(p = 2.0, dim = 1) {\n    dim = safeIndex(dim, this.dims.length);\n    const norm = this.norm(p, dim, true);\n    for (let i = 0; i < this.data.length; ++i) {\n      // Calculate the index in the resulting array\n      let resultIndex = 0;\n      for (let j = this.dims.length - 1, num = i, resultMultiplier = 1; j >= 0; --j) {\n        const size = this.dims[j];\n        if (j !== dim) {\n          const index = num % size;\n          resultIndex += index * resultMultiplier;\n          resultMultiplier *= this.dims[j];\n        }\n        num = Math.floor(num / size);\n      }\n\n      // Divide by normalized value\n      this.data[i] /= norm.data[resultIndex];\n    }\n    return this;\n  }\n\n  /**\n   * Performs `L_p` normalization of inputs over specified dimension.\n   * @param {number} [p=2] The exponent value in the norm formulation\n   * @param {number} [dim=1] The dimension to reduce\n   * @returns {Tensor} The normalized tensor.\n   */\n  normalize(p = 2.0, dim = 1) {\n    return this.clone().normalize_(p, dim);\n  }\n\n  /**\n   * Compute and return the stride of this tensor.\n   * Stride is the jump necessary to go from one element to the next one in the specified dimension dim.\n   * @returns {number[]} The stride of this tensor.\n   */\n  stride() {\n    return dimsToStride(this.dims);\n  }\n\n  /**\n   * Returns a tensor with all specified dimensions of input of size 1 removed.\n   * \n   * NOTE: The returned tensor shares the storage with the input tensor, so changing the contents of one will change the contents of the other.\n   * If you would like a copy, use `tensor.clone()` before squeezing.\n   * \n   * @param {number} [dim=null] If given, the input will be squeezed only in the specified dimensions.\n   * @returns The squeezed tensor\n   */\n  squeeze(dim = null) {\n    return new Tensor(this.type, this.data, calc_squeeze_dims(this.dims, dim));\n  }\n\n  /**\n   * In-place version of @see {@link Tensor.squeeze}\n   */\n  squeeze_(dim = null) {\n    this.dims = calc_squeeze_dims(this.dims, dim);\n    return this;\n  }\n\n  /**\n   * Returns a new tensor with a dimension of size one inserted at the specified position.\n   * \n   * NOTE: The returned tensor shares the same underlying data with this tensor.\n   * \n   * @param {number} dim The index at which to insert the singleton dimension\n   * @returns The unsqueezed tensor\n   */\n  unsqueeze(dim = null) {\n    return new Tensor(this.type, this.data, calc_unsqueeze_dims(this.dims, dim));\n  }\n\n  /**\n   * In-place version of @see {@link Tensor.unsqueeze}\n   */\n  unsqueeze_(dim = null) {\n    this.dims = calc_unsqueeze_dims(this.dims, dim);\n    return this;\n  }\n\n  /**\n   * In-place version of @see {@link Tensor.flatten}\n   */\n  flatten_(start_dim = 0, end_dim = -1) {\n    // TODO validate inputs\n    end_dim = (end_dim + this.dims.length) % this.dims.length;\n    let dimsToKeepBefore = this.dims.slice(0, start_dim);\n    let dimsToFlatten = this.dims.slice(start_dim, end_dim + 1);\n    let dimsToKeepAfter = this.dims.slice(end_dim + 1);\n    this.dims = [...dimsToKeepBefore, dimsToFlatten.reduce((a, b) => a * b, 1), ...dimsToKeepAfter];\n    return this;\n  }\n\n  /**\n   * Flattens input by reshaping it into a one-dimensional tensor.\n   * If `start_dim` or `end_dim` are passed, only dimensions starting with `start_dim`\n   * and ending with `end_dim` are flattened. The order of elements in input is unchanged.\n   * @param {number} start_dim the first dim to flatten\n   * @param {number} end_dim the last dim to flatten\n   * @returns The flattened tensor.\n   */\n  flatten(start_dim = 0, end_dim = -1) {\n    return this.clone().flatten_(start_dim, end_dim);\n  }\n\n  /**\n   * Returns a new tensor with the same data as the `self` tensor but of a different `shape`.\n   * @param  {...number} dims the desired size\n   * @returns {Tensor} The tensor with the same data but different shape\n   */\n  view(...dims) {\n    // TODO: validate dims\n    let inferredIndex = -1;\n    for (let i = 0; i < dims.length; ++i) {\n      if (dims[i] === -1) {\n        if (inferredIndex !== -1) {\n          throw new Error(\"Only one dimension can be inferred\");\n        }\n        inferredIndex = i;\n      }\n    }\n    if (inferredIndex !== -1) {\n      // Some dimension must be inferred\n      const productOther = dims.reduce((product, curr, index) => {\n        return index !== inferredIndex ? product * curr : product;\n      }, 1);\n      dims[inferredIndex] = this.data.length / productOther;\n    }\n    return new Tensor(this.type, this.data, dims); // NOTE: uses same underlying storage\n  }\n  neg_() {\n    for (let i = 0; i < this.data.length; ++i) {\n      this.data[i] = -this.data[i];\n    }\n    return this;\n  }\n  neg() {\n    return this.clone().neg_();\n  }\n\n  /**\n   * In-place version of @see {@link Tensor.clamp}\n   */\n  clamp_(min, max) {\n    for (let i = 0; i < this.data.length; ++i) {\n      this.data[i] = Math.min(Math.max(this.data[i], min), max);\n    }\n    return this;\n  }\n\n  /**\n   * Clamps all elements in input into the range [ min, max ]\n   * @param {number} min lower-bound of the range to be clamped to\n   * @param {number} max upper-bound of the range to be clamped to\n   * @returns the output tensor.\n   */\n  clamp(min, max) {\n    return this.clone().clamp_(min, max);\n  }\n\n  /**\n   * In-place version of @see {@link Tensor.round}\n   */\n  round_() {\n    for (let i = 0; i < this.data.length; ++i) {\n      this.data[i] = Math.round(this.data[i]);\n    }\n    return this;\n  }\n\n  /**\n   * Rounds elements of input to the nearest integer.\n   * @returns the output tensor.\n   */\n  round() {\n    return this.clone().round_();\n  }\n\n  /**\n   * Performs Tensor dtype conversion.\n   * @param {DataType} type The desired data type.\n   * @returns {Tensor} The converted tensor.\n   */\n  to(type) {\n    // If the self Tensor already has the correct dtype, then self is returned.\n    if (this.type === type) return this;\n\n    // Otherwise, the returned tensor is a copy of self with the desired dtype.\n    if (!DataTypeMap.hasOwnProperty(type)) {\n      throw new Error(`Unsupported type: ${type}`);\n    }\n    // @ts-ignore\n    return new Tensor(type, DataTypeMap[type].from(this.data), this.dims);\n  }\n}\n\n/**\n * This creates a nested array of a given type and depth (see examples).\n * \n * @example\n *   NestArray<string, 1>; // string[]\n * @example\n *   NestArray<number, 2>; // number[][]\n * @example\n *   NestArray<string, 3>; // string[][][] etc.\n * @template T\n * @template {number} Depth\n * @template {never[]} [Acc=[]]\n * @typedef {Acc['length'] extends Depth ? T : NestArray<T[], Depth, [...Acc, never]>} NestArray\n */\n\n/**\n * Reshapes a 1-dimensional array into an n-dimensional array, according to the provided dimensions.\n *\n * @example\n *   reshape([10                    ], [1      ]); // Type: number[]      Value: [10]\n *   reshape([1, 2, 3, 4            ], [2, 2   ]); // Type: number[][]    Value: [[1, 2], [3, 4]]\n *   reshape([1, 2, 3, 4, 5, 6, 7, 8], [2, 2, 2]); // Type: number[][][]  Value: [[[1, 2], [3, 4]], [[5, 6], [7, 8]]]\n *   reshape([1, 2, 3, 4, 5, 6, 7, 8], [4, 2   ]); // Type: number[][]    Value: [[1, 2], [3, 4], [5, 6], [7, 8]]\n * @param {T[]|DataArray} data The input array to reshape.\n * @param {DIM} dimensions The target shape/dimensions.\n * @template T\n * @template {[number]|number[]} DIM\n * @returns {NestArray<T, DIM[\"length\"]>} The reshaped array.\n */\nfunction reshape(data, dimensions) {\n  const totalElements = data.length;\n  const dimensionSize = dimensions.reduce((a, b) => a * b);\n  if (totalElements !== dimensionSize) {\n    throw Error(`cannot reshape array of size ${totalElements} into shape (${dimensions})`);\n  }\n\n  /** @type {any} */\n  let reshapedArray = data;\n  for (let i = dimensions.length - 1; i >= 0; i--) {\n    reshapedArray = reshapedArray.reduce((acc, val) => {\n      let lastArray = acc[acc.length - 1];\n      if (lastArray.length < dimensions[i]) {\n        lastArray.push(val);\n      } else {\n        acc.push([val]);\n      }\n      return acc;\n    }, [[]]);\n  }\n  return reshapedArray[0];\n}\n\n/**\n * Permutes a tensor according to the provided axes.\n * @param {any} tensor The input tensor to permute.\n * @param {Array} axes The axes to permute the tensor along.\n * @returns {Tensor} The permuted tensor.\n */\nexport function permute(tensor, axes) {\n  const [permutedData, shape] = permute_data(tensor.data, tensor.dims, axes);\n  return new Tensor(tensor.type, permutedData, shape);\n}\n\n/**\n * Interpolates an Tensor to the given size.\n * @param {Tensor} input The input tensor to interpolate. Data must be channel-first (i.e., [c, h, w])\n * @param {number[]} size The output size of the image\n * @param {string} mode The interpolation mode\n * @param {boolean} align_corners Whether to align corners.\n * @returns {Tensor} The interpolated tensor.\n */\nexport function interpolate(input, [out_height, out_width], mode = 'bilinear', align_corners = false) {\n  // Input image dimensions\n  const in_channels = input.dims.at(-3) ?? 1;\n  const in_height = input.dims.at(-2);\n  const in_width = input.dims.at(-1);\n  let output = interpolate_data(/** @type {import('./maths.js').TypedArray}*/input.data, [in_channels, in_height, in_width], [out_height, out_width], mode, align_corners);\n  return new Tensor(input.type, output, [in_channels, out_height, out_width]);\n}\n\n/**\n * Perform mean pooling of the last hidden state followed by a normalization step.\n * @param {Tensor} last_hidden_state Tensor of shape [batchSize, seqLength, embedDim]\n * @param {Tensor} attention_mask Tensor of shape [batchSize, seqLength]\n * @returns {Tensor} Returns a new Tensor of shape [batchSize, embedDim].\n */\nexport function mean_pooling(last_hidden_state, attention_mask) {\n  // last_hidden_state: [batchSize, seqLength, embedDim]\n  // attention_mask:    [batchSize, seqLength]\n\n  let shape = [last_hidden_state.dims[0], last_hidden_state.dims[2]];\n  // @ts-ignore\n  let returnedData = new last_hidden_state.data.constructor(shape[0] * shape[1]);\n  let [batchSize, seqLength, embedDim] = last_hidden_state.dims;\n  let outIndex = 0;\n  for (let i = 0; i < batchSize; ++i) {\n    let offset = i * embedDim * seqLength;\n    for (let k = 0; k < embedDim; ++k) {\n      let sum = 0;\n      let count = 0;\n      let attnMaskOffset = i * seqLength;\n      let offset2 = offset + k;\n      // Pool over all words in sequence\n      for (let j = 0; j < seqLength; ++j) {\n        // index into attention mask\n        let attn = Number(attention_mask.data[attnMaskOffset + j]);\n        count += attn;\n        sum += last_hidden_state.data[offset2 + j * embedDim] * attn;\n      }\n      let avg = sum / count;\n      returnedData[outIndex++] = avg;\n    }\n  }\n  return new Tensor(last_hidden_state.type, returnedData, shape);\n}\n\n/**\n * Apply Layer Normalization for last certain number of dimensions.\n * @param {Tensor} input The input tensor\n * @param {number[]} normalized_shape input shape from an expected input of size\n * @param {Object} options The options for the layer normalization\n * @param {number} [options.eps=1e-5] A value added to the denominator for numerical stability.\n * @returns {Tensor} The normalized tensor.\n */\nexport function layer_norm(input, normalized_shape, {\n  eps = 1e-5\n} = {}) {\n  if (input.dims.length !== 2) {\n    throw new Error('`layer_norm` currently only supports 2D input.');\n  }\n  const [batchSize, featureDim] = input.dims;\n  if (normalized_shape.length !== 1 && normalized_shape[0] !== featureDim) {\n    throw new Error('`normalized_shape` must be a 1D array with shape `[input.dims[1]]`.');\n  }\n  const [std, mean] = std_mean(input, 1, 0, true);\n\n  // @ts-ignore\n  const returnedData = new input.data.constructor(input.data.length);\n  for (let i = 0; i < batchSize; ++i) {\n    const offset = i * featureDim;\n    for (let j = 0; j < featureDim; ++j) {\n      const offset2 = offset + j;\n      returnedData[offset2] = (input.data[offset2] - mean.data[i]) / (std.data[i] + eps);\n    }\n  }\n  return new Tensor(input.type, returnedData, input.dims);\n}\n\n/**\n * Helper function to calculate new dimensions when performing a squeeze operation.\n * @param {number[]} dims The dimensions of the tensor.\n * @param {number|number[]|null} dim The dimension(s) to squeeze.\n * @returns The new dimensions.\n * @private\n */\nfunction calc_squeeze_dims(dims, dim) {\n  dims = dims.slice();\n  if (dim === null) {\n    dims = dims.filter(d => d !== 1);\n  } else if (typeof dim === 'number') {\n    if (dims[dim] === 1) {\n      dims.splice(dim, 1);\n    }\n  } else if (Array.isArray(dim)) {\n    dims = dims.filter((x, i) => {\n      return x !== 1 || !dim.includes(i);\n    });\n  }\n  return dims;\n}\n\n/**\n * Helper function to calculate new dimensions when performing an unsqueeze operation.\n * @param {number[]} dims The dimensions of the tensor.\n * @param {number} dim The dimension to unsqueeze.\n * @returns The new dimensions.\n * @private\n */\nfunction calc_unsqueeze_dims(dims, dim) {\n  // Dimension out of range (e.g., \"expected to be in range of [-4, 3], but got 4\")\n  // + 1 since we allow inserting at the end (i.e. dim = -1)\n  dim = safeIndex(dim, dims.length + 1);\n  dims = dims.slice();\n  // Insert 1 into specified dimension\n  dims.splice(dim, 0, 1);\n  return dims;\n}\n\n/**\n * Safely calculate the index for an array of a given size, allowing negative indexing.\n * @param {number} index The index that will be used.\n * @param {number} size The size of the array.\n * @param {number} [dimension=null] The dimension that the index is for (optional).\n * @returns {number} The index, guaranteed to be non-negative and less than `arrayLength`.\n * \n * @throws {Error} If the index is out of range.\n * @private\n */\nfunction safeIndex(index, size, dimension = null) {\n  if (index < -size || index >= size) {\n    throw new Error(`IndexError: index ${index} is out of bounds for dimension${dimension === null ? '' : ' ' + dimension} with size ${size}`);\n  }\n  if (index < 0) {\n    // Negative indexing, ensuring positive index\n    index = (index % size + size) % size;\n  }\n  return index;\n}\n\n/**\n * Concatenates an array of tensors along a specified dimension.\n * @param {Tensor[]} tensors The array of tensors to concatenate.\n * @param {number} dim The dimension to concatenate along.\n * @returns {Tensor} The concatenated tensor.\n */\nexport function cat(tensors, dim = 0) {\n  dim = safeIndex(dim, tensors[0].dims.length);\n\n  // TODO do validation of shapes\n\n  const resultDims = tensors[0].dims.slice();\n  resultDims[dim] = tensors.reduce((a, b) => a + b.dims[dim], 0);\n\n  // Create a new array to store the accumulated values\n  const resultSize = resultDims.reduce((a, b) => a * b, 1);\n  // @ts-ignore\n  const result = new tensors[0].data.constructor(resultSize);\n\n  // Create output tensor of same type as first\n  const resultType = tensors[0].type;\n  if (dim === 0) {\n    // Handle special case for performance reasons\n\n    let offset = 0;\n    for (let t of tensors) {\n      result.set(t.data, offset);\n      offset += t.data.length;\n    }\n  } else {\n    let currentDim = 0;\n    for (let t = 0; t < tensors.length; ++t) {\n      let tensor = tensors[t];\n\n      // Iterate over the data array\n      for (let i = 0; i < tensor.data.length; ++i) {\n        // Calculate the index in the resulting array\n        let resultIndex = 0;\n        for (let j = tensor.dims.length - 1, num = i, resultMultiplier = 1; j >= 0; --j) {\n          const size = tensor.dims[j];\n          let index = num % size;\n          if (j === dim) {\n            index += currentDim;\n          }\n          resultIndex += index * resultMultiplier;\n          resultMultiplier *= resultDims[j];\n          num = Math.floor(num / size);\n        }\n        // Accumulate the value at the current index\n        result[resultIndex] = tensor.data[i];\n      }\n      currentDim += tensor.dims[dim];\n    }\n  }\n  return new Tensor(resultType, result, resultDims);\n}\n\n/**\n * Stack an array of tensors along a specified dimension.\n * @param {Tensor[]} tensors The array of tensors to stack.\n * @param {number} dim The dimension to stack along.\n * @returns {Tensor} The stacked tensor.\n */\nexport function stack(tensors, dim = 0) {\n  // TODO do validation of shapes\n  // NOTE: stack expects each tensor to be equal size\n  return cat(tensors.map(t => t.unsqueeze(dim)), dim);\n}\n\n/**\n * Calculates the standard deviation and mean over the dimensions specified by dim. dim can be a single dimension or `null` to reduce over all dimensions.\n * @param {Tensor} input the input tenso\n * @param {number|null} dim the dimension to reduce. If None, all dimensions are reduced.\n * @param {number} correction difference between the sample size and sample degrees of freedom. Defaults to Bessel's correction, correction=1.\n * @param {boolean} keepdim whether the output tensor has dim retained or not.\n * @returns {Tensor[]} A tuple of (std, mean) tensors.\n */\nexport function std_mean(input, dim = null, correction = 1, keepdim = false) {\n  if (dim === null) {\n    // None to reduce over all dimensions.\n    // @ts-ignore\n    const sum = input.data.reduce((a, b) => a + b, 0);\n    const mean = sum / input.data.length;\n    // @ts-ignore\n    const std = Math.sqrt(input.data.reduce((a, b) => a + (b - mean) ** 2, 0) / (input.data.length - correction));\n    const meanTensor = new Tensor(input.type, [mean], [/* scalar */]);\n    const stdTensor = new Tensor(input.type, [std], [/* scalar */]);\n    return [stdTensor, meanTensor];\n  }\n\n  // Negative indexing\n  dim = safeIndex(dim, input.dims.length);\n  const meanTensor = mean(input, dim, keepdim);\n\n  // Calculate the shape of the resulting array after summation\n  const resultDims = input.dims.slice(); // Copy the original dimensions\n  resultDims[dim] = 1; // Remove the specified axis\n\n  // Create a new array to store the accumulated values\n  // @ts-ignore\n  const result = new input.data.constructor(input.data.length / input.dims[dim]);\n\n  // Iterate over the data array\n  for (let i = 0; i < input.data.length; ++i) {\n    // Calculate the index in the resulting array\n    let resultIndex = 0;\n    for (let j = input.dims.length - 1, num = i, resultMultiplier = 1; j >= 0; --j) {\n      const size = input.dims[j];\n      if (j !== dim) {\n        const index = num % size;\n        resultIndex += index * resultMultiplier;\n        resultMultiplier *= resultDims[j];\n      }\n      num = Math.floor(num / size);\n    }\n\n    // Accumulate the value at the current index\n    result[resultIndex] += (input.data[i] - meanTensor.data[resultIndex]) ** 2;\n  }\n  for (let i = 0; i < result.length; ++i) {\n    result[i] = Math.sqrt(result[i] / (input.dims[dim] - correction));\n  }\n  if (!keepdim) {\n    resultDims.splice(dim, 1);\n  }\n  const stdTensor = new Tensor(input.type, result, resultDims);\n  return [stdTensor, meanTensor];\n}\n\n/**\n * Returns the mean value of each row of the input tensor in the given dimension dim.\n * @param {Tensor} input the input tensor.\n * @param {number|null} dim the dimension to reduce.\n * @param {boolean} keepdim whether the output tensor has dim retained or not.\n * @returns A new tensor with means taken along the specified dimension.\n */\nexport function mean(input, dim = null, keepdim = false) {\n  if (dim === null) {\n    // None to reduce over all dimensions.\n    // @ts-ignore\n    let val = input.data.reduce((a, b) => a + b, 0);\n    return new Tensor(input.type, [val / input.data.length], [/* scalar */]);\n  }\n\n  // Negative indexing\n  dim = safeIndex(dim, input.dims.length);\n\n  // Calculate the shape of the resulting array after summation\n  const resultDims = input.dims.slice(); // Copy the original dimensions\n  resultDims[dim] = 1; // Remove the specified axis\n\n  // Create a new array to store the accumulated values\n  // @ts-ignore\n  const result = new input.data.constructor(input.data.length / input.dims[dim]);\n\n  // Iterate over the data array\n  for (let i = 0; i < input.data.length; ++i) {\n    // Calculate the index in the resulting array\n    let resultIndex = 0;\n    for (let j = input.dims.length - 1, num = i, resultMultiplier = 1; j >= 0; --j) {\n      const size = input.dims[j];\n      if (j !== dim) {\n        const index = num % size;\n        resultIndex += index * resultMultiplier;\n        resultMultiplier *= resultDims[j];\n      }\n      num = Math.floor(num / size);\n    }\n\n    // Accumulate the value at the current index\n    result[resultIndex] += input.data[i];\n  }\n  if (input.dims[dim] !== 1) {\n    for (let i = 0; i < result.length; ++i) {\n      result[i] = result[i] / input.dims[dim];\n    }\n  }\n  if (!keepdim) {\n    resultDims.splice(dim, 1);\n  }\n  return new Tensor(input.type, result, resultDims);\n}\n\n/**\n *\n * Measures similarity between two temporal sequences (e.g., input audio and output tokens\n * to generate token-level timestamps).\n * @param {Tensor} matrix \n * @returns {number[][]}\n */\nexport function dynamicTimeWarping(matrix) {\n  const [output_length, input_length] = matrix.dims;\n  const outputShape = [output_length + 1, input_length + 1];\n  const cost = new Tensor('float32', new Float32Array(outputShape[0] * outputShape[1]).fill(Infinity), outputShape);\n  const trace = new Tensor('float32', new Float32Array(outputShape[0] * outputShape[1]).fill(-1), outputShape);\n\n  // same as `cost[0][0] = 0`;\n  cost[0].data[0] = 0;\n  for (let j = 1; j < input_length + 1; ++j) {\n    for (let i = 1; i < output_length + 1; ++i) {\n      const c0 = cost[i - 1][j - 1].item();\n      const c1 = cost[i - 1][j].item();\n      const c2 = cost[i][j - 1].item();\n      let c, t;\n      if (c0 < c1 && c0 < c2) {\n        c = c0;\n        t = 0;\n      } else if (c1 < c0 && c1 < c2) {\n        c = c1;\n        t = 1;\n      } else {\n        c = c2;\n        t = 2;\n      }\n      cost[i].data[j] = matrix[i - 1][j - 1].item() + c;\n      trace[i].data[j] = t;\n    }\n  }\n\n  // backtrace\n  let i = output_length;\n  let j = input_length;\n\n  // @ts-ignore\n  trace.data.fill(2, 0, outputShape[1]); // trace[0, :] = 2\n  for (let i = 0; i < outputShape[0]; ++i) {\n    // trace[:, 0] = 1\n    trace[i].data[0] = 1;\n  }\n  let text_indices = [];\n  let time_indices = [];\n  while (i > 0 || j > 0) {\n    text_indices.push(i - 1);\n    time_indices.push(j - 1);\n    const t = trace[i][j].item();\n    switch (t) {\n      case 0:\n        --i;\n        --j;\n        break;\n      case 1:\n        --i;\n        break;\n      case 2:\n        --j;\n        break;\n      default:\n        throw new Error(`Internal error in dynamic time warping. Unexpected trace[${i}, ${j}]. Please file a bug report.`);\n    }\n  }\n  text_indices.reverse();\n  time_indices.reverse();\n  return [text_indices, time_indices];\n}\nfunction dimsToStride(dims) {\n  const stride = new Array(dims.length);\n  for (let i = dims.length - 1, s2 = 1; i >= 0; --i) {\n    stride[i] = s2;\n    s2 *= dims[i];\n  }\n  return stride;\n}\n\n/**\n * Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.\n * @param {number[]} size A sequence of integers defining the shape of the output tensor.\n */\nexport function ones(size) {\n  const numElements = size.reduce((a, b) => a * b, 1);\n  return new Tensor('int64', new BigInt64Array(numElements).fill(1n), size);\n}\n\n/**\n * Returns a tensor filled with the scalar value 1, with the same size as input.\n * @param {Tensor} tensor The size of input will determine size of the output tensor.\n * @returns The ones tensor.\n */\nexport function ones_like(tensor) {\n  return ones(tensor.dims);\n}\n\n/**\n * Quantizes the embeddings tensor to binary or unsigned binary precision.\n * @param {Tensor} tensor The tensor to quantize.\n * @param {'binary'|'ubinary'} precision The precision to use for quantization.\n * @returns {Tensor} The quantized tensor.\n */\nexport function quantize_embeddings(tensor, precision) {\n  if (tensor.dims.length !== 2) {\n    throw new Error(\"The tensor must have 2 dimensions\");\n  }\n  if (tensor.dims.at(-1) % 8 !== 0) {\n    throw new Error(\"The last dimension of the tensor must be a multiple of 8\");\n  }\n  if (!['binary', 'ubinary'].includes(precision)) {\n    throw new Error(\"The precision must be either 'binary' or 'ubinary'\");\n  }\n  const signed = precision === 'binary';\n  const dtype = signed ? 'int8' : 'uint8';\n\n  // Create a typed array to store the packed bits\n  const cls = signed ? Int8Array : Uint8Array;\n  const inputData = tensor.data;\n  const outputData = new cls(inputData.length / 8);\n\n  // Iterate over each number in the array\n  for (let i = 0; i < inputData.length; ++i) {\n    // Determine if the number is greater than 0\n    const bit = inputData[i] > 0 ? 1 : 0;\n\n    // Calculate the index in the typed array and the position within the byte\n    const arrayIndex = Math.floor(i / 8);\n    const bitPosition = i % 8;\n\n    // Pack the bit into the typed array\n    outputData[arrayIndex] |= bit << 7 - bitPosition;\n    if (signed && bitPosition === 0) {\n      outputData[arrayIndex] -= 128;\n    }\n  }\n  ;\n  return new Tensor(dtype, outputData, [tensor.dims[0], tensor.dims[1] / 8]);\n}","map":{"version":3,"names":["ONNX","interpolate_data","permute_data","DataTypeMap","Object","freeze","float32","Float32Array","float64","Float64Array","string","Array","int8","Int8Array","uint8","Uint8Array","int16","Int16Array","uint16","Uint16Array","int32","Int32Array","uint32","Uint32Array","int64","BigInt64Array","uint64","BigUint64Array","bool","ONNXTensor","Tensor","dims","type","data","size","constructor","args","assign","Proxy","get","obj","key","index","Number","isInteger","_getitem","set","value","Symbol","iterator","iterLength","iterDims","length","iterSize","reduce","a","b","i","_subarray","safeIndex","indexOf","item","o1","o2","subarray","slice","Error","tolist","reshape","sigmoid","clone","sigmoid_","Math","exp","mul","val","mul_","add","add_","slices","newTensorDims","newOffsets","sliceIndex","undefined","push","isArray","offsets","max","min","newDims","map","start","end","newBufferSize","stride","originalIndex","j","num","floor","permute","transpose","sum","dim","keepdim","norm","p","resultDims","result","resultIndex","resultMultiplier","splice","normalize_","normalize","dimsToStride","squeeze","calc_squeeze_dims","squeeze_","unsqueeze","calc_unsqueeze_dims","unsqueeze_","flatten_","start_dim","end_dim","dimsToKeepBefore","dimsToFlatten","dimsToKeepAfter","flatten","view","inferredIndex","productOther","product","curr","neg_","neg","clamp_","clamp","round_","round","to","hasOwnProperty","from","dimensions","totalElements","dimensionSize","reshapedArray","acc","lastArray","tensor","axes","permutedData","shape","interpolate","input","out_height","out_width","mode","align_corners","in_channels","at","in_height","in_width","output","mean_pooling","last_hidden_state","attention_mask","returnedData","batchSize","seqLength","embedDim","outIndex","offset","k","count","attnMaskOffset","offset2","attn","avg","layer_norm","normalized_shape","eps","featureDim","std","mean","std_mean","filter","d","x","includes","dimension","cat","tensors","resultSize","resultType","t","currentDim","stack","correction","sqrt","meanTensor","stdTensor","dynamicTimeWarping","matrix","output_length","input_length","outputShape","cost","fill","Infinity","trace","c0","c1","c2","c","text_indices","time_indices","reverse","s2","ones","numElements","ones_like","quantize_embeddings","precision","signed","dtype","cls","inputData","outputData","bit","arrayIndex","bitPosition"],"sources":["/Users/lorryrio/Project/calico/node_modules/@xenova/transformers/src/utils/tensor.js"],"sourcesContent":["/**\n * @file Helper module for `Tensor` processing.\n * \n * These functions and classes are only used internally, \n * meaning an end-user shouldn't need to access anything here.\n * \n * @module utils/tensor\n */\n\nimport { ONNX } from '../backends/onnx.js';\n\nimport {\n    interpolate_data,\n    permute_data\n} from './maths.js';\n\n\nconst DataTypeMap = Object.freeze({\n    float32: Float32Array,\n    float64: Float64Array,\n    string: Array, // string[]\n    int8: Int8Array,\n    uint8: Uint8Array,\n    int16: Int16Array,\n    uint16: Uint16Array,\n    int32: Int32Array,\n    uint32: Uint32Array,\n    int64: BigInt64Array,\n    uint64: BigUint64Array,\n    bool: Uint8Array,\n});\n\n/**\n * @typedef {keyof typeof DataTypeMap} DataType\n * @typedef {import('./maths.js').AnyTypedArray | any[]} DataArray\n */\n\nconst ONNXTensor = ONNX.Tensor;\n\nexport class Tensor {\n    /** @type {number[]} Dimensions of the tensor. */\n    dims;\n\n    /** @type {DataType} Type of the tensor. */\n    type;\n\n    /** @type {DataArray} The data stored in the tensor. */\n    data;\n\n    /** @type {number} The number of elements in the tensor. */\n    size;\n\n    /**\n     * Create a new Tensor or copy an existing Tensor.\n     * @param {[DataType, DataArray, number[]]|[import('onnxruntime-common').Tensor]} args\n     */\n    constructor(...args) {\n        if (args[0] instanceof ONNXTensor) {\n            // Create shallow copy\n            Object.assign(this, args[0]);\n\n        } else {\n            // Create new tensor\n            Object.assign(this, new ONNXTensor(\n                /** @type {DataType} */(args[0]),\n                /** @type {Exclude<import('./maths.js').AnyTypedArray, Uint8ClampedArray>} */(args[1]),\n                args[2]\n            ));\n        }\n\n        return new Proxy(this, {\n            get: (obj, key) => {\n                if (typeof key === 'string') {\n                    let index = Number(key);\n                    if (Number.isInteger(index)) {\n                        // key is an integer (i.e., index)\n                        return obj._getitem(index);\n                    }\n                }\n                // @ts-ignore\n                return obj[key];\n            },\n            set: (obj, key, value) => {\n                // TODO allow setting of data\n\n                // @ts-ignore\n                return obj[key] = value;\n            }\n        });\n    }\n\n    /**\n     * Returns an iterator object for iterating over the tensor data in row-major order.\n     * If the tensor has more than one dimension, the iterator will yield subarrays.\n     * @returns {Iterator} An iterator object for iterating over the tensor data in row-major order.\n     */\n    *[Symbol.iterator]() {\n        const [iterLength, ...iterDims] = this.dims;\n\n        if (iterDims.length > 0) {\n            const iterSize = iterDims.reduce((a, b) => a * b);\n            for (let i = 0; i < iterLength; ++i) {\n                yield this._subarray(i, iterSize, iterDims);\n            }\n        } else {\n            yield* this.data\n        }\n\n    }\n\n    /**\n     * Index into a Tensor object.\n     * @param {number} index The index to access.\n     * @returns {Tensor} The data at the specified index.\n     */\n    _getitem(index) {\n        const [iterLength, ...iterDims] = this.dims;\n\n        index = safeIndex(index, iterLength);\n\n        if (iterDims.length > 0) {\n            const iterSize = iterDims.reduce((a, b) => a * b);\n            return this._subarray(index, iterSize, iterDims);\n        } else {\n            return new Tensor(this.type, [this.data[index]], iterDims);\n        }\n    }\n\n    /**\n     * @param {number|bigint} item The item to search for in the tensor\n     * @returns {number} The index of the first occurrence of item in the tensor data.\n     */\n    indexOf(item) {\n        for (let index = 0; index < this.data.length; ++index) {\n            // Note: == instead of === so we can match Ints with BigInts\n            if (this.data[index] == item) {\n                return index;\n            }\n        }\n        return -1;\n    }\n\n    /**\n     * @param {number} index \n     * @param {number} iterSize \n     * @param {any} iterDims \n     * @returns {Tensor}\n     */\n    _subarray(index, iterSize, iterDims) {\n        const o1 = index * iterSize;\n        const o2 = (index + 1) * iterSize;\n\n        // We use subarray if available (typed array), otherwise we use slice (normal array)\n        const data =\n            ('subarray' in this.data)\n                ? this.data.subarray(o1, o2)\n                : this.data.slice(o1, o2);\n        return new Tensor(this.type, data, iterDims);\n    }\n\n    /**\n     * Returns the value of this tensor as a standard JavaScript Number. This only works\n     * for tensors with one element. For other cases, see `Tensor.tolist()`.\n     * @returns {number|bigint} The value of this tensor as a standard JavaScript Number.\n     * @throws {Error} If the tensor has more than one element.\n     */\n    item() {\n        if (this.data.length !== 1) {\n            throw new Error(`a Tensor with ${this.data.length} elements cannot be converted to Scalar`);\n        }\n        return this.data[0];\n    }\n\n    /**\n     * Convert tensor data to a n-dimensional JS list\n     * @returns {Array}\n     */\n    tolist() {\n        return reshape(this.data, this.dims)\n    }\n\n    /**\n     * Return a new Tensor with the sigmoid function applied to each element.\n     * @returns {Tensor} The tensor with the sigmoid function applied.\n     */\n    sigmoid() {\n        return this.clone().sigmoid_();\n    }\n\n    /**\n     * Applies the sigmoid function to the tensor in place.\n     * @returns {Tensor} Returns `this`.\n     */\n    sigmoid_() {\n        for (let i = 0; i < this.data.length; ++i) {\n            this.data[i] = 1 / (1 + Math.exp(-this.data[i]));\n        }\n        return this;\n    }\n\n    /**\n     * Return a new Tensor with every element multiplied by a constant.\n     * @param {number} val The value to multiply by.\n     * @returns {Tensor} The new tensor.\n     */\n    mul(val) {\n        return this.clone().mul_(val);\n    }\n\n    /**\n     * Multiply the tensor by a constant in place.\n     * @param {number} val The value to multiply by.\n     * @returns {Tensor} Returns `this`.\n     */\n    mul_(val) {\n        for (let i = 0; i < this.data.length; ++i) {\n            this.data[i] *= val;\n        }\n        return this;\n    }\n\n\n    /**\n     * Return a new Tensor with every element added by a constant.\n     * @param {number} val The value to add by.\n     * @returns {Tensor} The new tensor.\n     */\n    add(val) {\n        return this.clone().add_(val);\n    }\n\n    /**\n     * Add the tensor by a constant in place.\n     * @param {number} val The value to add by.\n     * @returns {Tensor} Returns `this`.\n     */\n    add_(val) {\n        for (let i = 0; i < this.data.length; ++i) {\n            this.data[i] += val;\n        }\n        return this;\n    }\n    clone() {\n        return new Tensor(this.type, this.data.slice(), this.dims.slice());\n    }\n\n    slice(...slices) {\n        // This allows for slicing with ranges and numbers\n        let newTensorDims = [];\n        let newOffsets = [];\n\n        // slices is an array of numbers or arrays of numbers\n        // e.g., slices = [0, [1, 3], null, [0, 3]]\n        for (let sliceIndex = 0; sliceIndex < this.dims.length; ++sliceIndex) {\n            let slice = slices[sliceIndex];\n\n            if (slice === null || slice === undefined) {\n                // null or undefined means take the whole dimension\n                newOffsets.push([0, this.dims[sliceIndex]]);\n                newTensorDims.push(this.dims[sliceIndex]);\n\n            } else if (typeof slice === 'number') {\n                slice = safeIndex(slice, this.dims[sliceIndex], sliceIndex);\n\n                // A number means take a single element\n                newOffsets.push([slice, slice + 1]);\n\n            } else if (Array.isArray(slice) && slice.length === 2) {\n                // An array of length 2 means take a range of elements\n\n                if (slice[0] > slice[1]) {\n                    throw new Error(`Invalid slice: ${slice}`);\n                }\n\n                let offsets = [\n                    Math.max(slice[0], 0),\n                    Math.min(slice[1], this.dims[sliceIndex])\n                ];\n\n                newOffsets.push(offsets);\n                newTensorDims.push(offsets[1] - offsets[0]);\n\n            } else {\n                throw new Error(`Invalid slice: ${slice}`);\n            }\n        }\n\n        let newDims = newOffsets.map(([start, end]) => end - start);\n        let newBufferSize = newDims.reduce((a, b) => a * b);\n\n        // Allocate memory\n        // @ts-ignore\n        let data = new this.data.constructor(newBufferSize);\n\n        // Precompute strides\n        const stride = this.stride();\n\n        for (let i = 0; i < newBufferSize; ++i) {\n            let originalIndex = 0;\n            for (let j = newDims.length - 1, num = i; j >= 0; --j) {\n                const size = newDims[j];\n                originalIndex += ((num % size) + newOffsets[j][0]) * stride[j];\n                num = Math.floor(num / size);\n            }\n            data[i] = this.data[originalIndex];\n        }\n        return new Tensor(this.type, data, newTensorDims);\n\n    }\n\n    /**\n     * Return a permuted version of this Tensor, according to the provided dimensions.\n     * @param  {...number} dims Dimensions to permute.\n     * @returns {Tensor} The permuted tensor.\n     */\n    permute(...dims) {\n        return permute(this, dims);\n    }\n\n    // TODO: implement transpose. For now (backwards compatibility), it's just an alias for permute()\n    transpose(...dims) {\n        return this.permute(...dims);\n    }\n\n    // TODO add .max() and .min() methods\n\n    /**\n     * Returns the sum of each row of the input tensor in the given dimension dim.\n     * \n     * @param {number} [dim=null] The dimension or dimensions to reduce. If `null`, all dimensions are reduced.\n     * @param {boolean} keepdim Whether the output tensor has `dim` retained or not.\n     * @returns The summed tensor\n     */\n    sum(dim = null, keepdim = false) {\n        return this.norm(1, dim, keepdim);\n    }\n\n    /**\n     * Returns the matrix norm or vector norm of a given tensor.\n     * @param {number|string} [p='fro'] The order of norm\n     * @param {number} [dim=null] Specifies which dimension of the tensor to calculate the norm across.\n     * If dim is None, the norm will be calculated across all dimensions of input.\n     * @param {boolean} [keepdim=false] Whether the output tensors have dim retained or not.\n     * @returns {Tensor} The norm of the tensor.\n     */\n    norm(p = 'fro', dim = null, keepdim = false) {\n        if (p === 'fro') {\n            // NOTE: Since we only support integer dims, Frobenius norm produces the same result as p=2.\n            p = 2;\n        } else if (typeof p === 'string') {\n            throw Error(`Unsupported norm: ${p}`);\n        }\n\n        if (dim === null) {\n            // @ts-ignore\n            let val = this.data.reduce((a, b) => a + (b ** p), 0) ** (1 / p);\n            return new Tensor(this.type, [val], []);\n        }\n\n        // Negative indexing\n        dim = safeIndex(dim, this.dims.length);\n\n        // Calculate the shape of the resulting array after summation\n        const resultDims = this.dims.slice(); // Copy the original dimensions\n        resultDims[dim] = 1; // Remove the specified axis\n\n        // Create a new array to store the accumulated values\n        // @ts-ignore\n        const result = new this.data.constructor(this.data.length / this.dims[dim]);\n\n        // Iterate over the data array\n        for (let i = 0; i < this.data.length; ++i) {\n\n            // Calculate the index in the resulting array\n            let resultIndex = 0;\n\n            for (let j = this.dims.length - 1, num = i, resultMultiplier = 1; j >= 0; --j) {\n                const size = this.dims[j];\n                if (j !== dim) {\n                    const index = num % size;\n                    resultIndex += index * resultMultiplier;\n                    resultMultiplier *= resultDims[j];\n                }\n                num = Math.floor(num / size);\n            }\n\n            // Accumulate the value at the current index\n            result[resultIndex] += (this.data[i]) ** p;\n        }\n\n        if (p !== 1) {\n            for (let i = 0; i < result.length; ++i) {\n                result[i] = result[i] ** (1 / p);\n            }\n        }\n\n        if (!keepdim) {\n            resultDims.splice(dim, 1);\n        }\n\n        return new Tensor(this.type, result, resultDims);\n    }\n\n    /**\n     * Performs `L_p` normalization of inputs over specified dimension. Operates in place.\n     * @param {number} [p=2] The exponent value in the norm formulation\n     * @param {number} [dim=1] The dimension to reduce\n     * @returns {Tensor} `this` for operation chaining.\n     */\n    normalize_(p = 2.0, dim = 1) {\n        dim = safeIndex(dim, this.dims.length);\n\n        const norm = this.norm(p, dim, true);\n\n        for (let i = 0; i < this.data.length; ++i) {\n\n            // Calculate the index in the resulting array\n            let resultIndex = 0;\n\n            for (let j = this.dims.length - 1, num = i, resultMultiplier = 1; j >= 0; --j) {\n                const size = this.dims[j];\n                if (j !== dim) {\n                    const index = num % size;\n                    resultIndex += index * resultMultiplier;\n                    resultMultiplier *= this.dims[j];\n                }\n                num = Math.floor(num / size);\n            }\n\n            // Divide by normalized value\n            this.data[i] /= norm.data[resultIndex];\n        }\n\n        return this;\n    }\n\n    /**\n     * Performs `L_p` normalization of inputs over specified dimension.\n     * @param {number} [p=2] The exponent value in the norm formulation\n     * @param {number} [dim=1] The dimension to reduce\n     * @returns {Tensor} The normalized tensor.\n     */\n    normalize(p = 2.0, dim = 1) {\n        return this.clone().normalize_(p, dim);\n    }\n\n    /**\n     * Compute and return the stride of this tensor.\n     * Stride is the jump necessary to go from one element to the next one in the specified dimension dim.\n     * @returns {number[]} The stride of this tensor.\n     */\n    stride() {\n        return dimsToStride(this.dims);\n    }\n\n    /**\n     * Returns a tensor with all specified dimensions of input of size 1 removed.\n     * \n     * NOTE: The returned tensor shares the storage with the input tensor, so changing the contents of one will change the contents of the other.\n     * If you would like a copy, use `tensor.clone()` before squeezing.\n     * \n     * @param {number} [dim=null] If given, the input will be squeezed only in the specified dimensions.\n     * @returns The squeezed tensor\n     */\n    squeeze(dim = null) {\n        return new Tensor(\n            this.type,\n            this.data,\n            calc_squeeze_dims(this.dims, dim)\n        )\n    }\n\n    /**\n     * In-place version of @see {@link Tensor.squeeze}\n     */\n    squeeze_(dim = null) {\n        this.dims = calc_squeeze_dims(this.dims, dim);\n        return this;\n    }\n\n    /**\n     * Returns a new tensor with a dimension of size one inserted at the specified position.\n     * \n     * NOTE: The returned tensor shares the same underlying data with this tensor.\n     * \n     * @param {number} dim The index at which to insert the singleton dimension\n     * @returns The unsqueezed tensor\n     */\n    unsqueeze(dim = null) {\n        return new Tensor(\n            this.type,\n            this.data,\n            calc_unsqueeze_dims(this.dims, dim)\n        );\n    }\n\n    /**\n     * In-place version of @see {@link Tensor.unsqueeze}\n     */\n    unsqueeze_(dim = null) {\n        this.dims = calc_unsqueeze_dims(this.dims, dim);\n        return this;\n    }\n\n    /**\n     * In-place version of @see {@link Tensor.flatten}\n     */\n    flatten_(start_dim = 0, end_dim = -1) {\n        // TODO validate inputs\n        end_dim = (end_dim + this.dims.length) % this.dims.length;\n\n        let dimsToKeepBefore = this.dims.slice(0, start_dim);\n        let dimsToFlatten = this.dims.slice(start_dim, end_dim + 1);\n        let dimsToKeepAfter = this.dims.slice(end_dim + 1);\n\n        this.dims = [...dimsToKeepBefore, dimsToFlatten.reduce((a, b) => a * b, 1), ...dimsToKeepAfter]\n        return this;\n    }\n\n    /**\n     * Flattens input by reshaping it into a one-dimensional tensor.\n     * If `start_dim` or `end_dim` are passed, only dimensions starting with `start_dim`\n     * and ending with `end_dim` are flattened. The order of elements in input is unchanged.\n     * @param {number} start_dim the first dim to flatten\n     * @param {number} end_dim the last dim to flatten\n     * @returns The flattened tensor.\n     */\n    flatten(start_dim = 0, end_dim = -1) {\n        return this.clone().flatten_(start_dim, end_dim);\n    }\n\n    /**\n     * Returns a new tensor with the same data as the `self` tensor but of a different `shape`.\n     * @param  {...number} dims the desired size\n     * @returns {Tensor} The tensor with the same data but different shape\n     */\n    view(...dims) {\n        // TODO: validate dims\n        let inferredIndex = -1;\n        for (let i = 0; i < dims.length; ++i) {\n            if (dims[i] === -1) {\n                if (inferredIndex !== -1) {\n                    throw new Error(\"Only one dimension can be inferred\");\n                }\n                inferredIndex = i;\n            }\n        }\n\n        if (inferredIndex !== -1) {\n            // Some dimension must be inferred\n            const productOther = dims.reduce((product, curr, index) => {\n                return index !== inferredIndex ? product * curr : product\n            }, 1);\n\n            dims[inferredIndex] = this.data.length / productOther;\n        }\n        return new Tensor(this.type, this.data, dims); // NOTE: uses same underlying storage\n    }\n\n    neg_() {\n        for (let i = 0; i < this.data.length; ++i) {\n            this.data[i] = -this.data[i];\n        }\n        return this;\n    }\n    neg() {\n        return this.clone().neg_();\n    }\n\n    /**\n     * In-place version of @see {@link Tensor.clamp}\n     */\n    clamp_(min, max) {\n        for (let i = 0; i < this.data.length; ++i) {\n            this.data[i] = Math.min(Math.max(this.data[i], min), max);\n        }\n        return this;\n    }\n\n    /**\n     * Clamps all elements in input into the range [ min, max ]\n     * @param {number} min lower-bound of the range to be clamped to\n     * @param {number} max upper-bound of the range to be clamped to\n     * @returns the output tensor.\n     */\n    clamp(min, max) {\n        return this.clone().clamp_(min, max);\n    }\n\n    /**\n     * In-place version of @see {@link Tensor.round}\n     */\n    round_() {\n        for (let i = 0; i < this.data.length; ++i) {\n            this.data[i] = Math.round(this.data[i]);\n        }\n        return this;\n    }\n\n    /**\n     * Rounds elements of input to the nearest integer.\n     * @returns the output tensor.\n     */\n    round() {\n        return this.clone().round_();\n    }\n\n    /**\n     * Performs Tensor dtype conversion.\n     * @param {DataType} type The desired data type.\n     * @returns {Tensor} The converted tensor.\n     */\n    to(type) {\n        // If the self Tensor already has the correct dtype, then self is returned.\n        if (this.type === type) return this;\n\n        // Otherwise, the returned tensor is a copy of self with the desired dtype.\n        if (!DataTypeMap.hasOwnProperty(type)) {\n            throw new Error(`Unsupported type: ${type}`);\n        }\n        // @ts-ignore\n        return new Tensor(type, DataTypeMap[type].from(this.data), this.dims);\n    }\n}\n\n/**\n * This creates a nested array of a given type and depth (see examples).\n * \n * @example\n *   NestArray<string, 1>; // string[]\n * @example\n *   NestArray<number, 2>; // number[][]\n * @example\n *   NestArray<string, 3>; // string[][][] etc.\n * @template T\n * @template {number} Depth\n * @template {never[]} [Acc=[]]\n * @typedef {Acc['length'] extends Depth ? T : NestArray<T[], Depth, [...Acc, never]>} NestArray\n */\n\n/**\n * Reshapes a 1-dimensional array into an n-dimensional array, according to the provided dimensions.\n *\n * @example\n *   reshape([10                    ], [1      ]); // Type: number[]      Value: [10]\n *   reshape([1, 2, 3, 4            ], [2, 2   ]); // Type: number[][]    Value: [[1, 2], [3, 4]]\n *   reshape([1, 2, 3, 4, 5, 6, 7, 8], [2, 2, 2]); // Type: number[][][]  Value: [[[1, 2], [3, 4]], [[5, 6], [7, 8]]]\n *   reshape([1, 2, 3, 4, 5, 6, 7, 8], [4, 2   ]); // Type: number[][]    Value: [[1, 2], [3, 4], [5, 6], [7, 8]]\n * @param {T[]|DataArray} data The input array to reshape.\n * @param {DIM} dimensions The target shape/dimensions.\n * @template T\n * @template {[number]|number[]} DIM\n * @returns {NestArray<T, DIM[\"length\"]>} The reshaped array.\n */\nfunction reshape(data, dimensions) {\n\n    const totalElements = data.length;\n    const dimensionSize = dimensions.reduce((a, b) => a * b);\n\n    if (totalElements !== dimensionSize) {\n        throw Error(`cannot reshape array of size ${totalElements} into shape (${dimensions})`);\n    }\n\n    /** @type {any} */\n    let reshapedArray = data;\n\n    for (let i = dimensions.length - 1; i >= 0; i--) {\n        reshapedArray = reshapedArray.reduce((acc, val) => {\n            let lastArray = acc[acc.length - 1];\n\n            if (lastArray.length < dimensions[i]) {\n                lastArray.push(val);\n            } else {\n                acc.push([val]);\n            }\n\n            return acc;\n        }, [[]]);\n    }\n\n    return reshapedArray[0];\n}\n\n/**\n * Permutes a tensor according to the provided axes.\n * @param {any} tensor The input tensor to permute.\n * @param {Array} axes The axes to permute the tensor along.\n * @returns {Tensor} The permuted tensor.\n */\nexport function permute(tensor, axes) {\n    const [permutedData, shape] = permute_data(tensor.data, tensor.dims, axes);\n    return new Tensor(tensor.type, permutedData, shape);\n}\n\n\n/**\n * Interpolates an Tensor to the given size.\n * @param {Tensor} input The input tensor to interpolate. Data must be channel-first (i.e., [c, h, w])\n * @param {number[]} size The output size of the image\n * @param {string} mode The interpolation mode\n * @param {boolean} align_corners Whether to align corners.\n * @returns {Tensor} The interpolated tensor.\n */\nexport function interpolate(input, [out_height, out_width], mode = 'bilinear', align_corners = false) {\n\n    // Input image dimensions\n    const in_channels = input.dims.at(-3) ?? 1;\n    const in_height = input.dims.at(-2);\n    const in_width = input.dims.at(-1);\n\n    let output = interpolate_data(\n        /** @type {import('./maths.js').TypedArray}*/(input.data),\n        [in_channels, in_height, in_width],\n        [out_height, out_width],\n        mode,\n        align_corners\n    );\n    return new Tensor(input.type, output, [in_channels, out_height, out_width]);\n}\n\n/**\n * Perform mean pooling of the last hidden state followed by a normalization step.\n * @param {Tensor} last_hidden_state Tensor of shape [batchSize, seqLength, embedDim]\n * @param {Tensor} attention_mask Tensor of shape [batchSize, seqLength]\n * @returns {Tensor} Returns a new Tensor of shape [batchSize, embedDim].\n */\nexport function mean_pooling(last_hidden_state, attention_mask) {\n    // last_hidden_state: [batchSize, seqLength, embedDim]\n    // attention_mask:    [batchSize, seqLength]\n\n    let shape = [last_hidden_state.dims[0], last_hidden_state.dims[2]];\n    // @ts-ignore\n    let returnedData = new last_hidden_state.data.constructor(shape[0] * shape[1]);\n    let [batchSize, seqLength, embedDim] = last_hidden_state.dims;\n\n    let outIndex = 0;\n    for (let i = 0; i < batchSize; ++i) {\n        let offset = i * embedDim * seqLength;\n\n        for (let k = 0; k < embedDim; ++k) {\n            let sum = 0;\n            let count = 0;\n\n            let attnMaskOffset = i * seqLength;\n            let offset2 = offset + k;\n            // Pool over all words in sequence\n            for (let j = 0; j < seqLength; ++j) {\n                // index into attention mask\n                let attn = Number(attention_mask.data[attnMaskOffset + j]);\n\n                count += attn;\n                sum += last_hidden_state.data[offset2 + j * embedDim] * attn;\n            }\n\n            let avg = sum / count;\n            returnedData[outIndex++] = avg;\n        }\n    }\n\n    return new Tensor(\n        last_hidden_state.type,\n        returnedData,\n        shape\n    )\n}\n\n/**\n * Apply Layer Normalization for last certain number of dimensions.\n * @param {Tensor} input The input tensor\n * @param {number[]} normalized_shape input shape from an expected input of size\n * @param {Object} options The options for the layer normalization\n * @param {number} [options.eps=1e-5] A value added to the denominator for numerical stability.\n * @returns {Tensor} The normalized tensor.\n */\nexport function layer_norm(input, normalized_shape, {\n    eps = 1e-5,\n} = {}) {\n    if (input.dims.length !== 2) {\n        throw new Error('`layer_norm` currently only supports 2D input.');\n    }\n\n    const [batchSize, featureDim] = input.dims;\n\n    if (normalized_shape.length !== 1 && normalized_shape[0] !== featureDim) {\n        throw new Error('`normalized_shape` must be a 1D array with shape `[input.dims[1]]`.');\n    }\n\n    const [std, mean] = std_mean(input, 1, 0, true);\n\n    // @ts-ignore\n    const returnedData = new input.data.constructor(input.data.length);\n\n    for (let i = 0; i < batchSize; ++i) {\n        const offset = i * featureDim;\n        for (let j = 0; j < featureDim; ++j) {\n            const offset2 = offset + j;\n            returnedData[offset2] = (input.data[offset2] - mean.data[i]) / (std.data[i] + eps);\n        }\n    }\n    return new Tensor(input.type, returnedData, input.dims);\n}\n\n/**\n * Helper function to calculate new dimensions when performing a squeeze operation.\n * @param {number[]} dims The dimensions of the tensor.\n * @param {number|number[]|null} dim The dimension(s) to squeeze.\n * @returns The new dimensions.\n * @private\n */\nfunction calc_squeeze_dims(dims, dim) {\n    dims = dims.slice();\n    if (dim === null) {\n        dims = dims.filter((d) => d !== 1);\n    } else if (typeof dim === 'number') {\n        if (dims[dim] === 1) {\n            dims.splice(dim, 1);\n        }\n    } else if (Array.isArray(dim)) {\n        dims = dims.filter((x, i) => {\n            return x !== 1 || !dim.includes(i);\n        });\n    }\n    return dims;\n}\n\n/**\n * Helper function to calculate new dimensions when performing an unsqueeze operation.\n * @param {number[]} dims The dimensions of the tensor.\n * @param {number} dim The dimension to unsqueeze.\n * @returns The new dimensions.\n * @private\n */\nfunction calc_unsqueeze_dims(dims, dim) {\n    // Dimension out of range (e.g., \"expected to be in range of [-4, 3], but got 4\")\n    // + 1 since we allow inserting at the end (i.e. dim = -1)\n    dim = safeIndex(dim, dims.length + 1);\n    dims = dims.slice();\n    // Insert 1 into specified dimension\n    dims.splice(dim, 0, 1);\n    return dims;\n}\n\n/**\n * Safely calculate the index for an array of a given size, allowing negative indexing.\n * @param {number} index The index that will be used.\n * @param {number} size The size of the array.\n * @param {number} [dimension=null] The dimension that the index is for (optional).\n * @returns {number} The index, guaranteed to be non-negative and less than `arrayLength`.\n * \n * @throws {Error} If the index is out of range.\n * @private\n */\nfunction safeIndex(index, size, dimension = null) {\n    if (index < -size || index >= size) {\n        throw new Error(`IndexError: index ${index} is out of bounds for dimension${dimension === null ? '' : ' ' + dimension} with size ${size}`);\n    }\n\n    if (index < 0) {\n        // Negative indexing, ensuring positive index\n        index = ((index % size) + size) % size;\n    }\n    return index;\n}\n\n/**\n * Concatenates an array of tensors along a specified dimension.\n * @param {Tensor[]} tensors The array of tensors to concatenate.\n * @param {number} dim The dimension to concatenate along.\n * @returns {Tensor} The concatenated tensor.\n */\nexport function cat(tensors, dim = 0) {\n    dim = safeIndex(dim, tensors[0].dims.length);\n\n    // TODO do validation of shapes\n\n    const resultDims = tensors[0].dims.slice();\n    resultDims[dim] = tensors.reduce((a, b) => a + b.dims[dim], 0);\n\n    // Create a new array to store the accumulated values\n    const resultSize = resultDims.reduce((a, b) => a * b, 1);\n    // @ts-ignore\n    const result = new tensors[0].data.constructor(resultSize);\n\n    // Create output tensor of same type as first\n    const resultType = tensors[0].type;\n\n    if (dim === 0) {\n        // Handle special case for performance reasons\n\n        let offset = 0;\n        for (let t of tensors) {\n            result.set(t.data, offset);\n            offset += t.data.length;\n        }\n\n    } else {\n\n        let currentDim = 0;\n\n        for (let t = 0; t < tensors.length; ++t) {\n            let tensor = tensors[t];\n\n            // Iterate over the data array\n            for (let i = 0; i < tensor.data.length; ++i) {\n                // Calculate the index in the resulting array\n                let resultIndex = 0;\n\n                for (let j = tensor.dims.length - 1, num = i, resultMultiplier = 1; j >= 0; --j) {\n                    const size = tensor.dims[j];\n                    let index = num % size;\n                    if (j === dim) {\n                        index += currentDim;\n                    }\n                    resultIndex += index * resultMultiplier;\n                    resultMultiplier *= resultDims[j];\n                    num = Math.floor(num / size);\n                }\n                // Accumulate the value at the current index\n                result[resultIndex] = tensor.data[i];\n            }\n\n            currentDim += tensor.dims[dim];\n        }\n    }\n    return new Tensor(resultType, result, resultDims);\n}\n\n/**\n * Stack an array of tensors along a specified dimension.\n * @param {Tensor[]} tensors The array of tensors to stack.\n * @param {number} dim The dimension to stack along.\n * @returns {Tensor} The stacked tensor.\n */\nexport function stack(tensors, dim = 0) {\n    // TODO do validation of shapes\n    // NOTE: stack expects each tensor to be equal size\n    return cat(tensors.map(t => t.unsqueeze(dim)), dim);\n}\n\n\n/**\n * Calculates the standard deviation and mean over the dimensions specified by dim. dim can be a single dimension or `null` to reduce over all dimensions.\n * @param {Tensor} input the input tenso\n * @param {number|null} dim the dimension to reduce. If None, all dimensions are reduced.\n * @param {number} correction difference between the sample size and sample degrees of freedom. Defaults to Bessel's correction, correction=1.\n * @param {boolean} keepdim whether the output tensor has dim retained or not.\n * @returns {Tensor[]} A tuple of (std, mean) tensors.\n */\nexport function std_mean(input, dim = null, correction = 1, keepdim = false) {\n\n    if (dim === null) {\n        // None to reduce over all dimensions.\n        // @ts-ignore\n        const sum = input.data.reduce((a, b) => a + b, 0);\n        const mean = sum / input.data.length;\n        // @ts-ignore\n        const std = Math.sqrt(input.data.reduce((a, b) => a + (b - mean) ** 2, 0) / (input.data.length - correction));\n\n        const meanTensor = new Tensor(input.type, [mean], [/* scalar */]);\n        const stdTensor = new Tensor(input.type, [std], [/* scalar */]);\n\n        return [stdTensor, meanTensor];\n    }\n\n    // Negative indexing\n    dim = safeIndex(dim, input.dims.length);\n\n    const meanTensor = mean(input, dim, keepdim);\n\n    // Calculate the shape of the resulting array after summation\n    const resultDims = input.dims.slice(); // Copy the original dimensions\n    resultDims[dim] = 1; // Remove the specified axis\n\n    // Create a new array to store the accumulated values\n    // @ts-ignore\n    const result = new input.data.constructor(input.data.length / input.dims[dim]);\n\n    // Iterate over the data array\n    for (let i = 0; i < input.data.length; ++i) {\n\n        // Calculate the index in the resulting array\n        let resultIndex = 0;\n\n        for (let j = input.dims.length - 1, num = i, resultMultiplier = 1; j >= 0; --j) {\n            const size = input.dims[j];\n            if (j !== dim) {\n                const index = num % size;\n                resultIndex += index * resultMultiplier;\n                resultMultiplier *= resultDims[j];\n            }\n            num = Math.floor(num / size);\n        }\n\n        // Accumulate the value at the current index\n        result[resultIndex] += (input.data[i] - meanTensor.data[resultIndex]) ** 2;\n    }\n\n    for (let i = 0; i < result.length; ++i) {\n        result[i] = Math.sqrt(result[i] / (input.dims[dim] - correction));\n    }\n\n    if (!keepdim) {\n        resultDims.splice(dim, 1);\n    }\n\n    const stdTensor = new Tensor(input.type, result, resultDims);\n\n    return [stdTensor, meanTensor];\n}\n\n\n/**\n * Returns the mean value of each row of the input tensor in the given dimension dim.\n * @param {Tensor} input the input tensor.\n * @param {number|null} dim the dimension to reduce.\n * @param {boolean} keepdim whether the output tensor has dim retained or not.\n * @returns A new tensor with means taken along the specified dimension.\n */\nexport function mean(input, dim = null, keepdim = false) {\n\n    if (dim === null) {\n        // None to reduce over all dimensions.\n        // @ts-ignore\n        let val = input.data.reduce((a, b) => a + b, 0);\n        return new Tensor(input.type, [val / input.data.length], [/* scalar */]);\n    }\n\n    // Negative indexing\n    dim = safeIndex(dim, input.dims.length);\n\n    // Calculate the shape of the resulting array after summation\n    const resultDims = input.dims.slice(); // Copy the original dimensions\n    resultDims[dim] = 1; // Remove the specified axis\n\n    // Create a new array to store the accumulated values\n    // @ts-ignore\n    const result = new input.data.constructor(input.data.length / input.dims[dim]);\n\n    // Iterate over the data array\n    for (let i = 0; i < input.data.length; ++i) {\n\n        // Calculate the index in the resulting array\n        let resultIndex = 0;\n\n        for (let j = input.dims.length - 1, num = i, resultMultiplier = 1; j >= 0; --j) {\n            const size = input.dims[j];\n            if (j !== dim) {\n                const index = num % size;\n                resultIndex += index * resultMultiplier;\n                resultMultiplier *= resultDims[j];\n            }\n            num = Math.floor(num / size);\n        }\n\n        // Accumulate the value at the current index\n        result[resultIndex] += input.data[i];\n    }\n\n    if (input.dims[dim] !== 1) {\n        for (let i = 0; i < result.length; ++i) {\n            result[i] = result[i] / input.dims[dim];\n        }\n    }\n\n    if (!keepdim) {\n        resultDims.splice(dim, 1);\n    }\n\n    return new Tensor(input.type, result, resultDims);\n}\n\n\n/**\n *\n * Measures similarity between two temporal sequences (e.g., input audio and output tokens\n * to generate token-level timestamps).\n * @param {Tensor} matrix \n * @returns {number[][]}\n */\nexport function dynamicTimeWarping(matrix) {\n    const [output_length, input_length] = matrix.dims;\n\n    const outputShape = [output_length + 1, input_length + 1];\n\n    const cost = new Tensor(\n        'float32',\n        new Float32Array(outputShape[0] * outputShape[1]).fill(Infinity),\n        outputShape\n    );\n\n    const trace = new Tensor(\n        'float32',\n        new Float32Array(outputShape[0] * outputShape[1]).fill(-1),\n        outputShape\n    )\n\n    // same as `cost[0][0] = 0`;\n    cost[0].data[0] = 0;\n\n    for (let j = 1; j < input_length + 1; ++j) {\n        for (let i = 1; i < output_length + 1; ++i) {\n\n            const c0 = cost[i - 1][j - 1].item();\n            const c1 = cost[i - 1][j].item();\n            const c2 = cost[i][j - 1].item();\n\n            let c, t;\n            if (c0 < c1 && c0 < c2) {\n                c = c0;\n                t = 0;\n            } else if (c1 < c0 && c1 < c2) {\n                c = c1;\n                t = 1;\n            } else {\n                c = c2;\n                t = 2;\n            }\n\n            cost[i].data[j] = matrix[i - 1][j - 1].item() + c;\n            trace[i].data[j] = t;\n        }\n    }\n\n    // backtrace\n    let i = output_length;\n    let j = input_length;\n\n    // @ts-ignore\n    trace.data.fill(2, 0, outputShape[1]) // trace[0, :] = 2\n    for (let i = 0; i < outputShape[0]; ++i) { // trace[:, 0] = 1\n        trace[i].data[0] = 1;\n    }\n\n    let text_indices = [];\n    let time_indices = [];\n\n    while (i > 0 || j > 0) {\n        text_indices.push(i - 1);\n        time_indices.push(j - 1);\n\n        const t = trace[i][j].item();\n        switch (t) {\n            case 0:\n                --i; --j;\n                break;\n            case 1:\n                --i;\n                break;\n            case 2:\n                --j;\n                break;\n            default:\n                throw new Error(\n                    `Internal error in dynamic time warping. Unexpected trace[${i}, ${j}]. Please file a bug report.`\n                )\n        }\n    }\n\n    text_indices.reverse();\n    time_indices.reverse();\n\n    return [text_indices, time_indices];\n\n}\n\nfunction dimsToStride(dims) {\n    const stride = new Array(dims.length);\n    for (let i = dims.length - 1, s2 = 1; i >= 0; --i) {\n        stride[i] = s2;\n        s2 *= dims[i];\n    }\n    return stride;\n}\n\n/**\n * Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.\n * @param {number[]} size A sequence of integers defining the shape of the output tensor.\n */\nexport function ones(size) {\n    const numElements = size.reduce((a, b) => a * b, 1);\n    return new Tensor(\n        'int64',\n        new BigInt64Array(numElements).fill(1n),\n        size\n    )\n}\n\n/**\n * Returns a tensor filled with the scalar value 1, with the same size as input.\n * @param {Tensor} tensor The size of input will determine size of the output tensor.\n * @returns The ones tensor.\n */\nexport function ones_like(tensor) {\n    return ones(tensor.dims);\n}\n\n/**\n * Quantizes the embeddings tensor to binary or unsigned binary precision.\n * @param {Tensor} tensor The tensor to quantize.\n * @param {'binary'|'ubinary'} precision The precision to use for quantization.\n * @returns {Tensor} The quantized tensor.\n */\nexport function quantize_embeddings(tensor, precision) {\n    if (tensor.dims.length !== 2) {\n        throw new Error(\"The tensor must have 2 dimensions\");\n    }\n    if (tensor.dims.at(-1) % 8 !== 0) {\n        throw new Error(\"The last dimension of the tensor must be a multiple of 8\");\n    }\n    if (!['binary', 'ubinary'].includes(precision)) {\n        throw new Error(\"The precision must be either 'binary' or 'ubinary'\");\n    }\n\n    const signed = precision === 'binary';\n    const dtype = signed ? 'int8' : 'uint8';\n\n    // Create a typed array to store the packed bits\n    const cls = signed ? Int8Array : Uint8Array;\n    const inputData = tensor.data;\n    const outputData = new cls(inputData.length / 8);\n\n    // Iterate over each number in the array\n    for (let i = 0; i < inputData.length; ++i) {\n        // Determine if the number is greater than 0\n        const bit = inputData[i] > 0 ? 1 : 0;\n\n        // Calculate the index in the typed array and the position within the byte\n        const arrayIndex = Math.floor(i / 8);\n        const bitPosition = i % 8;\n\n        // Pack the bit into the typed array\n        outputData[arrayIndex] |= bit << (7 - bitPosition);\n        if (signed && bitPosition === 0) {\n            outputData[arrayIndex] -= 128;\n        }\n    };\n\n    return new Tensor(dtype, outputData, [tensor.dims[0], tensor.dims[1] / 8]);\n}\n"],"mappings":"AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,SAASA,IAAI,QAAQ,qBAAqB;AAE1C,SACIC,gBAAgB,EAChBC,YAAY,QACT,YAAY;AAGnB,MAAMC,WAAW,GAAGC,MAAM,CAACC,MAAM,CAAC;EAC9BC,OAAO,EAAEC,YAAY;EACrBC,OAAO,EAAEC,YAAY;EACrBC,MAAM,EAAEC,KAAK;EAAE;EACfC,IAAI,EAAEC,SAAS;EACfC,KAAK,EAAEC,UAAU;EACjBC,KAAK,EAAEC,UAAU;EACjBC,MAAM,EAAEC,WAAW;EACnBC,KAAK,EAAEC,UAAU;EACjBC,MAAM,EAAEC,WAAW;EACnBC,KAAK,EAAEC,aAAa;EACpBC,MAAM,EAAEC,cAAc;EACtBC,IAAI,EAAEb;AACV,CAAC,CAAC;;AAEF;AACA;AACA;AACA;;AAEA,MAAMc,UAAU,GAAG7B,IAAI,CAAC8B,MAAM;AAE9B,OAAO,MAAMA,MAAM,CAAC;EAChB;EACAC,IAAI;;EAEJ;EACAC,IAAI;;EAEJ;EACAC,IAAI;;EAEJ;EACAC,IAAI;;EAEJ;AACJ;AACA;AACA;EACIC,WAAWA,CAAC,GAAGC,IAAI,EAAE;IACjB,IAAIA,IAAI,CAAC,CAAC,CAAC,YAAYP,UAAU,EAAE;MAC/B;MACAzB,MAAM,CAACiC,MAAM,CAAC,IAAI,EAAED,IAAI,CAAC,CAAC,CAAC,CAAC;IAEhC,CAAC,MAAM;MACH;MACAhC,MAAM,CAACiC,MAAM,CAAC,IAAI,EAAE,IAAIR,UAAU,CAC9B,uBAAwBO,IAAI,CAAC,CAAC,CAAC,EAC/B,6EAA8EA,IAAI,CAAC,CAAC,CAAC,EACrFA,IAAI,CAAC,CAAC,CACV,CAAC,CAAC;IACN;IAEA,OAAO,IAAIE,KAAK,CAAC,IAAI,EAAE;MACnBC,GAAG,EAAEA,CAACC,GAAG,EAAEC,GAAG,KAAK;QACf,IAAI,OAAOA,GAAG,KAAK,QAAQ,EAAE;UACzB,IAAIC,KAAK,GAAGC,MAAM,CAACF,GAAG,CAAC;UACvB,IAAIE,MAAM,CAACC,SAAS,CAACF,KAAK,CAAC,EAAE;YACzB;YACA,OAAOF,GAAG,CAACK,QAAQ,CAACH,KAAK,CAAC;UAC9B;QACJ;QACA;QACA,OAAOF,GAAG,CAACC,GAAG,CAAC;MACnB,CAAC;MACDK,GAAG,EAAEA,CAACN,GAAG,EAAEC,GAAG,EAAEM,KAAK,KAAK;QACtB;;QAEA;QACA,OAAOP,GAAG,CAACC,GAAG,CAAC,GAAGM,KAAK;MAC3B;IACJ,CAAC,CAAC;EACN;;EAEA;AACJ;AACA;AACA;AACA;EACI,EAAEC,MAAM,CAACC,QAAQ,IAAI;IACjB,MAAM,CAACC,UAAU,EAAE,GAAGC,QAAQ,CAAC,GAAG,IAAI,CAACpB,IAAI;IAE3C,IAAIoB,QAAQ,CAACC,MAAM,GAAG,CAAC,EAAE;MACrB,MAAMC,QAAQ,GAAGF,QAAQ,CAACG,MAAM,CAAC,CAACC,CAAC,EAAEC,CAAC,KAAKD,CAAC,GAAGC,CAAC,CAAC;MACjD,KAAK,IAAIC,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGP,UAAU,EAAE,EAAEO,CAAC,EAAE;QACjC,MAAM,IAAI,CAACC,SAAS,CAACD,CAAC,EAAEJ,QAAQ,EAAEF,QAAQ,CAAC;MAC/C;IACJ,CAAC,MAAM;MACH,OAAO,IAAI,CAAClB,IAAI;IACpB;EAEJ;;EAEA;AACJ;AACA;AACA;AACA;EACIY,QAAQA,CAACH,KAAK,EAAE;IACZ,MAAM,CAACQ,UAAU,EAAE,GAAGC,QAAQ,CAAC,GAAG,IAAI,CAACpB,IAAI;IAE3CW,KAAK,GAAGiB,SAAS,CAACjB,KAAK,EAAEQ,UAAU,CAAC;IAEpC,IAAIC,QAAQ,CAACC,MAAM,GAAG,CAAC,EAAE;MACrB,MAAMC,QAAQ,GAAGF,QAAQ,CAACG,MAAM,CAAC,CAACC,CAAC,EAAEC,CAAC,KAAKD,CAAC,GAAGC,CAAC,CAAC;MACjD,OAAO,IAAI,CAACE,SAAS,CAAChB,KAAK,EAAEW,QAAQ,EAAEF,QAAQ,CAAC;IACpD,CAAC,MAAM;MACH,OAAO,IAAIrB,MAAM,CAAC,IAAI,CAACE,IAAI,EAAE,CAAC,IAAI,CAACC,IAAI,CAACS,KAAK,CAAC,CAAC,EAAES,QAAQ,CAAC;IAC9D;EACJ;;EAEA;AACJ;AACA;AACA;EACIS,OAAOA,CAACC,IAAI,EAAE;IACV,KAAK,IAAInB,KAAK,GAAG,CAAC,EAAEA,KAAK,GAAG,IAAI,CAACT,IAAI,CAACmB,MAAM,EAAE,EAAEV,KAAK,EAAE;MACnD;MACA,IAAI,IAAI,CAACT,IAAI,CAACS,KAAK,CAAC,IAAImB,IAAI,EAAE;QAC1B,OAAOnB,KAAK;MAChB;IACJ;IACA,OAAO,CAAC,CAAC;EACb;;EAEA;AACJ;AACA;AACA;AACA;AACA;EACIgB,SAASA,CAAChB,KAAK,EAAEW,QAAQ,EAAEF,QAAQ,EAAE;IACjC,MAAMW,EAAE,GAAGpB,KAAK,GAAGW,QAAQ;IAC3B,MAAMU,EAAE,GAAG,CAACrB,KAAK,GAAG,CAAC,IAAIW,QAAQ;;IAEjC;IACA,MAAMpB,IAAI,GACL,UAAU,IAAI,IAAI,CAACA,IAAI,GAClB,IAAI,CAACA,IAAI,CAAC+B,QAAQ,CAACF,EAAE,EAAEC,EAAE,CAAC,GAC1B,IAAI,CAAC9B,IAAI,CAACgC,KAAK,CAACH,EAAE,EAAEC,EAAE,CAAC;IACjC,OAAO,IAAIjC,MAAM,CAAC,IAAI,CAACE,IAAI,EAAEC,IAAI,EAAEkB,QAAQ,CAAC;EAChD;;EAEA;AACJ;AACA;AACA;AACA;AACA;EACIU,IAAIA,CAAA,EAAG;IACH,IAAI,IAAI,CAAC5B,IAAI,CAACmB,MAAM,KAAK,CAAC,EAAE;MACxB,MAAM,IAAIc,KAAK,CAAC,iBAAiB,IAAI,CAACjC,IAAI,CAACmB,MAAM,yCAAyC,CAAC;IAC/F;IACA,OAAO,IAAI,CAACnB,IAAI,CAAC,CAAC,CAAC;EACvB;;EAEA;AACJ;AACA;AACA;EACIkC,MAAMA,CAAA,EAAG;IACL,OAAOC,OAAO,CAAC,IAAI,CAACnC,IAAI,EAAE,IAAI,CAACF,IAAI,CAAC;EACxC;;EAEA;AACJ;AACA;AACA;EACIsC,OAAOA,CAAA,EAAG;IACN,OAAO,IAAI,CAACC,KAAK,CAAC,CAAC,CAACC,QAAQ,CAAC,CAAC;EAClC;;EAEA;AACJ;AACA;AACA;EACIA,QAAQA,CAAA,EAAG;IACP,KAAK,IAAId,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG,IAAI,CAACxB,IAAI,CAACmB,MAAM,EAAE,EAAEK,CAAC,EAAE;MACvC,IAAI,CAACxB,IAAI,CAACwB,CAAC,CAAC,GAAG,CAAC,IAAI,CAAC,GAAGe,IAAI,CAACC,GAAG,CAAC,CAAC,IAAI,CAACxC,IAAI,CAACwB,CAAC,CAAC,CAAC,CAAC;IACpD;IACA,OAAO,IAAI;EACf;;EAEA;AACJ;AACA;AACA;AACA;EACIiB,GAAGA,CAACC,GAAG,EAAE;IACL,OAAO,IAAI,CAACL,KAAK,CAAC,CAAC,CAACM,IAAI,CAACD,GAAG,CAAC;EACjC;;EAEA;AACJ;AACA;AACA;AACA;EACIC,IAAIA,CAACD,GAAG,EAAE;IACN,KAAK,IAAIlB,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG,IAAI,CAACxB,IAAI,CAACmB,MAAM,EAAE,EAAEK,CAAC,EAAE;MACvC,IAAI,CAACxB,IAAI,CAACwB,CAAC,CAAC,IAAIkB,GAAG;IACvB;IACA,OAAO,IAAI;EACf;;EAGA;AACJ;AACA;AACA;AACA;EACIE,GAAGA,CAACF,GAAG,EAAE;IACL,OAAO,IAAI,CAACL,KAAK,CAAC,CAAC,CAACQ,IAAI,CAACH,GAAG,CAAC;EACjC;;EAEA;AACJ;AACA;AACA;AACA;EACIG,IAAIA,CAACH,GAAG,EAAE;IACN,KAAK,IAAIlB,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG,IAAI,CAACxB,IAAI,CAACmB,MAAM,EAAE,EAAEK,CAAC,EAAE;MACvC,IAAI,CAACxB,IAAI,CAACwB,CAAC,CAAC,IAAIkB,GAAG;IACvB;IACA,OAAO,IAAI;EACf;EACAL,KAAKA,CAAA,EAAG;IACJ,OAAO,IAAIxC,MAAM,CAAC,IAAI,CAACE,IAAI,EAAE,IAAI,CAACC,IAAI,CAACgC,KAAK,CAAC,CAAC,EAAE,IAAI,CAAClC,IAAI,CAACkC,KAAK,CAAC,CAAC,CAAC;EACtE;EAEAA,KAAKA,CAAC,GAAGc,MAAM,EAAE;IACb;IACA,IAAIC,aAAa,GAAG,EAAE;IACtB,IAAIC,UAAU,GAAG,EAAE;;IAEnB;IACA;IACA,KAAK,IAAIC,UAAU,GAAG,CAAC,EAAEA,UAAU,GAAG,IAAI,CAACnD,IAAI,CAACqB,MAAM,EAAE,EAAE8B,UAAU,EAAE;MAClE,IAAIjB,KAAK,GAAGc,MAAM,CAACG,UAAU,CAAC;MAE9B,IAAIjB,KAAK,KAAK,IAAI,IAAIA,KAAK,KAAKkB,SAAS,EAAE;QACvC;QACAF,UAAU,CAACG,IAAI,CAAC,CAAC,CAAC,EAAE,IAAI,CAACrD,IAAI,CAACmD,UAAU,CAAC,CAAC,CAAC;QAC3CF,aAAa,CAACI,IAAI,CAAC,IAAI,CAACrD,IAAI,CAACmD,UAAU,CAAC,CAAC;MAE7C,CAAC,MAAM,IAAI,OAAOjB,KAAK,KAAK,QAAQ,EAAE;QAClCA,KAAK,GAAGN,SAAS,CAACM,KAAK,EAAE,IAAI,CAAClC,IAAI,CAACmD,UAAU,CAAC,EAAEA,UAAU,CAAC;;QAE3D;QACAD,UAAU,CAACG,IAAI,CAAC,CAACnB,KAAK,EAAEA,KAAK,GAAG,CAAC,CAAC,CAAC;MAEvC,CAAC,MAAM,IAAItD,KAAK,CAAC0E,OAAO,CAACpB,KAAK,CAAC,IAAIA,KAAK,CAACb,MAAM,KAAK,CAAC,EAAE;QACnD;;QAEA,IAAIa,KAAK,CAAC,CAAC,CAAC,GAAGA,KAAK,CAAC,CAAC,CAAC,EAAE;UACrB,MAAM,IAAIC,KAAK,CAAC,kBAAkBD,KAAK,EAAE,CAAC;QAC9C;QAEA,IAAIqB,OAAO,GAAG,CACVd,IAAI,CAACe,GAAG,CAACtB,KAAK,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EACrBO,IAAI,CAACgB,GAAG,CAACvB,KAAK,CAAC,CAAC,CAAC,EAAE,IAAI,CAAClC,IAAI,CAACmD,UAAU,CAAC,CAAC,CAC5C;QAEDD,UAAU,CAACG,IAAI,CAACE,OAAO,CAAC;QACxBN,aAAa,CAACI,IAAI,CAACE,OAAO,CAAC,CAAC,CAAC,GAAGA,OAAO,CAAC,CAAC,CAAC,CAAC;MAE/C,CAAC,MAAM;QACH,MAAM,IAAIpB,KAAK,CAAC,kBAAkBD,KAAK,EAAE,CAAC;MAC9C;IACJ;IAEA,IAAIwB,OAAO,GAAGR,UAAU,CAACS,GAAG,CAAC,CAAC,CAACC,KAAK,EAAEC,GAAG,CAAC,KAAKA,GAAG,GAAGD,KAAK,CAAC;IAC3D,IAAIE,aAAa,GAAGJ,OAAO,CAACnC,MAAM,CAAC,CAACC,CAAC,EAAEC,CAAC,KAAKD,CAAC,GAAGC,CAAC,CAAC;;IAEnD;IACA;IACA,IAAIvB,IAAI,GAAG,IAAI,IAAI,CAACA,IAAI,CAACE,WAAW,CAAC0D,aAAa,CAAC;;IAEnD;IACA,MAAMC,MAAM,GAAG,IAAI,CAACA,MAAM,CAAC,CAAC;IAE5B,KAAK,IAAIrC,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGoC,aAAa,EAAE,EAAEpC,CAAC,EAAE;MACpC,IAAIsC,aAAa,GAAG,CAAC;MACrB,KAAK,IAAIC,CAAC,GAAGP,OAAO,CAACrC,MAAM,GAAG,CAAC,EAAE6C,GAAG,GAAGxC,CAAC,EAAEuC,CAAC,IAAI,CAAC,EAAE,EAAEA,CAAC,EAAE;QACnD,MAAM9D,IAAI,GAAGuD,OAAO,CAACO,CAAC,CAAC;QACvBD,aAAa,IAAI,CAAEE,GAAG,GAAG/D,IAAI,GAAI+C,UAAU,CAACe,CAAC,CAAC,CAAC,CAAC,CAAC,IAAIF,MAAM,CAACE,CAAC,CAAC;QAC9DC,GAAG,GAAGzB,IAAI,CAAC0B,KAAK,CAACD,GAAG,GAAG/D,IAAI,CAAC;MAChC;MACAD,IAAI,CAACwB,CAAC,CAAC,GAAG,IAAI,CAACxB,IAAI,CAAC8D,aAAa,CAAC;IACtC;IACA,OAAO,IAAIjE,MAAM,CAAC,IAAI,CAACE,IAAI,EAAEC,IAAI,EAAE+C,aAAa,CAAC;EAErD;;EAEA;AACJ;AACA;AACA;AACA;EACImB,OAAOA,CAAC,GAAGpE,IAAI,EAAE;IACb,OAAOoE,OAAO,CAAC,IAAI,EAAEpE,IAAI,CAAC;EAC9B;;EAEA;EACAqE,SAASA,CAAC,GAAGrE,IAAI,EAAE;IACf,OAAO,IAAI,CAACoE,OAAO,CAAC,GAAGpE,IAAI,CAAC;EAChC;;EAEA;;EAEA;AACJ;AACA;AACA;AACA;AACA;AACA;EACIsE,GAAGA,CAACC,GAAG,GAAG,IAAI,EAAEC,OAAO,GAAG,KAAK,EAAE;IAC7B,OAAO,IAAI,CAACC,IAAI,CAAC,CAAC,EAAEF,GAAG,EAAEC,OAAO,CAAC;EACrC;;EAEA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;EACIC,IAAIA,CAACC,CAAC,GAAG,KAAK,EAAEH,GAAG,GAAG,IAAI,EAAEC,OAAO,GAAG,KAAK,EAAE;IACzC,IAAIE,CAAC,KAAK,KAAK,EAAE;MACb;MACAA,CAAC,GAAG,CAAC;IACT,CAAC,MAAM,IAAI,OAAOA,CAAC,KAAK,QAAQ,EAAE;MAC9B,MAAMvC,KAAK,CAAC,qBAAqBuC,CAAC,EAAE,CAAC;IACzC;IAEA,IAAIH,GAAG,KAAK,IAAI,EAAE;MACd;MACA,IAAI3B,GAAG,GAAG,IAAI,CAAC1C,IAAI,CAACqB,MAAM,CAAC,CAACC,CAAC,EAAEC,CAAC,KAAKD,CAAC,GAAIC,CAAC,IAAIiD,CAAE,EAAE,CAAC,CAAC,KAAK,CAAC,GAAGA,CAAC,CAAC;MAChE,OAAO,IAAI3E,MAAM,CAAC,IAAI,CAACE,IAAI,EAAE,CAAC2C,GAAG,CAAC,EAAE,EAAE,CAAC;IAC3C;;IAEA;IACA2B,GAAG,GAAG3C,SAAS,CAAC2C,GAAG,EAAE,IAAI,CAACvE,IAAI,CAACqB,MAAM,CAAC;;IAEtC;IACA,MAAMsD,UAAU,GAAG,IAAI,CAAC3E,IAAI,CAACkC,KAAK,CAAC,CAAC,CAAC,CAAC;IACtCyC,UAAU,CAACJ,GAAG,CAAC,GAAG,CAAC,CAAC,CAAC;;IAErB;IACA;IACA,MAAMK,MAAM,GAAG,IAAI,IAAI,CAAC1E,IAAI,CAACE,WAAW,CAAC,IAAI,CAACF,IAAI,CAACmB,MAAM,GAAG,IAAI,CAACrB,IAAI,CAACuE,GAAG,CAAC,CAAC;;IAE3E;IACA,KAAK,IAAI7C,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG,IAAI,CAACxB,IAAI,CAACmB,MAAM,EAAE,EAAEK,CAAC,EAAE;MAEvC;MACA,IAAImD,WAAW,GAAG,CAAC;MAEnB,KAAK,IAAIZ,CAAC,GAAG,IAAI,CAACjE,IAAI,CAACqB,MAAM,GAAG,CAAC,EAAE6C,GAAG,GAAGxC,CAAC,EAAEoD,gBAAgB,GAAG,CAAC,EAAEb,CAAC,IAAI,CAAC,EAAE,EAAEA,CAAC,EAAE;QAC3E,MAAM9D,IAAI,GAAG,IAAI,CAACH,IAAI,CAACiE,CAAC,CAAC;QACzB,IAAIA,CAAC,KAAKM,GAAG,EAAE;UACX,MAAM5D,KAAK,GAAGuD,GAAG,GAAG/D,IAAI;UACxB0E,WAAW,IAAIlE,KAAK,GAAGmE,gBAAgB;UACvCA,gBAAgB,IAAIH,UAAU,CAACV,CAAC,CAAC;QACrC;QACAC,GAAG,GAAGzB,IAAI,CAAC0B,KAAK,CAACD,GAAG,GAAG/D,IAAI,CAAC;MAChC;;MAEA;MACAyE,MAAM,CAACC,WAAW,CAAC,IAAK,IAAI,CAAC3E,IAAI,CAACwB,CAAC,CAAC,IAAKgD,CAAC;IAC9C;IAEA,IAAIA,CAAC,KAAK,CAAC,EAAE;MACT,KAAK,IAAIhD,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGkD,MAAM,CAACvD,MAAM,EAAE,EAAEK,CAAC,EAAE;QACpCkD,MAAM,CAAClD,CAAC,CAAC,GAAGkD,MAAM,CAAClD,CAAC,CAAC,KAAK,CAAC,GAAGgD,CAAC,CAAC;MACpC;IACJ;IAEA,IAAI,CAACF,OAAO,EAAE;MACVG,UAAU,CAACI,MAAM,CAACR,GAAG,EAAE,CAAC,CAAC;IAC7B;IAEA,OAAO,IAAIxE,MAAM,CAAC,IAAI,CAACE,IAAI,EAAE2E,MAAM,EAAED,UAAU,CAAC;EACpD;;EAEA;AACJ;AACA;AACA;AACA;AACA;EACIK,UAAUA,CAACN,CAAC,GAAG,GAAG,EAAEH,GAAG,GAAG,CAAC,EAAE;IACzBA,GAAG,GAAG3C,SAAS,CAAC2C,GAAG,EAAE,IAAI,CAACvE,IAAI,CAACqB,MAAM,CAAC;IAEtC,MAAMoD,IAAI,GAAG,IAAI,CAACA,IAAI,CAACC,CAAC,EAAEH,GAAG,EAAE,IAAI,CAAC;IAEpC,KAAK,IAAI7C,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG,IAAI,CAACxB,IAAI,CAACmB,MAAM,EAAE,EAAEK,CAAC,EAAE;MAEvC;MACA,IAAImD,WAAW,GAAG,CAAC;MAEnB,KAAK,IAAIZ,CAAC,GAAG,IAAI,CAACjE,IAAI,CAACqB,MAAM,GAAG,CAAC,EAAE6C,GAAG,GAAGxC,CAAC,EAAEoD,gBAAgB,GAAG,CAAC,EAAEb,CAAC,IAAI,CAAC,EAAE,EAAEA,CAAC,EAAE;QAC3E,MAAM9D,IAAI,GAAG,IAAI,CAACH,IAAI,CAACiE,CAAC,CAAC;QACzB,IAAIA,CAAC,KAAKM,GAAG,EAAE;UACX,MAAM5D,KAAK,GAAGuD,GAAG,GAAG/D,IAAI;UACxB0E,WAAW,IAAIlE,KAAK,GAAGmE,gBAAgB;UACvCA,gBAAgB,IAAI,IAAI,CAAC9E,IAAI,CAACiE,CAAC,CAAC;QACpC;QACAC,GAAG,GAAGzB,IAAI,CAAC0B,KAAK,CAACD,GAAG,GAAG/D,IAAI,CAAC;MAChC;;MAEA;MACA,IAAI,CAACD,IAAI,CAACwB,CAAC,CAAC,IAAI+C,IAAI,CAACvE,IAAI,CAAC2E,WAAW,CAAC;IAC1C;IAEA,OAAO,IAAI;EACf;;EAEA;AACJ;AACA;AACA;AACA;AACA;EACII,SAASA,CAACP,CAAC,GAAG,GAAG,EAAEH,GAAG,GAAG,CAAC,EAAE;IACxB,OAAO,IAAI,CAAChC,KAAK,CAAC,CAAC,CAACyC,UAAU,CAACN,CAAC,EAAEH,GAAG,CAAC;EAC1C;;EAEA;AACJ;AACA;AACA;AACA;EACIR,MAAMA,CAAA,EAAG;IACL,OAAOmB,YAAY,CAAC,IAAI,CAAClF,IAAI,CAAC;EAClC;;EAEA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACImF,OAAOA,CAACZ,GAAG,GAAG,IAAI,EAAE;IAChB,OAAO,IAAIxE,MAAM,CACb,IAAI,CAACE,IAAI,EACT,IAAI,CAACC,IAAI,EACTkF,iBAAiB,CAAC,IAAI,CAACpF,IAAI,EAAEuE,GAAG,CACpC,CAAC;EACL;;EAEA;AACJ;AACA;EACIc,QAAQA,CAACd,GAAG,GAAG,IAAI,EAAE;IACjB,IAAI,CAACvE,IAAI,GAAGoF,iBAAiB,CAAC,IAAI,CAACpF,IAAI,EAAEuE,GAAG,CAAC;IAC7C,OAAO,IAAI;EACf;;EAEA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;EACIe,SAASA,CAACf,GAAG,GAAG,IAAI,EAAE;IAClB,OAAO,IAAIxE,MAAM,CACb,IAAI,CAACE,IAAI,EACT,IAAI,CAACC,IAAI,EACTqF,mBAAmB,CAAC,IAAI,CAACvF,IAAI,EAAEuE,GAAG,CACtC,CAAC;EACL;;EAEA;AACJ;AACA;EACIiB,UAAUA,CAACjB,GAAG,GAAG,IAAI,EAAE;IACnB,IAAI,CAACvE,IAAI,GAAGuF,mBAAmB,CAAC,IAAI,CAACvF,IAAI,EAAEuE,GAAG,CAAC;IAC/C,OAAO,IAAI;EACf;;EAEA;AACJ;AACA;EACIkB,QAAQA,CAACC,SAAS,GAAG,CAAC,EAAEC,OAAO,GAAG,CAAC,CAAC,EAAE;IAClC;IACAA,OAAO,GAAG,CAACA,OAAO,GAAG,IAAI,CAAC3F,IAAI,CAACqB,MAAM,IAAI,IAAI,CAACrB,IAAI,CAACqB,MAAM;IAEzD,IAAIuE,gBAAgB,GAAG,IAAI,CAAC5F,IAAI,CAACkC,KAAK,CAAC,CAAC,EAAEwD,SAAS,CAAC;IACpD,IAAIG,aAAa,GAAG,IAAI,CAAC7F,IAAI,CAACkC,KAAK,CAACwD,SAAS,EAAEC,OAAO,GAAG,CAAC,CAAC;IAC3D,IAAIG,eAAe,GAAG,IAAI,CAAC9F,IAAI,CAACkC,KAAK,CAACyD,OAAO,GAAG,CAAC,CAAC;IAElD,IAAI,CAAC3F,IAAI,GAAG,CAAC,GAAG4F,gBAAgB,EAAEC,aAAa,CAACtE,MAAM,CAAC,CAACC,CAAC,EAAEC,CAAC,KAAKD,CAAC,GAAGC,CAAC,EAAE,CAAC,CAAC,EAAE,GAAGqE,eAAe,CAAC;IAC/F,OAAO,IAAI;EACf;;EAEA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;EACIC,OAAOA,CAACL,SAAS,GAAG,CAAC,EAAEC,OAAO,GAAG,CAAC,CAAC,EAAE;IACjC,OAAO,IAAI,CAACpD,KAAK,CAAC,CAAC,CAACkD,QAAQ,CAACC,SAAS,EAAEC,OAAO,CAAC;EACpD;;EAEA;AACJ;AACA;AACA;AACA;EACIK,IAAIA,CAAC,GAAGhG,IAAI,EAAE;IACV;IACA,IAAIiG,aAAa,GAAG,CAAC,CAAC;IACtB,KAAK,IAAIvE,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG1B,IAAI,CAACqB,MAAM,EAAE,EAAEK,CAAC,EAAE;MAClC,IAAI1B,IAAI,CAAC0B,CAAC,CAAC,KAAK,CAAC,CAAC,EAAE;QAChB,IAAIuE,aAAa,KAAK,CAAC,CAAC,EAAE;UACtB,MAAM,IAAI9D,KAAK,CAAC,oCAAoC,CAAC;QACzD;QACA8D,aAAa,GAAGvE,CAAC;MACrB;IACJ;IAEA,IAAIuE,aAAa,KAAK,CAAC,CAAC,EAAE;MACtB;MACA,MAAMC,YAAY,GAAGlG,IAAI,CAACuB,MAAM,CAAC,CAAC4E,OAAO,EAAEC,IAAI,EAAEzF,KAAK,KAAK;QACvD,OAAOA,KAAK,KAAKsF,aAAa,GAAGE,OAAO,GAAGC,IAAI,GAAGD,OAAO;MAC7D,CAAC,EAAE,CAAC,CAAC;MAELnG,IAAI,CAACiG,aAAa,CAAC,GAAG,IAAI,CAAC/F,IAAI,CAACmB,MAAM,GAAG6E,YAAY;IACzD;IACA,OAAO,IAAInG,MAAM,CAAC,IAAI,CAACE,IAAI,EAAE,IAAI,CAACC,IAAI,EAAEF,IAAI,CAAC,CAAC,CAAC;EACnD;EAEAqG,IAAIA,CAAA,EAAG;IACH,KAAK,IAAI3E,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG,IAAI,CAACxB,IAAI,CAACmB,MAAM,EAAE,EAAEK,CAAC,EAAE;MACvC,IAAI,CAACxB,IAAI,CAACwB,CAAC,CAAC,GAAG,CAAC,IAAI,CAACxB,IAAI,CAACwB,CAAC,CAAC;IAChC;IACA,OAAO,IAAI;EACf;EACA4E,GAAGA,CAAA,EAAG;IACF,OAAO,IAAI,CAAC/D,KAAK,CAAC,CAAC,CAAC8D,IAAI,CAAC,CAAC;EAC9B;;EAEA;AACJ;AACA;EACIE,MAAMA,CAAC9C,GAAG,EAAED,GAAG,EAAE;IACb,KAAK,IAAI9B,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG,IAAI,CAACxB,IAAI,CAACmB,MAAM,EAAE,EAAEK,CAAC,EAAE;MACvC,IAAI,CAACxB,IAAI,CAACwB,CAAC,CAAC,GAAGe,IAAI,CAACgB,GAAG,CAAChB,IAAI,CAACe,GAAG,CAAC,IAAI,CAACtD,IAAI,CAACwB,CAAC,CAAC,EAAE+B,GAAG,CAAC,EAAED,GAAG,CAAC;IAC7D;IACA,OAAO,IAAI;EACf;;EAEA;AACJ;AACA;AACA;AACA;AACA;EACIgD,KAAKA,CAAC/C,GAAG,EAAED,GAAG,EAAE;IACZ,OAAO,IAAI,CAACjB,KAAK,CAAC,CAAC,CAACgE,MAAM,CAAC9C,GAAG,EAAED,GAAG,CAAC;EACxC;;EAEA;AACJ;AACA;EACIiD,MAAMA,CAAA,EAAG;IACL,KAAK,IAAI/E,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG,IAAI,CAACxB,IAAI,CAACmB,MAAM,EAAE,EAAEK,CAAC,EAAE;MACvC,IAAI,CAACxB,IAAI,CAACwB,CAAC,CAAC,GAAGe,IAAI,CAACiE,KAAK,CAAC,IAAI,CAACxG,IAAI,CAACwB,CAAC,CAAC,CAAC;IAC3C;IACA,OAAO,IAAI;EACf;;EAEA;AACJ;AACA;AACA;EACIgF,KAAKA,CAAA,EAAG;IACJ,OAAO,IAAI,CAACnE,KAAK,CAAC,CAAC,CAACkE,MAAM,CAAC,CAAC;EAChC;;EAEA;AACJ;AACA;AACA;AACA;EACIE,EAAEA,CAAC1G,IAAI,EAAE;IACL;IACA,IAAI,IAAI,CAACA,IAAI,KAAKA,IAAI,EAAE,OAAO,IAAI;;IAEnC;IACA,IAAI,CAAC7B,WAAW,CAACwI,cAAc,CAAC3G,IAAI,CAAC,EAAE;MACnC,MAAM,IAAIkC,KAAK,CAAC,qBAAqBlC,IAAI,EAAE,CAAC;IAChD;IACA;IACA,OAAO,IAAIF,MAAM,CAACE,IAAI,EAAE7B,WAAW,CAAC6B,IAAI,CAAC,CAAC4G,IAAI,CAAC,IAAI,CAAC3G,IAAI,CAAC,EAAE,IAAI,CAACF,IAAI,CAAC;EACzE;AACJ;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAASqC,OAAOA,CAACnC,IAAI,EAAE4G,UAAU,EAAE;EAE/B,MAAMC,aAAa,GAAG7G,IAAI,CAACmB,MAAM;EACjC,MAAM2F,aAAa,GAAGF,UAAU,CAACvF,MAAM,CAAC,CAACC,CAAC,EAAEC,CAAC,KAAKD,CAAC,GAAGC,CAAC,CAAC;EAExD,IAAIsF,aAAa,KAAKC,aAAa,EAAE;IACjC,MAAM7E,KAAK,CAAC,gCAAgC4E,aAAa,gBAAgBD,UAAU,GAAG,CAAC;EAC3F;;EAEA;EACA,IAAIG,aAAa,GAAG/G,IAAI;EAExB,KAAK,IAAIwB,CAAC,GAAGoF,UAAU,CAACzF,MAAM,GAAG,CAAC,EAAEK,CAAC,IAAI,CAAC,EAAEA,CAAC,EAAE,EAAE;IAC7CuF,aAAa,GAAGA,aAAa,CAAC1F,MAAM,CAAC,CAAC2F,GAAG,EAAEtE,GAAG,KAAK;MAC/C,IAAIuE,SAAS,GAAGD,GAAG,CAACA,GAAG,CAAC7F,MAAM,GAAG,CAAC,CAAC;MAEnC,IAAI8F,SAAS,CAAC9F,MAAM,GAAGyF,UAAU,CAACpF,CAAC,CAAC,EAAE;QAClCyF,SAAS,CAAC9D,IAAI,CAACT,GAAG,CAAC;MACvB,CAAC,MAAM;QACHsE,GAAG,CAAC7D,IAAI,CAAC,CAACT,GAAG,CAAC,CAAC;MACnB;MAEA,OAAOsE,GAAG;IACd,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC;EACZ;EAEA,OAAOD,aAAa,CAAC,CAAC,CAAC;AAC3B;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,SAAS7C,OAAOA,CAACgD,MAAM,EAAEC,IAAI,EAAE;EAClC,MAAM,CAACC,YAAY,EAAEC,KAAK,CAAC,GAAGpJ,YAAY,CAACiJ,MAAM,CAAClH,IAAI,EAAEkH,MAAM,CAACpH,IAAI,EAAEqH,IAAI,CAAC;EAC1E,OAAO,IAAItH,MAAM,CAACqH,MAAM,CAACnH,IAAI,EAAEqH,YAAY,EAAEC,KAAK,CAAC;AACvD;;AAGA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,SAASC,WAAWA,CAACC,KAAK,EAAE,CAACC,UAAU,EAAEC,SAAS,CAAC,EAAEC,IAAI,GAAG,UAAU,EAAEC,aAAa,GAAG,KAAK,EAAE;EAElG;EACA,MAAMC,WAAW,GAAGL,KAAK,CAACzH,IAAI,CAAC+H,EAAE,CAAC,CAAC,CAAC,CAAC,IAAI,CAAC;EAC1C,MAAMC,SAAS,GAAGP,KAAK,CAACzH,IAAI,CAAC+H,EAAE,CAAC,CAAC,CAAC,CAAC;EACnC,MAAME,QAAQ,GAAGR,KAAK,CAACzH,IAAI,CAAC+H,EAAE,CAAC,CAAC,CAAC,CAAC;EAElC,IAAIG,MAAM,GAAGhK,gBAAgB,CACzB,6CAA8CuJ,KAAK,CAACvH,IAAI,EACxD,CAAC4H,WAAW,EAAEE,SAAS,EAAEC,QAAQ,CAAC,EAClC,CAACP,UAAU,EAAEC,SAAS,CAAC,EACvBC,IAAI,EACJC,aACJ,CAAC;EACD,OAAO,IAAI9H,MAAM,CAAC0H,KAAK,CAACxH,IAAI,EAAEiI,MAAM,EAAE,CAACJ,WAAW,EAAEJ,UAAU,EAAEC,SAAS,CAAC,CAAC;AAC/E;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,SAASQ,YAAYA,CAACC,iBAAiB,EAAEC,cAAc,EAAE;EAC5D;EACA;;EAEA,IAAId,KAAK,GAAG,CAACa,iBAAiB,CAACpI,IAAI,CAAC,CAAC,CAAC,EAAEoI,iBAAiB,CAACpI,IAAI,CAAC,CAAC,CAAC,CAAC;EAClE;EACA,IAAIsI,YAAY,GAAG,IAAIF,iBAAiB,CAAClI,IAAI,CAACE,WAAW,CAACmH,KAAK,CAAC,CAAC,CAAC,GAAGA,KAAK,CAAC,CAAC,CAAC,CAAC;EAC9E,IAAI,CAACgB,SAAS,EAAEC,SAAS,EAAEC,QAAQ,CAAC,GAAGL,iBAAiB,CAACpI,IAAI;EAE7D,IAAI0I,QAAQ,GAAG,CAAC;EAChB,KAAK,IAAIhH,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG6G,SAAS,EAAE,EAAE7G,CAAC,EAAE;IAChC,IAAIiH,MAAM,GAAGjH,CAAC,GAAG+G,QAAQ,GAAGD,SAAS;IAErC,KAAK,IAAII,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGH,QAAQ,EAAE,EAAEG,CAAC,EAAE;MAC/B,IAAItE,GAAG,GAAG,CAAC;MACX,IAAIuE,KAAK,GAAG,CAAC;MAEb,IAAIC,cAAc,GAAGpH,CAAC,GAAG8G,SAAS;MAClC,IAAIO,OAAO,GAAGJ,MAAM,GAAGC,CAAC;MACxB;MACA,KAAK,IAAI3E,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGuE,SAAS,EAAE,EAAEvE,CAAC,EAAE;QAChC;QACA,IAAI+E,IAAI,GAAGpI,MAAM,CAACyH,cAAc,CAACnI,IAAI,CAAC4I,cAAc,GAAG7E,CAAC,CAAC,CAAC;QAE1D4E,KAAK,IAAIG,IAAI;QACb1E,GAAG,IAAI8D,iBAAiB,CAAClI,IAAI,CAAC6I,OAAO,GAAG9E,CAAC,GAAGwE,QAAQ,CAAC,GAAGO,IAAI;MAChE;MAEA,IAAIC,GAAG,GAAG3E,GAAG,GAAGuE,KAAK;MACrBP,YAAY,CAACI,QAAQ,EAAE,CAAC,GAAGO,GAAG;IAClC;EACJ;EAEA,OAAO,IAAIlJ,MAAM,CACbqI,iBAAiB,CAACnI,IAAI,EACtBqI,YAAY,EACZf,KACJ,CAAC;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,SAAS2B,UAAUA,CAACzB,KAAK,EAAE0B,gBAAgB,EAAE;EAChDC,GAAG,GAAG;AACV,CAAC,GAAG,CAAC,CAAC,EAAE;EACJ,IAAI3B,KAAK,CAACzH,IAAI,CAACqB,MAAM,KAAK,CAAC,EAAE;IACzB,MAAM,IAAIc,KAAK,CAAC,gDAAgD,CAAC;EACrE;EAEA,MAAM,CAACoG,SAAS,EAAEc,UAAU,CAAC,GAAG5B,KAAK,CAACzH,IAAI;EAE1C,IAAImJ,gBAAgB,CAAC9H,MAAM,KAAK,CAAC,IAAI8H,gBAAgB,CAAC,CAAC,CAAC,KAAKE,UAAU,EAAE;IACrE,MAAM,IAAIlH,KAAK,CAAC,qEAAqE,CAAC;EAC1F;EAEA,MAAM,CAACmH,GAAG,EAAEC,IAAI,CAAC,GAAGC,QAAQ,CAAC/B,KAAK,EAAE,CAAC,EAAE,CAAC,EAAE,IAAI,CAAC;;EAE/C;EACA,MAAMa,YAAY,GAAG,IAAIb,KAAK,CAACvH,IAAI,CAACE,WAAW,CAACqH,KAAK,CAACvH,IAAI,CAACmB,MAAM,CAAC;EAElE,KAAK,IAAIK,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG6G,SAAS,EAAE,EAAE7G,CAAC,EAAE;IAChC,MAAMiH,MAAM,GAAGjH,CAAC,GAAG2H,UAAU;IAC7B,KAAK,IAAIpF,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGoF,UAAU,EAAE,EAAEpF,CAAC,EAAE;MACjC,MAAM8E,OAAO,GAAGJ,MAAM,GAAG1E,CAAC;MAC1BqE,YAAY,CAACS,OAAO,CAAC,GAAG,CAACtB,KAAK,CAACvH,IAAI,CAAC6I,OAAO,CAAC,GAAGQ,IAAI,CAACrJ,IAAI,CAACwB,CAAC,CAAC,KAAK4H,GAAG,CAACpJ,IAAI,CAACwB,CAAC,CAAC,GAAG0H,GAAG,CAAC;IACtF;EACJ;EACA,OAAO,IAAIrJ,MAAM,CAAC0H,KAAK,CAACxH,IAAI,EAAEqI,YAAY,EAAEb,KAAK,CAACzH,IAAI,CAAC;AAC3D;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAASoF,iBAAiBA,CAACpF,IAAI,EAAEuE,GAAG,EAAE;EAClCvE,IAAI,GAAGA,IAAI,CAACkC,KAAK,CAAC,CAAC;EACnB,IAAIqC,GAAG,KAAK,IAAI,EAAE;IACdvE,IAAI,GAAGA,IAAI,CAACyJ,MAAM,CAAEC,CAAC,IAAKA,CAAC,KAAK,CAAC,CAAC;EACtC,CAAC,MAAM,IAAI,OAAOnF,GAAG,KAAK,QAAQ,EAAE;IAChC,IAAIvE,IAAI,CAACuE,GAAG,CAAC,KAAK,CAAC,EAAE;MACjBvE,IAAI,CAAC+E,MAAM,CAACR,GAAG,EAAE,CAAC,CAAC;IACvB;EACJ,CAAC,MAAM,IAAI3F,KAAK,CAAC0E,OAAO,CAACiB,GAAG,CAAC,EAAE;IAC3BvE,IAAI,GAAGA,IAAI,CAACyJ,MAAM,CAAC,CAACE,CAAC,EAAEjI,CAAC,KAAK;MACzB,OAAOiI,CAAC,KAAK,CAAC,IAAI,CAACpF,GAAG,CAACqF,QAAQ,CAAClI,CAAC,CAAC;IACtC,CAAC,CAAC;EACN;EACA,OAAO1B,IAAI;AACf;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAASuF,mBAAmBA,CAACvF,IAAI,EAAEuE,GAAG,EAAE;EACpC;EACA;EACAA,GAAG,GAAG3C,SAAS,CAAC2C,GAAG,EAAEvE,IAAI,CAACqB,MAAM,GAAG,CAAC,CAAC;EACrCrB,IAAI,GAAGA,IAAI,CAACkC,KAAK,CAAC,CAAC;EACnB;EACAlC,IAAI,CAAC+E,MAAM,CAACR,GAAG,EAAE,CAAC,EAAE,CAAC,CAAC;EACtB,OAAOvE,IAAI;AACf;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS4B,SAASA,CAACjB,KAAK,EAAER,IAAI,EAAE0J,SAAS,GAAG,IAAI,EAAE;EAC9C,IAAIlJ,KAAK,GAAG,CAACR,IAAI,IAAIQ,KAAK,IAAIR,IAAI,EAAE;IAChC,MAAM,IAAIgC,KAAK,CAAC,qBAAqBxB,KAAK,kCAAkCkJ,SAAS,KAAK,IAAI,GAAG,EAAE,GAAG,GAAG,GAAGA,SAAS,cAAc1J,IAAI,EAAE,CAAC;EAC9I;EAEA,IAAIQ,KAAK,GAAG,CAAC,EAAE;IACX;IACAA,KAAK,GAAG,CAAEA,KAAK,GAAGR,IAAI,GAAIA,IAAI,IAAIA,IAAI;EAC1C;EACA,OAAOQ,KAAK;AAChB;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,SAASmJ,GAAGA,CAACC,OAAO,EAAExF,GAAG,GAAG,CAAC,EAAE;EAClCA,GAAG,GAAG3C,SAAS,CAAC2C,GAAG,EAAEwF,OAAO,CAAC,CAAC,CAAC,CAAC/J,IAAI,CAACqB,MAAM,CAAC;;EAE5C;;EAEA,MAAMsD,UAAU,GAAGoF,OAAO,CAAC,CAAC,CAAC,CAAC/J,IAAI,CAACkC,KAAK,CAAC,CAAC;EAC1CyC,UAAU,CAACJ,GAAG,CAAC,GAAGwF,OAAO,CAACxI,MAAM,CAAC,CAACC,CAAC,EAAEC,CAAC,KAAKD,CAAC,GAAGC,CAAC,CAACzB,IAAI,CAACuE,GAAG,CAAC,EAAE,CAAC,CAAC;;EAE9D;EACA,MAAMyF,UAAU,GAAGrF,UAAU,CAACpD,MAAM,CAAC,CAACC,CAAC,EAAEC,CAAC,KAAKD,CAAC,GAAGC,CAAC,EAAE,CAAC,CAAC;EACxD;EACA,MAAMmD,MAAM,GAAG,IAAImF,OAAO,CAAC,CAAC,CAAC,CAAC7J,IAAI,CAACE,WAAW,CAAC4J,UAAU,CAAC;;EAE1D;EACA,MAAMC,UAAU,GAAGF,OAAO,CAAC,CAAC,CAAC,CAAC9J,IAAI;EAElC,IAAIsE,GAAG,KAAK,CAAC,EAAE;IACX;;IAEA,IAAIoE,MAAM,GAAG,CAAC;IACd,KAAK,IAAIuB,CAAC,IAAIH,OAAO,EAAE;MACnBnF,MAAM,CAAC7D,GAAG,CAACmJ,CAAC,CAAChK,IAAI,EAAEyI,MAAM,CAAC;MAC1BA,MAAM,IAAIuB,CAAC,CAAChK,IAAI,CAACmB,MAAM;IAC3B;EAEJ,CAAC,MAAM;IAEH,IAAI8I,UAAU,GAAG,CAAC;IAElB,KAAK,IAAID,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGH,OAAO,CAAC1I,MAAM,EAAE,EAAE6I,CAAC,EAAE;MACrC,IAAI9C,MAAM,GAAG2C,OAAO,CAACG,CAAC,CAAC;;MAEvB;MACA,KAAK,IAAIxI,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG0F,MAAM,CAAClH,IAAI,CAACmB,MAAM,EAAE,EAAEK,CAAC,EAAE;QACzC;QACA,IAAImD,WAAW,GAAG,CAAC;QAEnB,KAAK,IAAIZ,CAAC,GAAGmD,MAAM,CAACpH,IAAI,CAACqB,MAAM,GAAG,CAAC,EAAE6C,GAAG,GAAGxC,CAAC,EAAEoD,gBAAgB,GAAG,CAAC,EAAEb,CAAC,IAAI,CAAC,EAAE,EAAEA,CAAC,EAAE;UAC7E,MAAM9D,IAAI,GAAGiH,MAAM,CAACpH,IAAI,CAACiE,CAAC,CAAC;UAC3B,IAAItD,KAAK,GAAGuD,GAAG,GAAG/D,IAAI;UACtB,IAAI8D,CAAC,KAAKM,GAAG,EAAE;YACX5D,KAAK,IAAIwJ,UAAU;UACvB;UACAtF,WAAW,IAAIlE,KAAK,GAAGmE,gBAAgB;UACvCA,gBAAgB,IAAIH,UAAU,CAACV,CAAC,CAAC;UACjCC,GAAG,GAAGzB,IAAI,CAAC0B,KAAK,CAACD,GAAG,GAAG/D,IAAI,CAAC;QAChC;QACA;QACAyE,MAAM,CAACC,WAAW,CAAC,GAAGuC,MAAM,CAAClH,IAAI,CAACwB,CAAC,CAAC;MACxC;MAEAyI,UAAU,IAAI/C,MAAM,CAACpH,IAAI,CAACuE,GAAG,CAAC;IAClC;EACJ;EACA,OAAO,IAAIxE,MAAM,CAACkK,UAAU,EAAErF,MAAM,EAAED,UAAU,CAAC;AACrD;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,SAASyF,KAAKA,CAACL,OAAO,EAAExF,GAAG,GAAG,CAAC,EAAE;EACpC;EACA;EACA,OAAOuF,GAAG,CAACC,OAAO,CAACpG,GAAG,CAACuG,CAAC,IAAIA,CAAC,CAAC5E,SAAS,CAACf,GAAG,CAAC,CAAC,EAAEA,GAAG,CAAC;AACvD;;AAGA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,SAASiF,QAAQA,CAAC/B,KAAK,EAAElD,GAAG,GAAG,IAAI,EAAE8F,UAAU,GAAG,CAAC,EAAE7F,OAAO,GAAG,KAAK,EAAE;EAEzE,IAAID,GAAG,KAAK,IAAI,EAAE;IACd;IACA;IACA,MAAMD,GAAG,GAAGmD,KAAK,CAACvH,IAAI,CAACqB,MAAM,CAAC,CAACC,CAAC,EAAEC,CAAC,KAAKD,CAAC,GAAGC,CAAC,EAAE,CAAC,CAAC;IACjD,MAAM8H,IAAI,GAAGjF,GAAG,GAAGmD,KAAK,CAACvH,IAAI,CAACmB,MAAM;IACpC;IACA,MAAMiI,GAAG,GAAG7G,IAAI,CAAC6H,IAAI,CAAC7C,KAAK,CAACvH,IAAI,CAACqB,MAAM,CAAC,CAACC,CAAC,EAAEC,CAAC,KAAKD,CAAC,GAAG,CAACC,CAAC,GAAG8H,IAAI,KAAK,CAAC,EAAE,CAAC,CAAC,IAAI9B,KAAK,CAACvH,IAAI,CAACmB,MAAM,GAAGgJ,UAAU,CAAC,CAAC;IAE7G,MAAME,UAAU,GAAG,IAAIxK,MAAM,CAAC0H,KAAK,CAACxH,IAAI,EAAE,CAACsJ,IAAI,CAAC,EAAE,CAAC,aAAa,CAAC;IACjE,MAAMiB,SAAS,GAAG,IAAIzK,MAAM,CAAC0H,KAAK,CAACxH,IAAI,EAAE,CAACqJ,GAAG,CAAC,EAAE,CAAC,aAAa,CAAC;IAE/D,OAAO,CAACkB,SAAS,EAAED,UAAU,CAAC;EAClC;;EAEA;EACAhG,GAAG,GAAG3C,SAAS,CAAC2C,GAAG,EAAEkD,KAAK,CAACzH,IAAI,CAACqB,MAAM,CAAC;EAEvC,MAAMkJ,UAAU,GAAGhB,IAAI,CAAC9B,KAAK,EAAElD,GAAG,EAAEC,OAAO,CAAC;;EAE5C;EACA,MAAMG,UAAU,GAAG8C,KAAK,CAACzH,IAAI,CAACkC,KAAK,CAAC,CAAC,CAAC,CAAC;EACvCyC,UAAU,CAACJ,GAAG,CAAC,GAAG,CAAC,CAAC,CAAC;;EAErB;EACA;EACA,MAAMK,MAAM,GAAG,IAAI6C,KAAK,CAACvH,IAAI,CAACE,WAAW,CAACqH,KAAK,CAACvH,IAAI,CAACmB,MAAM,GAAGoG,KAAK,CAACzH,IAAI,CAACuE,GAAG,CAAC,CAAC;;EAE9E;EACA,KAAK,IAAI7C,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG+F,KAAK,CAACvH,IAAI,CAACmB,MAAM,EAAE,EAAEK,CAAC,EAAE;IAExC;IACA,IAAImD,WAAW,GAAG,CAAC;IAEnB,KAAK,IAAIZ,CAAC,GAAGwD,KAAK,CAACzH,IAAI,CAACqB,MAAM,GAAG,CAAC,EAAE6C,GAAG,GAAGxC,CAAC,EAAEoD,gBAAgB,GAAG,CAAC,EAAEb,CAAC,IAAI,CAAC,EAAE,EAAEA,CAAC,EAAE;MAC5E,MAAM9D,IAAI,GAAGsH,KAAK,CAACzH,IAAI,CAACiE,CAAC,CAAC;MAC1B,IAAIA,CAAC,KAAKM,GAAG,EAAE;QACX,MAAM5D,KAAK,GAAGuD,GAAG,GAAG/D,IAAI;QACxB0E,WAAW,IAAIlE,KAAK,GAAGmE,gBAAgB;QACvCA,gBAAgB,IAAIH,UAAU,CAACV,CAAC,CAAC;MACrC;MACAC,GAAG,GAAGzB,IAAI,CAAC0B,KAAK,CAACD,GAAG,GAAG/D,IAAI,CAAC;IAChC;;IAEA;IACAyE,MAAM,CAACC,WAAW,CAAC,IAAI,CAAC4C,KAAK,CAACvH,IAAI,CAACwB,CAAC,CAAC,GAAG6I,UAAU,CAACrK,IAAI,CAAC2E,WAAW,CAAC,KAAK,CAAC;EAC9E;EAEA,KAAK,IAAInD,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGkD,MAAM,CAACvD,MAAM,EAAE,EAAEK,CAAC,EAAE;IACpCkD,MAAM,CAAClD,CAAC,CAAC,GAAGe,IAAI,CAAC6H,IAAI,CAAC1F,MAAM,CAAClD,CAAC,CAAC,IAAI+F,KAAK,CAACzH,IAAI,CAACuE,GAAG,CAAC,GAAG8F,UAAU,CAAC,CAAC;EACrE;EAEA,IAAI,CAAC7F,OAAO,EAAE;IACVG,UAAU,CAACI,MAAM,CAACR,GAAG,EAAE,CAAC,CAAC;EAC7B;EAEA,MAAMiG,SAAS,GAAG,IAAIzK,MAAM,CAAC0H,KAAK,CAACxH,IAAI,EAAE2E,MAAM,EAAED,UAAU,CAAC;EAE5D,OAAO,CAAC6F,SAAS,EAAED,UAAU,CAAC;AAClC;;AAGA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,SAAShB,IAAIA,CAAC9B,KAAK,EAAElD,GAAG,GAAG,IAAI,EAAEC,OAAO,GAAG,KAAK,EAAE;EAErD,IAAID,GAAG,KAAK,IAAI,EAAE;IACd;IACA;IACA,IAAI3B,GAAG,GAAG6E,KAAK,CAACvH,IAAI,CAACqB,MAAM,CAAC,CAACC,CAAC,EAAEC,CAAC,KAAKD,CAAC,GAAGC,CAAC,EAAE,CAAC,CAAC;IAC/C,OAAO,IAAI1B,MAAM,CAAC0H,KAAK,CAACxH,IAAI,EAAE,CAAC2C,GAAG,GAAG6E,KAAK,CAACvH,IAAI,CAACmB,MAAM,CAAC,EAAE,CAAC,aAAa,CAAC;EAC5E;;EAEA;EACAkD,GAAG,GAAG3C,SAAS,CAAC2C,GAAG,EAAEkD,KAAK,CAACzH,IAAI,CAACqB,MAAM,CAAC;;EAEvC;EACA,MAAMsD,UAAU,GAAG8C,KAAK,CAACzH,IAAI,CAACkC,KAAK,CAAC,CAAC,CAAC,CAAC;EACvCyC,UAAU,CAACJ,GAAG,CAAC,GAAG,CAAC,CAAC,CAAC;;EAErB;EACA;EACA,MAAMK,MAAM,GAAG,IAAI6C,KAAK,CAACvH,IAAI,CAACE,WAAW,CAACqH,KAAK,CAACvH,IAAI,CAACmB,MAAM,GAAGoG,KAAK,CAACzH,IAAI,CAACuE,GAAG,CAAC,CAAC;;EAE9E;EACA,KAAK,IAAI7C,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG+F,KAAK,CAACvH,IAAI,CAACmB,MAAM,EAAE,EAAEK,CAAC,EAAE;IAExC;IACA,IAAImD,WAAW,GAAG,CAAC;IAEnB,KAAK,IAAIZ,CAAC,GAAGwD,KAAK,CAACzH,IAAI,CAACqB,MAAM,GAAG,CAAC,EAAE6C,GAAG,GAAGxC,CAAC,EAAEoD,gBAAgB,GAAG,CAAC,EAAEb,CAAC,IAAI,CAAC,EAAE,EAAEA,CAAC,EAAE;MAC5E,MAAM9D,IAAI,GAAGsH,KAAK,CAACzH,IAAI,CAACiE,CAAC,CAAC;MAC1B,IAAIA,CAAC,KAAKM,GAAG,EAAE;QACX,MAAM5D,KAAK,GAAGuD,GAAG,GAAG/D,IAAI;QACxB0E,WAAW,IAAIlE,KAAK,GAAGmE,gBAAgB;QACvCA,gBAAgB,IAAIH,UAAU,CAACV,CAAC,CAAC;MACrC;MACAC,GAAG,GAAGzB,IAAI,CAAC0B,KAAK,CAACD,GAAG,GAAG/D,IAAI,CAAC;IAChC;;IAEA;IACAyE,MAAM,CAACC,WAAW,CAAC,IAAI4C,KAAK,CAACvH,IAAI,CAACwB,CAAC,CAAC;EACxC;EAEA,IAAI+F,KAAK,CAACzH,IAAI,CAACuE,GAAG,CAAC,KAAK,CAAC,EAAE;IACvB,KAAK,IAAI7C,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGkD,MAAM,CAACvD,MAAM,EAAE,EAAEK,CAAC,EAAE;MACpCkD,MAAM,CAAClD,CAAC,CAAC,GAAGkD,MAAM,CAAClD,CAAC,CAAC,GAAG+F,KAAK,CAACzH,IAAI,CAACuE,GAAG,CAAC;IAC3C;EACJ;EAEA,IAAI,CAACC,OAAO,EAAE;IACVG,UAAU,CAACI,MAAM,CAACR,GAAG,EAAE,CAAC,CAAC;EAC7B;EAEA,OAAO,IAAIxE,MAAM,CAAC0H,KAAK,CAACxH,IAAI,EAAE2E,MAAM,EAAED,UAAU,CAAC;AACrD;;AAGA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,SAAS8F,kBAAkBA,CAACC,MAAM,EAAE;EACvC,MAAM,CAACC,aAAa,EAAEC,YAAY,CAAC,GAAGF,MAAM,CAAC1K,IAAI;EAEjD,MAAM6K,WAAW,GAAG,CAACF,aAAa,GAAG,CAAC,EAAEC,YAAY,GAAG,CAAC,CAAC;EAEzD,MAAME,IAAI,GAAG,IAAI/K,MAAM,CACnB,SAAS,EACT,IAAIvB,YAAY,CAACqM,WAAW,CAAC,CAAC,CAAC,GAAGA,WAAW,CAAC,CAAC,CAAC,CAAC,CAACE,IAAI,CAACC,QAAQ,CAAC,EAChEH,WACJ,CAAC;EAED,MAAMI,KAAK,GAAG,IAAIlL,MAAM,CACpB,SAAS,EACT,IAAIvB,YAAY,CAACqM,WAAW,CAAC,CAAC,CAAC,GAAGA,WAAW,CAAC,CAAC,CAAC,CAAC,CAACE,IAAI,CAAC,CAAC,CAAC,CAAC,EAC1DF,WACJ,CAAC;;EAED;EACAC,IAAI,CAAC,CAAC,CAAC,CAAC5K,IAAI,CAAC,CAAC,CAAC,GAAG,CAAC;EAEnB,KAAK,IAAI+D,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG2G,YAAY,GAAG,CAAC,EAAE,EAAE3G,CAAC,EAAE;IACvC,KAAK,IAAIvC,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGiJ,aAAa,GAAG,CAAC,EAAE,EAAEjJ,CAAC,EAAE;MAExC,MAAMwJ,EAAE,GAAGJ,IAAI,CAACpJ,CAAC,GAAG,CAAC,CAAC,CAACuC,CAAC,GAAG,CAAC,CAAC,CAACnC,IAAI,CAAC,CAAC;MACpC,MAAMqJ,EAAE,GAAGL,IAAI,CAACpJ,CAAC,GAAG,CAAC,CAAC,CAACuC,CAAC,CAAC,CAACnC,IAAI,CAAC,CAAC;MAChC,MAAMsJ,EAAE,GAAGN,IAAI,CAACpJ,CAAC,CAAC,CAACuC,CAAC,GAAG,CAAC,CAAC,CAACnC,IAAI,CAAC,CAAC;MAEhC,IAAIuJ,CAAC,EAAEnB,CAAC;MACR,IAAIgB,EAAE,GAAGC,EAAE,IAAID,EAAE,GAAGE,EAAE,EAAE;QACpBC,CAAC,GAAGH,EAAE;QACNhB,CAAC,GAAG,CAAC;MACT,CAAC,MAAM,IAAIiB,EAAE,GAAGD,EAAE,IAAIC,EAAE,GAAGC,EAAE,EAAE;QAC3BC,CAAC,GAAGF,EAAE;QACNjB,CAAC,GAAG,CAAC;MACT,CAAC,MAAM;QACHmB,CAAC,GAAGD,EAAE;QACNlB,CAAC,GAAG,CAAC;MACT;MAEAY,IAAI,CAACpJ,CAAC,CAAC,CAACxB,IAAI,CAAC+D,CAAC,CAAC,GAAGyG,MAAM,CAAChJ,CAAC,GAAG,CAAC,CAAC,CAACuC,CAAC,GAAG,CAAC,CAAC,CAACnC,IAAI,CAAC,CAAC,GAAGuJ,CAAC;MACjDJ,KAAK,CAACvJ,CAAC,CAAC,CAACxB,IAAI,CAAC+D,CAAC,CAAC,GAAGiG,CAAC;IACxB;EACJ;;EAEA;EACA,IAAIxI,CAAC,GAAGiJ,aAAa;EACrB,IAAI1G,CAAC,GAAG2G,YAAY;;EAEpB;EACAK,KAAK,CAAC/K,IAAI,CAAC6K,IAAI,CAAC,CAAC,EAAE,CAAC,EAAEF,WAAW,CAAC,CAAC,CAAC,CAAC,EAAC;EACtC,KAAK,IAAInJ,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGmJ,WAAW,CAAC,CAAC,CAAC,EAAE,EAAEnJ,CAAC,EAAE;IAAE;IACvCuJ,KAAK,CAACvJ,CAAC,CAAC,CAACxB,IAAI,CAAC,CAAC,CAAC,GAAG,CAAC;EACxB;EAEA,IAAIoL,YAAY,GAAG,EAAE;EACrB,IAAIC,YAAY,GAAG,EAAE;EAErB,OAAO7J,CAAC,GAAG,CAAC,IAAIuC,CAAC,GAAG,CAAC,EAAE;IACnBqH,YAAY,CAACjI,IAAI,CAAC3B,CAAC,GAAG,CAAC,CAAC;IACxB6J,YAAY,CAAClI,IAAI,CAACY,CAAC,GAAG,CAAC,CAAC;IAExB,MAAMiG,CAAC,GAAGe,KAAK,CAACvJ,CAAC,CAAC,CAACuC,CAAC,CAAC,CAACnC,IAAI,CAAC,CAAC;IAC5B,QAAQoI,CAAC;MACL,KAAK,CAAC;QACF,EAAExI,CAAC;QAAE,EAAEuC,CAAC;QACR;MACJ,KAAK,CAAC;QACF,EAAEvC,CAAC;QACH;MACJ,KAAK,CAAC;QACF,EAAEuC,CAAC;QACH;MACJ;QACI,MAAM,IAAI9B,KAAK,CACX,4DAA4DT,CAAC,KAAKuC,CAAC,8BACvE,CAAC;IACT;EACJ;EAEAqH,YAAY,CAACE,OAAO,CAAC,CAAC;EACtBD,YAAY,CAACC,OAAO,CAAC,CAAC;EAEtB,OAAO,CAACF,YAAY,EAAEC,YAAY,CAAC;AAEvC;AAEA,SAASrG,YAAYA,CAAClF,IAAI,EAAE;EACxB,MAAM+D,MAAM,GAAG,IAAInF,KAAK,CAACoB,IAAI,CAACqB,MAAM,CAAC;EACrC,KAAK,IAAIK,CAAC,GAAG1B,IAAI,CAACqB,MAAM,GAAG,CAAC,EAAEoK,EAAE,GAAG,CAAC,EAAE/J,CAAC,IAAI,CAAC,EAAE,EAAEA,CAAC,EAAE;IAC/CqC,MAAM,CAACrC,CAAC,CAAC,GAAG+J,EAAE;IACdA,EAAE,IAAIzL,IAAI,CAAC0B,CAAC,CAAC;EACjB;EACA,OAAOqC,MAAM;AACjB;;AAEA;AACA;AACA;AACA;AACA,OAAO,SAAS2H,IAAIA,CAACvL,IAAI,EAAE;EACvB,MAAMwL,WAAW,GAAGxL,IAAI,CAACoB,MAAM,CAAC,CAACC,CAAC,EAAEC,CAAC,KAAKD,CAAC,GAAGC,CAAC,EAAE,CAAC,CAAC;EACnD,OAAO,IAAI1B,MAAM,CACb,OAAO,EACP,IAAIL,aAAa,CAACiM,WAAW,CAAC,CAACZ,IAAI,CAAC,EAAE,CAAC,EACvC5K,IACJ,CAAC;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA,OAAO,SAASyL,SAASA,CAACxE,MAAM,EAAE;EAC9B,OAAOsE,IAAI,CAACtE,MAAM,CAACpH,IAAI,CAAC;AAC5B;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,SAAS6L,mBAAmBA,CAACzE,MAAM,EAAE0E,SAAS,EAAE;EACnD,IAAI1E,MAAM,CAACpH,IAAI,CAACqB,MAAM,KAAK,CAAC,EAAE;IAC1B,MAAM,IAAIc,KAAK,CAAC,mCAAmC,CAAC;EACxD;EACA,IAAIiF,MAAM,CAACpH,IAAI,CAAC+H,EAAE,CAAC,CAAC,CAAC,CAAC,GAAG,CAAC,KAAK,CAAC,EAAE;IAC9B,MAAM,IAAI5F,KAAK,CAAC,0DAA0D,CAAC;EAC/E;EACA,IAAI,CAAC,CAAC,QAAQ,EAAE,SAAS,CAAC,CAACyH,QAAQ,CAACkC,SAAS,CAAC,EAAE;IAC5C,MAAM,IAAI3J,KAAK,CAAC,oDAAoD,CAAC;EACzE;EAEA,MAAM4J,MAAM,GAAGD,SAAS,KAAK,QAAQ;EACrC,MAAME,KAAK,GAAGD,MAAM,GAAG,MAAM,GAAG,OAAO;;EAEvC;EACA,MAAME,GAAG,GAAGF,MAAM,GAAGjN,SAAS,GAAGE,UAAU;EAC3C,MAAMkN,SAAS,GAAG9E,MAAM,CAAClH,IAAI;EAC7B,MAAMiM,UAAU,GAAG,IAAIF,GAAG,CAACC,SAAS,CAAC7K,MAAM,GAAG,CAAC,CAAC;;EAEhD;EACA,KAAK,IAAIK,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGwK,SAAS,CAAC7K,MAAM,EAAE,EAAEK,CAAC,EAAE;IACvC;IACA,MAAM0K,GAAG,GAAGF,SAAS,CAACxK,CAAC,CAAC,GAAG,CAAC,GAAG,CAAC,GAAG,CAAC;;IAEpC;IACA,MAAM2K,UAAU,GAAG5J,IAAI,CAAC0B,KAAK,CAACzC,CAAC,GAAG,CAAC,CAAC;IACpC,MAAM4K,WAAW,GAAG5K,CAAC,GAAG,CAAC;;IAEzB;IACAyK,UAAU,CAACE,UAAU,CAAC,IAAID,GAAG,IAAK,CAAC,GAAGE,WAAY;IAClD,IAAIP,MAAM,IAAIO,WAAW,KAAK,CAAC,EAAE;MAC7BH,UAAU,CAACE,UAAU,CAAC,IAAI,GAAG;IACjC;EACJ;EAAC;EAED,OAAO,IAAItM,MAAM,CAACiM,KAAK,EAAEG,UAAU,EAAE,CAAC/E,MAAM,CAACpH,IAAI,CAAC,CAAC,CAAC,EAAEoH,MAAM,CAACpH,IAAI,CAAC,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC;AAC9E","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}
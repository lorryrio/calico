{"ast":null,"code":"/**\n * @file Processors are used to prepare non-textual inputs (e.g., image or audio) for a model.\n * \n * **Example:** Using a `WhisperProcessor` to prepare an audio input for a model.\n * ```javascript\n * import { AutoProcessor, read_audio } from '@xenova/transformers';\n *\n * let processor = await AutoProcessor.from_pretrained('openai/whisper-tiny.en');\n * let audio = await read_audio('https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac', 16000);\n * let { input_features } = await processor(audio);\n * // Tensor {\n * //   data: Float32Array(240000) [0.4752984642982483, 0.5597258806228638, 0.56434166431427, ...],\n * //   dims: [1, 80, 3000],\n * //   type: 'float32',\n * //   size: 240000,\n * // }\n * ```\n * \n * @module processors\n */\nimport { Callable, calculateDimensions, calculateReflectOffset } from './utils/core.js';\nimport { getModelJSON } from './utils/hub.js';\nimport { min, max, softmax, bankers_round } from './utils/maths.js';\nimport { Tensor, permute, cat, interpolate, stack } from './utils/tensor.js';\nimport { RawImage } from './utils/image.js';\nimport { window_function, spectrogram, mel_filter_bank } from './utils/audio.js';\n\n// Helper functions\n\n/**\n * Converts bounding boxes from center format to corners format.\n * \n * @param {number[]} arr The coordinate for the center of the box and its width, height dimensions (center_x, center_y, width, height)\n * @returns {number[]} The coodinates for the top-left and bottom-right corners of the box (top_left_x, top_left_y, bottom_right_x, bottom_right_y)\n */\nfunction center_to_corners_format([centerX, centerY, width, height]) {\n  return [centerX - width / 2, centerY - height / 2, centerX + width / 2, centerY + height / 2];\n}\n\n/**\n * Post-processes the outputs of the model (for object detection).\n * @param {Object} outputs The outputs of the model that must be post-processed\n * @param {Tensor} outputs.logits The logits\n * @param {Tensor} outputs.pred_boxes The predicted boxes.\n * @param {number} [threshold=0.5] The threshold to use for the scores.\n * @param {number[][]} [target_sizes=null] The sizes of the original images.\n * @param {boolean} [is_zero_shot=false] Whether zero-shot object detection was performed.\n * @return {Object[]} An array of objects containing the post-processed outputs.\n * @private\n */\nfunction post_process_object_detection(outputs, threshold = 0.5, target_sizes = null, is_zero_shot = false) {\n  const out_logits = outputs.logits;\n  const out_bbox = outputs.pred_boxes;\n  const [batch_size, num_boxes, num_classes] = out_logits.dims;\n  if (target_sizes !== null && target_sizes.length !== batch_size) {\n    throw Error(\"Make sure that you pass in as many target sizes as the batch dimension of the logits\");\n  }\n  let toReturn = [];\n  for (let i = 0; i < batch_size; ++i) {\n    let target_size = target_sizes !== null ? target_sizes[i] : null;\n    let info = {\n      boxes: [],\n      classes: [],\n      scores: []\n    };\n    let logits = out_logits[i];\n    let bbox = out_bbox[i];\n    for (let j = 0; j < num_boxes; ++j) {\n      let logit = logits[j];\n      let indices = [];\n      let probs;\n      if (is_zero_shot) {\n        // Get indices of classes with high enough probability\n        probs = logit.sigmoid().data;\n        for (let k = 0; k < probs.length; ++k) {\n          if (probs[k] > threshold) {\n            indices.push(k);\n          }\n        }\n      } else {\n        // Get most probable class\n        let maxIndex = max(logit.data)[1];\n        if (maxIndex === num_classes - 1) {\n          // This is the background class, skip it\n          continue;\n        }\n        indices.push(maxIndex);\n\n        // Compute softmax over classes\n        probs = softmax(logit.data);\n      }\n      for (const index of indices) {\n        // Some class has a high enough probability\n        /** @type {number[]} */\n        let box = bbox[j].data;\n\n        // convert to [x0, y0, x1, y1] format\n        box = center_to_corners_format(box);\n        if (target_size !== null) {\n          box = box.map((x, i) => x * target_size[(i + 1) % 2]);\n        }\n        info.boxes.push(box);\n        info.classes.push(index);\n        info.scores.push(probs[index]);\n      }\n    }\n    toReturn.push(info);\n  }\n  return toReturn;\n}\n\n/**\n * Named tuple to indicate the order we are using is (height x width), even though\n * the Graphicsâ€™ industry standard is (width x height).\n * @typedef {[height: number, width: number]} HeightWidth\n */\n\n/**\n * Helper function to validate audio inputs.\n * @param {any} audio The audio data.\n * @param {string} feature_extractor The name of the feature extractor.\n * @private\n */\nfunction validate_audio_inputs(audio, feature_extractor) {\n  if (!(audio instanceof Float32Array || audio instanceof Float64Array)) {\n    throw new Error(`${feature_extractor} expects input to be a Float32Array or a Float64Array, but got ${audio?.constructor?.name ?? typeof audio} instead. ` + `If using the feature extractor directly, remember to use \\`read_audio(url, sampling_rate)\\` to obtain the raw audio data of the file/url.`);\n  }\n}\n\n/**\n * Helper function to constrain a value to be a multiple of a number.\n * @param {number} val The value to constrain.\n * @param {number} multiple The number to constrain to.\n * @param {number} [minVal=0] The minimum value to constrain to.\n * @param {number} [maxVal=null] The maximum value to constrain to.\n * @returns {number} The constrained value.\n * @private\n */\nfunction constraint_to_multiple_of(val, multiple, minVal = 0, maxVal = null) {\n  const a = val / multiple;\n  let x = bankers_round(a) * multiple;\n  if (maxVal !== null && x > maxVal) {\n    x = Math.floor(a) * multiple;\n  }\n  if (x < minVal) {\n    x = Math.ceil(a) * multiple;\n  }\n  return x;\n}\n\n/**\n * Rounds the height and width down to the closest multiple of size_divisibility\n * @param {[number, number]} size The size of the image\n * @param {number} divisor The divisor to use.\n * @returns {[number, number]} The rounded size.\n */\nfunction enforce_size_divisibility([width, height], divisor) {\n  return [Math.max(Math.floor(width / divisor), 1) * divisor, Math.max(Math.floor(height / divisor), 1) * divisor];\n}\n\n/**\n * Base class for feature extractors.\n *\n * @extends Callable\n */\nexport class FeatureExtractor extends Callable {\n  /**\n   * Constructs a new FeatureExtractor instance.\n   *\n   * @param {Object} config The configuration for the feature extractor.\n   */\n  constructor(config) {\n    super();\n    this.config = config;\n  }\n}\n\n/**\n * @typedef {object} ImageFeatureExtractorResult\n * @property {Tensor} pixel_values The pixel values of the batched preprocessed images.\n * @property {HeightWidth[]} original_sizes Array of two-dimensional tuples like [[480, 640]].\n * @property {HeightWidth[]} reshaped_input_sizes Array of two-dimensional tuples like [[1000, 1330]].\n */\n\n/**\n * Feature extractor for image models.\n *\n * @extends FeatureExtractor\n */\nexport class ImageFeatureExtractor extends FeatureExtractor {\n  /**\n   * Constructs a new ImageFeatureExtractor instance.\n   *\n   * @param {Object} config The configuration for the feature extractor.\n   * @param {number[]} config.image_mean The mean values for image normalization.\n   * @param {number[]} config.image_std The standard deviation values for image normalization.\n   * @param {boolean} config.do_rescale Whether to rescale the image pixel values to the [0,1] range.\n   * @param {number} config.rescale_factor The factor to use for rescaling the image pixel values.\n   * @param {boolean} config.do_normalize Whether to normalize the image pixel values.\n   * @param {boolean} config.do_resize Whether to resize the image.\n   * @param {number} config.resample What method to use for resampling.\n   * @param {number|Object} config.size The size to resize the image to.\n   * @param {boolean} [config.do_flip_channel_order=false] Whether to flip the color channels from RGB to BGR.\n   * Can be overridden by the `do_flip_channel_order` parameter in the `preprocess` method.\n   */\n  constructor(config) {\n    super(config);\n    this.image_mean = this.config.image_mean ?? this.config.mean;\n    this.image_std = this.config.image_std ?? this.config.std;\n    this.resample = this.config.resample ?? 2; // 2 => bilinear\n    this.do_rescale = this.config.do_rescale ?? true;\n    this.rescale_factor = this.config.rescale_factor ?? 1 / 255;\n    this.do_normalize = this.config.do_normalize;\n    this.do_resize = this.config.do_resize;\n    this.do_thumbnail = this.config.do_thumbnail;\n    this.size = this.config.size;\n    this.size_divisibility = this.config.size_divisibility ?? this.config.size_divisor;\n    this.do_center_crop = this.config.do_center_crop;\n    this.crop_size = this.config.crop_size;\n    this.do_convert_rgb = this.config.do_convert_rgb ?? true;\n    this.do_crop_margin = this.config.do_crop_margin;\n    this.pad_size = this.config.pad_size;\n    this.do_pad = this.config.do_pad;\n    if (this.do_pad && !this.pad_size && this.size && this.size.width !== undefined && this.size.height !== undefined) {\n      // Should pad, but no pad size specified\n      // We infer the pad size from the resize size\n      this.pad_size = this.size;\n    }\n    this.do_flip_channel_order = this.config.do_flip_channel_order ?? false;\n  }\n\n  /**\n   * Resize the image to make a thumbnail. The image is resized so that no dimension is larger than any\n   * corresponding dimension of the specified size.\n   * @param {RawImage} image The image to be resized.\n   * @param {{height:number, width:number}} size The size `{\"height\": h, \"width\": w}` to resize the image to.\n   * @param {string | 0 | 1 | 2 | 3 | 4 | 5} [resample=2] The resampling filter to use.\n   * @returns {Promise<RawImage>} The resized image.\n   */\n  async thumbnail(image, size, resample = 2) {\n    const input_height = image.height;\n    const input_width = image.width;\n    const output_height = size.height;\n    const output_width = size.width;\n\n    // We always resize to the smallest of either the input or output size.\n    let height = Math.min(input_height, output_height);\n    let width = Math.min(input_width, output_width);\n    if (height === input_height && width === input_width) {\n      return image;\n    }\n    if (input_height > input_width) {\n      width = Math.floor(input_width * height / input_height);\n    } else if (input_width > input_height) {\n      height = Math.floor(input_height * width / input_width);\n    }\n    return await image.resize(width, height, {\n      resample\n    });\n  }\n\n  /**\n   * Crops the margin of the image. Gray pixels are considered margin (i.e., pixels with a value below the threshold).\n   * @param {RawImage} image The image to be cropped.\n   * @param {number} gray_threshold Value below which pixels are considered to be gray.\n   * @returns {Promise<RawImage>} The cropped image.\n   */\n  async crop_margin(image, gray_threshold = 200) {\n    const gray_image = image.clone().grayscale();\n    const minValue = min(gray_image.data)[0];\n    const maxValue = max(gray_image.data)[0];\n    const diff = maxValue - minValue;\n    if (diff === 0) {\n      return image;\n    }\n    const threshold = gray_threshold / 255;\n    let x_min = gray_image.width,\n      y_min = gray_image.height,\n      x_max = 0,\n      y_max = 0;\n    for (let j = 0; j < gray_image.height; ++j) {\n      const row = j * gray_image.width;\n      for (let i = 0; i < gray_image.width; ++i) {\n        if ((gray_image.data[row + i] - minValue) / diff < threshold) {\n          // We have a non-zero pixel, so we update the min/max values accordingly\n          x_min = Math.min(x_min, i);\n          y_min = Math.min(y_min, j);\n          x_max = Math.max(x_max, i);\n          y_max = Math.max(y_max, j);\n        }\n      }\n    }\n    image = await image.crop([x_min, y_min, x_max, y_max]);\n    return image;\n  }\n\n  /**\n   * Pad the image by a certain amount.\n   * @param {Float32Array} pixelData The pixel data to pad.\n   * @param {number[]} imgDims The dimensions of the image (height, width, channels).\n   * @param {{width:number; height:number}|number} padSize The dimensions of the padded image.\n   * @param {Object} options The options for padding.\n   * @param {'constant'|'symmetric'} [options.mode='constant'] The type of padding to add.\n   * @param {boolean} [options.center=false] Whether to center the image.\n   * @param {number} [options.constant_values=0] The constant value to use for padding.\n   * @returns {[Float32Array, number[]]} The padded pixel data and image dimensions.\n   */\n  pad_image(pixelData, imgDims, padSize, {\n    mode = 'constant',\n    center = false,\n    constant_values = 0\n  } = {}) {\n    const [imageHeight, imageWidth, imageChannels] = imgDims;\n    let paddedImageWidth, paddedImageHeight;\n    if (typeof padSize === 'number') {\n      paddedImageWidth = padSize;\n      paddedImageHeight = padSize;\n    } else {\n      paddedImageWidth = padSize.width;\n      paddedImageHeight = padSize.height;\n    }\n\n    // Only add padding if there is a difference in size\n    if (paddedImageWidth !== imageWidth || paddedImageHeight !== imageHeight) {\n      const paddedPixelData = new Float32Array(paddedImageWidth * paddedImageHeight * imageChannels);\n      if (Array.isArray(constant_values)) {\n        // Fill with constant values, cycling through the array\n        for (let i = 0; i < paddedPixelData.length; ++i) {\n          paddedPixelData[i] = constant_values[i % imageChannels];\n        }\n      } else if (constant_values !== 0) {\n        paddedPixelData.fill(constant_values);\n      }\n      const [left, top] = center ? [Math.floor((paddedImageWidth - imageWidth) / 2), Math.floor((paddedImageHeight - imageHeight) / 2)] : [0, 0];\n\n      // Copy the original image into the padded image\n      for (let i = 0; i < imageHeight; ++i) {\n        const a = (i + top) * paddedImageWidth;\n        const b = i * imageWidth;\n        for (let j = 0; j < imageWidth; ++j) {\n          const c = (a + j + left) * imageChannels;\n          const d = (b + j) * imageChannels;\n          for (let k = 0; k < imageChannels; ++k) {\n            paddedPixelData[c + k] = pixelData[d + k];\n          }\n        }\n      }\n      if (mode === 'symmetric') {\n        if (center) {\n          throw new Error('`center` padding is not supported when `mode` is set to `symmetric`.');\n          // TODO: Implement this\n        }\n        const h1 = imageHeight - 1;\n        const w1 = imageWidth - 1;\n        for (let i = 0; i < paddedImageHeight; ++i) {\n          const a = i * paddedImageWidth;\n          const b = calculateReflectOffset(i, h1) * imageWidth;\n          for (let j = 0; j < paddedImageWidth; ++j) {\n            if (i < imageHeight && j < imageWidth) continue; // Do not overwrite original image\n            const c = (a + j) * imageChannels;\n            const d = (b + calculateReflectOffset(j, w1)) * imageChannels;\n\n            // Copy channel-wise\n            for (let k = 0; k < imageChannels; ++k) {\n              paddedPixelData[c + k] = pixelData[d + k];\n            }\n          }\n        }\n      }\n\n      // Update pixel data and image dimensions\n      pixelData = paddedPixelData;\n      imgDims = [paddedImageHeight, paddedImageWidth, imageChannels];\n    }\n    return [pixelData, imgDims];\n  }\n\n  /**\n   * Rescale the image' pixel values by `this.rescale_factor`.\n   * @param {Float32Array} pixelData The pixel data to rescale.\n   * @returns {void}\n   */\n  rescale(pixelData) {\n    for (let i = 0; i < pixelData.length; ++i) {\n      pixelData[i] = this.rescale_factor * pixelData[i];\n    }\n  }\n\n  /**\n   * Find the target (width, height) dimension of the output image after\n   * resizing given the input image and the desired size.\n   * @param {RawImage} image The image to resize.\n   * @param {any} size The size to use for resizing the image. \n   * @returns {[number, number]} The target (width, height) dimension of the output image after resizing.\n   */\n  get_resize_output_image_size(image, size) {\n    // `size` comes in many forms, so we need to handle them all here:\n    // 1. `size` is an integer, in which case we resize the image to be a square \n\n    const [srcWidth, srcHeight] = image.size;\n    let shortest_edge;\n    let longest_edge;\n    if (this.do_thumbnail) {\n      // NOTE: custom logic for `Donut` models\n      const {\n        height,\n        width\n      } = size;\n      shortest_edge = Math.min(height, width);\n    }\n    // Support both formats for backwards compatibility\n    else if (Number.isInteger(size)) {\n      shortest_edge = size;\n      longest_edge = this.config.max_size ?? shortest_edge;\n    } else if (size !== undefined) {\n      // Extract known properties from `size`\n      shortest_edge = size.shortest_edge;\n      longest_edge = size.longest_edge;\n    }\n\n    // If `longest_edge` and `shortest_edge` are set, maintain aspect ratio and resize to `shortest_edge`\n    // while keeping the largest dimension <= `longest_edge`\n    if (shortest_edge !== undefined || longest_edge !== undefined) {\n      // http://opensourcehacker.com/2011/12/01/calculate-aspect-ratio-conserving-resize-for-images-in-javascript/\n      // Try resize so that shortest edge is `shortest_edge` (target)\n      const shortResizeFactor = shortest_edge === undefined ? 1 // If `shortest_edge` is not set, don't upscale\n      : Math.max(shortest_edge / srcWidth, shortest_edge / srcHeight);\n      const newWidth = srcWidth * shortResizeFactor;\n      const newHeight = srcHeight * shortResizeFactor;\n\n      // The new width and height might be greater than `longest_edge`, so\n      // we downscale again to ensure the largest dimension is `longest_edge` \n      const longResizeFactor = longest_edge === undefined ? 1 // If `longest_edge` is not set, don't downscale\n      : Math.min(longest_edge / newWidth, longest_edge / newHeight);\n\n      // To avoid certain floating point precision issues, we round to 2 decimal places\n      let finalWidth = Math.floor(Number((newWidth * longResizeFactor).toFixed(2)));\n      let finalHeight = Math.floor(Number((newHeight * longResizeFactor).toFixed(2)));\n      if (this.size_divisibility !== undefined) {\n        [finalWidth, finalHeight] = enforce_size_divisibility([finalWidth, finalHeight], this.size_divisibility);\n      }\n      return [finalWidth, finalHeight];\n    } else if (size !== undefined && size.width !== undefined && size.height !== undefined) {\n      // If `width` and `height` are set, resize to those dimensions\n\n      let newWidth = size.width;\n      let newHeight = size.height;\n\n      // Custom for DPT models\n      if (this.config.keep_aspect_ratio && this.config.ensure_multiple_of) {\n        // determine new height and width\n        let scale_height = newHeight / srcHeight;\n        let scale_width = newWidth / srcWidth;\n\n        // scale as little as possible\n        if (Math.abs(1 - scale_width) < Math.abs(1 - scale_height)) {\n          // fit width\n          scale_height = scale_width;\n        } else {\n          // fit height\n          scale_width = scale_height;\n        }\n        newHeight = constraint_to_multiple_of(scale_height * srcHeight, this.config.ensure_multiple_of);\n        newWidth = constraint_to_multiple_of(scale_width * srcWidth, this.config.ensure_multiple_of);\n      }\n      return [newWidth, newHeight];\n    } else if (this.size_divisibility !== undefined) {\n      return enforce_size_divisibility([srcWidth, srcHeight], this.size_divisibility);\n    } else {\n      throw new Error(`Could not resize image due to unsupported \\`this.size\\` option in config: ${JSON.stringify(size)}`);\n    }\n  }\n\n  /**\n   * Resizes the image.\n   * @param {RawImage} image The image to resize.\n   * @returns {Promise<RawImage>} The resized image.\n   */\n  async resize(image) {\n    const [newWidth, newHeight] = this.get_resize_output_image_size(image, this.size);\n    return await image.resize(newWidth, newHeight, {\n      resample: this.resample\n    });\n  }\n\n  /**\n   * @typedef {object} PreprocessedImage\n   * @property {HeightWidth} original_size The original size of the image.\n   * @property {HeightWidth} reshaped_input_size The reshaped input size of the image.\n   * @property {Tensor} pixel_values The pixel values of the preprocessed image.\n   */\n\n  /**\n   * Preprocesses the given image.\n   *\n   * @param {RawImage} image The image to preprocess.\n   * @param {Object} overrides The overrides for the preprocessing options.\n   * @returns {Promise<PreprocessedImage>} The preprocessed image.\n   */\n  async preprocess(image, {\n    do_normalize = null,\n    do_pad = null,\n    do_convert_rgb = null,\n    do_convert_grayscale = null,\n    do_flip_channel_order = null\n  } = {}) {\n    if (this.do_crop_margin) {\n      // NOTE: Specific to nougat processors. This is done before resizing,\n      // and can be interpreted as a pre-preprocessing step.\n      image = await this.crop_margin(image);\n    }\n    const [srcWidth, srcHeight] = image.size; // original image size\n\n    // Convert image to RGB if specified in config.\n    if (do_convert_rgb ?? this.do_convert_rgb) {\n      image = image.rgb();\n    } else if (do_convert_grayscale) {\n      image = image.grayscale();\n    }\n\n    // TODO:\n    // For efficiency reasons, it might be best to merge the resize and center crop operations into one.\n\n    // Resize all images\n    if (this.do_resize) {\n      image = await this.resize(image);\n    }\n\n    // Resize the image using thumbnail method.\n    if (this.do_thumbnail) {\n      image = await this.thumbnail(image, this.size, this.resample);\n    }\n    if (this.do_center_crop) {\n      let crop_width;\n      let crop_height;\n      if (Number.isInteger(this.crop_size)) {\n        crop_width = this.crop_size;\n        crop_height = this.crop_size;\n      } else {\n        crop_width = this.crop_size.width;\n        crop_height = this.crop_size.height;\n      }\n      image = await image.center_crop(crop_width, crop_height);\n    }\n\n    /** @type {HeightWidth} */\n    const reshaped_input_size = [image.height, image.width];\n\n    // NOTE: All pixel-level manipulation (i.e., modifying `pixelData`)\n    // occurs with data in the hwc format (height, width, channels), \n    // to emulate the behavior of the original Python code (w/ numpy).\n    let pixelData = Float32Array.from(image.data);\n    let imgDims = [image.height, image.width, image.channels];\n    if (this.do_rescale) {\n      this.rescale(pixelData);\n    }\n    if (do_normalize ?? this.do_normalize) {\n      let image_mean = this.image_mean;\n      if (!Array.isArray(this.image_mean)) {\n        image_mean = new Array(image.channels).fill(image_mean);\n      }\n      let image_std = this.image_std;\n      if (!Array.isArray(this.image_std)) {\n        image_std = new Array(image.channels).fill(image_mean);\n      }\n      if (image_mean.length !== image.channels || image_std.length !== image.channels) {\n        throw new Error(`When set to arrays, the length of \\`image_mean\\` (${image_mean.length}) and \\`image_std\\` (${image_std.length}) must match the number of channels in the image (${image.channels}).`);\n      }\n      for (let i = 0; i < pixelData.length; i += image.channels) {\n        for (let j = 0; j < image.channels; ++j) {\n          pixelData[i + j] = (pixelData[i + j] - image_mean[j]) / image_std[j];\n        }\n      }\n    }\n\n    // do padding after rescaling/normalizing\n    if (do_pad ?? this.do_pad) {\n      if (this.pad_size) {\n        const padded = this.pad_image(pixelData, [image.height, image.width, image.channels], this.pad_size);\n        [pixelData, imgDims] = padded; // Update pixel data and image dimensions\n      } else if (this.size_divisibility) {\n        const [paddedWidth, paddedHeight] = enforce_size_divisibility([imgDims[1], imgDims[0]], this.size_divisibility);\n        [pixelData, imgDims] = this.pad_image(pixelData, imgDims, {\n          width: paddedWidth,\n          height: paddedHeight\n        });\n      }\n    }\n    if (do_flip_channel_order ?? this.do_flip_channel_order) {\n      if (imgDims[2] !== 3) {\n        throw new Error('Flipping channel order is only supported for RGB images.');\n      }\n      // Convert RGB to BGR\n      for (let i = 0; i < pixelData.length; i += 3) {\n        const temp = pixelData[i];\n        pixelData[i] = pixelData[i + 2];\n        pixelData[i + 2] = temp;\n      }\n    }\n    const pixel_values = new Tensor('float32', pixelData, imgDims).permute(2, 0, 1); // convert to channel dimension format (hwc -> chw)\n\n    return {\n      original_size: [srcHeight, srcWidth],\n      reshaped_input_size: reshaped_input_size,\n      pixel_values: pixel_values\n    };\n  }\n\n  /**\n   * Calls the feature extraction process on an array of images,\n   * preprocesses each image, and concatenates the resulting\n   * features into a single Tensor.\n   * @param {RawImage[]} images The image(s) to extract features from.\n   * @param {...any} args Additional arguments.\n   * @returns {Promise<ImageFeatureExtractorResult>} An object containing the concatenated pixel values (and other metadata) of the preprocessed images.\n   */\n  async _call(images, ...args) {\n    if (!Array.isArray(images)) {\n      images = [images];\n    }\n    /** @type {PreprocessedImage[]} */\n    const imageData = await Promise.all(images.map(x => this.preprocess(x)));\n\n    // Stack pixel values\n    const pixel_values = stack(imageData.map(x => x.pixel_values), 0);\n    return {\n      pixel_values: pixel_values,\n      // Original sizes of images\n      original_sizes: imageData.map(x => x.original_size),\n      // Reshaped sizes of images, before padding or cropping\n      reshaped_input_sizes: imageData.map(x => x.reshaped_input_size)\n    };\n  }\n}\nexport class SegformerFeatureExtractor extends ImageFeatureExtractor {\n  /**\n   * Converts the output of `SegformerForSemanticSegmentation` into semantic segmentation maps.\n   * @param {*} outputs Raw outputs of the model.\n   * @param {number[][]} [target_sizes=null] List of tuples corresponding to the requested final size\n   * (height, width) of each prediction. If unset, predictions will not be resized.\n   * @returns {{segmentation: Tensor; labels: number[]}[]} The semantic segmentation maps.\n   */\n  post_process_semantic_segmentation(outputs, target_sizes = null) {\n    const logits = outputs.logits;\n    const batch_size = logits.dims[0];\n    if (target_sizes !== null && target_sizes.length !== batch_size) {\n      throw Error(\"Make sure that you pass in as many target sizes as the batch dimension of the logits\");\n    }\n    const toReturn = [];\n    for (let i = 0; i < batch_size; ++i) {\n      const target_size = target_sizes !== null ? target_sizes[i] : null;\n      let data = logits[i];\n\n      // 1. If target_size is not null, we need to resize the masks to the target size\n      if (target_size !== null) {\n        // resize the masks to the target size\n        data = interpolate(data, target_size, 'bilinear', false);\n      }\n      const [height, width] = target_size ?? data.dims.slice(-2);\n      const segmentation = new Tensor('int32', new Int32Array(height * width), [height, width]);\n\n      // Buffer to store current largest value\n      const buffer = data[0].data;\n      for (let j = 1; j < data.dims[0]; ++j) {\n        const row = data[j].data;\n        for (let k = 0; k < row.length; ++k) {\n          if (row[k] > buffer[k]) {\n            buffer[k] = row[k];\n            segmentation.data[k] = j;\n          }\n        }\n      }\n\n      // Store which objects have labels\n      // This is much more efficient that creating a set of the final values\n      const hasLabel = new Array(data.dims[0]);\n      const out = segmentation.data;\n      for (let j = 0; j < out.length; ++j) {\n        const index = out[j];\n        hasLabel[index] = index;\n      }\n      /** @type {number[]} The unique list of labels that were detected */\n      const labels = hasLabel.filter(x => x !== undefined);\n      toReturn.push({\n        segmentation,\n        labels\n      });\n    }\n    return toReturn;\n  }\n}\nexport class DPTFeatureExtractor extends ImageFeatureExtractor {}\nexport class DPTImageProcessor extends DPTFeatureExtractor {} // NOTE: extends DPTFeatureExtractor\nexport class BitImageProcessor extends ImageFeatureExtractor {}\nexport class GLPNFeatureExtractor extends ImageFeatureExtractor {}\nexport class CLIPFeatureExtractor extends ImageFeatureExtractor {}\nexport class ChineseCLIPFeatureExtractor extends ImageFeatureExtractor {}\nexport class SiglipImageProcessor extends ImageFeatureExtractor {}\nexport class ConvNextFeatureExtractor extends ImageFeatureExtractor {\n  constructor(config) {\n    super(config);\n\n    /**\n     * Percentage of the image to crop. Only has an effect if this.size < 384.\n     */\n    this.crop_pct = this.config.crop_pct ?? 224 / 256;\n  }\n  async resize(image) {\n    const shortest_edge = this.size?.shortest_edge;\n    if (shortest_edge === undefined) {\n      throw new Error(`Size dictionary must contain 'shortest_edge' key.`);\n    }\n    if (shortest_edge < 384) {\n      // maintain same ratio, resizing shortest edge to shortest_edge/crop_pct\n      const resize_shortest_edge = Math.floor(shortest_edge / this.crop_pct);\n      const [newWidth, newHeight] = this.get_resize_output_image_size(image, {\n        shortest_edge: resize_shortest_edge\n      });\n      image = await image.resize(newWidth, newHeight, {\n        resample: this.resample\n      });\n\n      // then crop to (shortest_edge, shortest_edge)\n      image = await image.center_crop(shortest_edge, shortest_edge);\n    } else {\n      // warping (no cropping) when evaluated at 384 or larger\n      image = await image.resize(shortest_edge, shortest_edge, {\n        resample: this.resample\n      });\n    }\n    return image;\n  }\n}\nexport class ConvNextImageProcessor extends ConvNextFeatureExtractor {} // NOTE extends ConvNextFeatureExtractor\nexport class ViTFeatureExtractor extends ImageFeatureExtractor {}\nexport class ViTImageProcessor extends ImageFeatureExtractor {}\nexport class EfficientNetImageProcessor extends ImageFeatureExtractor {\n  constructor(config) {\n    super(config);\n    this.include_top = this.config.include_top ?? true;\n    if (this.include_top) {\n      this.image_std = this.image_std.map(x => x * x);\n    }\n  }\n}\nexport class MobileViTFeatureExtractor extends ImageFeatureExtractor {}\nexport class MobileViTImageProcessor extends MobileViTFeatureExtractor {} // NOTE extends MobileViTFeatureExtractor\nexport class OwlViTFeatureExtractor extends ImageFeatureExtractor {\n  /** @type {post_process_object_detection} */\n  post_process_object_detection(...args) {\n    return post_process_object_detection(...args);\n  }\n}\nexport class Owlv2ImageProcessor extends OwlViTFeatureExtractor {} // NOTE extends OwlViTFeatureExtractor\n\nexport class DeiTFeatureExtractor extends ImageFeatureExtractor {}\nexport class BeitFeatureExtractor extends ImageFeatureExtractor {}\nexport class DonutFeatureExtractor extends ImageFeatureExtractor {\n  pad_image(pixelData, imgDims, padSize, options = {}) {\n    const [imageHeight, imageWidth, imageChannels] = imgDims;\n    let image_mean = this.image_mean;\n    if (!Array.isArray(this.image_mean)) {\n      image_mean = new Array(imageChannels).fill(image_mean);\n    }\n    let image_std = this.image_std;\n    if (!Array.isArray(image_std)) {\n      image_std = new Array(imageChannels).fill(image_mean);\n    }\n    const constant_values = image_mean.map((x, i) => -x / image_std[i]);\n    return super.pad_image(pixelData, imgDims, padSize, {\n      center: true,\n      // Since normalization is done after padding, we need to use certain constant values to ensure the same behaviour is observed.\n      // For more information, see https://github.com/huggingface/transformers/blob/main/src/transformers/models/donut/image_processing_donut.py#L433-L451\n      constant_values: constant_values,\n      ...options\n    });\n  }\n}\nexport class NougatImageProcessor extends DonutFeatureExtractor {} // NOTE extends DonutFeatureExtractor\n\n/**\n * @typedef {object} DetrFeatureExtractorResultProps\n * @property {Tensor} pixel_mask\n * @typedef {ImageFeatureExtractorResult & DetrFeatureExtractorResultProps} DetrFeatureExtractorResult\n */\n\n/**\n * Detr Feature Extractor.\n *\n * @extends ImageFeatureExtractor\n */\nexport class DetrFeatureExtractor extends ImageFeatureExtractor {\n  /**\n   * Calls the feature extraction process on an array of images, preprocesses\n   * each image, and concatenates the resulting features into a single Tensor.\n   * @param {RawImage[]} images The image(s) to extract features from.\n   * @returns {Promise<DetrFeatureExtractorResult>} An object containing the concatenated pixel values of the preprocessed images.\n   */\n  async _call(images) {\n    const result = await super._call(images);\n\n    // TODO support differently-sized images, for now assume all images are the same size.\n    // TODO support different mask sizes (not just 64x64)\n    // Currently, just fill pixel mask with 1s\n    const maskSize = [result.pixel_values.dims[0], 64, 64];\n    const pixel_mask = new Tensor('int64', new BigInt64Array(maskSize.reduce((a, b) => a * b)).fill(1n), maskSize);\n    return {\n      ...result,\n      pixel_mask\n    };\n  }\n\n  /**\n   * Post-processes the outputs of the model (for object detection).\n   * @param {Object} outputs The outputs of the model that must be post-processed\n   * @param {Tensor} outputs.logits The logits\n   * @param {Tensor} outputs.pred_boxes The predicted boxes.\n   * @return {Object[]} An array of objects containing the post-processed outputs.\n   */\n\n  /** @type {post_process_object_detection} */\n  post_process_object_detection(...args) {\n    return post_process_object_detection(...args);\n  }\n\n  /**\n   * Binarize the given masks using `object_mask_threshold`, it returns the associated values of `masks`, `scores` and `labels`.\n   * @param {Tensor} class_logits The class logits.\n   * @param {Tensor} mask_logits The mask logits.\n   * @param {number} object_mask_threshold A number between 0 and 1 used to binarize the masks.\n   * @param {number} num_labels The number of labels.\n   * @returns {[Tensor[], number[], number[]]} The binarized masks, the scores, and the labels.\n   */\n  remove_low_and_no_objects(class_logits, mask_logits, object_mask_threshold, num_labels) {\n    let mask_probs_item = [];\n    let pred_scores_item = [];\n    let pred_labels_item = [];\n    for (let j = 0; j < class_logits.dims[0]; ++j) {\n      let cls = class_logits[j];\n      let mask = mask_logits[j];\n      let pred_label = max(cls.data)[1];\n      if (pred_label === num_labels) {\n        // Is the background, so we ignore it\n        continue;\n      }\n      let scores = softmax(cls.data);\n      let pred_score = scores[pred_label];\n      if (pred_score > object_mask_threshold) {\n        mask_probs_item.push(mask);\n        pred_scores_item.push(pred_score);\n        pred_labels_item.push(pred_label);\n      }\n    }\n    return [mask_probs_item, pred_scores_item, pred_labels_item];\n  }\n\n  /**\n   * Checks whether the segment is valid or not.\n   * @param {Int32Array} mask_labels Labels for each pixel in the mask.\n   * @param {Tensor[]} mask_probs Probabilities for each pixel in the masks.\n   * @param {number} k The class id of the segment.\n   * @param {number} mask_threshold The mask threshold.\n   * @param {number} overlap_mask_area_threshold The overlap mask area threshold.\n   * @returns {[boolean, number[]]} Whether the segment is valid or not, and the indices of the valid labels.\n   */\n  check_segment_validity(mask_labels, mask_probs, k, mask_threshold = 0.5, overlap_mask_area_threshold = 0.8) {\n    // mask_k is a 1D array of indices, indicating where the mask is equal to k\n    let mask_k = [];\n    let mask_k_area = 0;\n    let original_area = 0;\n\n    // Compute the area of all the stuff in query k\n    for (let i = 0; i < mask_labels.length; ++i) {\n      if (mask_labels[i] === k) {\n        mask_k.push(i);\n        ++mask_k_area;\n      }\n      if (mask_probs[k].data[i] >= mask_threshold) {\n        ++original_area;\n      }\n    }\n    let mask_exists = mask_k_area > 0 && original_area > 0;\n\n    // Eliminate disconnected tiny segments\n    if (mask_exists) {\n      // Perform additional check\n      let area_ratio = mask_k_area / original_area;\n      mask_exists = area_ratio > overlap_mask_area_threshold;\n    }\n    return [mask_exists, mask_k];\n  }\n\n  /**\n   * Computes the segments.\n   * @param {Tensor[]} mask_probs The mask probabilities.\n   * @param {number[]} pred_scores The predicted scores.\n   * @param {number[]} pred_labels The predicted labels.\n   * @param {number} mask_threshold The mask threshold.\n   * @param {number} overlap_mask_area_threshold The overlap mask area threshold.\n   * @param {Set<number>} label_ids_to_fuse The label ids to fuse.\n   * @param {number[]} target_size The target size of the image.\n   * @returns {[Tensor, Array<{id: number, label_id: number, score: number}>]} The computed segments.\n   */\n  compute_segments(mask_probs, pred_scores, pred_labels, mask_threshold, overlap_mask_area_threshold, label_ids_to_fuse = null, target_size = null) {\n    let [height, width] = target_size ?? mask_probs[0].dims;\n    let segmentation = new Tensor('int32', new Int32Array(height * width), [height, width]);\n    let segments = [];\n\n    // 1. If target_size is not null, we need to resize the masks to the target size\n    if (target_size !== null) {\n      // resize the masks to the target size\n      for (let i = 0; i < mask_probs.length; ++i) {\n        mask_probs[i] = interpolate(mask_probs[i], target_size, 'bilinear', false);\n      }\n    }\n\n    // 2. Weigh each mask by its prediction score\n    // NOTE: `mask_probs` is updated in-place\n    // \n    // Temporary storage for the best label/scores for each pixel ([height, width]):\n    let mask_labels = new Int32Array(mask_probs[0].data.length);\n    let bestScores = new Float32Array(mask_probs[0].data.length);\n    for (let i = 0; i < mask_probs.length; ++i) {\n      let score = pred_scores[i];\n      for (let j = 0; j < mask_probs[i].data.length; ++j) {\n        mask_probs[i].data[j] *= score;\n        if (mask_probs[i].data[j] > bestScores[j]) {\n          mask_labels[j] = i;\n          bestScores[j] = mask_probs[i].data[j];\n        }\n      }\n    }\n    let current_segment_id = 0;\n\n    // let stuff_memory_list = {}\n    for (let k = 0; k < pred_labels.length; ++k) {\n      let pred_class = pred_labels[k];\n\n      // TODO add `should_fuse`\n      // let should_fuse = pred_class in label_ids_to_fuse\n\n      // Check if mask exists and large enough to be a segment\n      let [mask_exists, mask_k] = this.check_segment_validity(mask_labels, mask_probs, k, mask_threshold, overlap_mask_area_threshold);\n      if (!mask_exists) {\n        // Nothing to see here\n        continue;\n      }\n\n      // TODO\n      // if (pred_class in stuff_memory_list) {\n      //     current_segment_id = stuff_memory_list[pred_class]\n      // } else {\n      //     current_segment_id += 1;\n      // }\n      ++current_segment_id;\n\n      // Add current object segment to final segmentation map\n      for (let index of mask_k) {\n        segmentation.data[index] = current_segment_id;\n      }\n      segments.push({\n        id: current_segment_id,\n        label_id: pred_class,\n        // was_fused: should_fuse, TODO\n        score: pred_scores[k]\n      });\n\n      // TODO\n      // if(should_fuse){\n      //     stuff_memory_list[pred_class] = current_segment_id\n      // }\n    }\n    return [segmentation, segments];\n  }\n\n  /**\n   * Post-process the model output to generate the final panoptic segmentation.\n   * @param {*} outputs The model output to post process\n   * @param {number} [threshold=0.5] The probability score threshold to keep predicted instance masks.\n   * @param {number} [mask_threshold=0.5] Threshold to use when turning the predicted masks into binary values.\n   * @param {number} [overlap_mask_area_threshold=0.8] The overlap mask area threshold to merge or discard small disconnected parts within each binary instance mask.\n   * @param {Set<number>} [label_ids_to_fuse=null] The labels in this state will have all their instances be fused together.\n   * @param {number[][]} [target_sizes=null] The target sizes to resize the masks to.\n   * @returns {Array<{ segmentation: Tensor, segments_info: Array<{id: number, label_id: number, score: number}>}>}\n   */\n  post_process_panoptic_segmentation(outputs, threshold = 0.5, mask_threshold = 0.5, overlap_mask_area_threshold = 0.8, label_ids_to_fuse = null, target_sizes = null) {\n    if (label_ids_to_fuse === null) {\n      console.warn(\"`label_ids_to_fuse` unset. No instance will be fused.\");\n      label_ids_to_fuse = new Set();\n    }\n    const class_queries_logits = outputs.logits; // [batch_size, num_queries, num_classes+1]\n    const masks_queries_logits = outputs.pred_masks; // [batch_size, num_queries, height, width]\n\n    const mask_probs = masks_queries_logits.sigmoid(); // [batch_size, num_queries, height, width]\n\n    let [batch_size, num_queries, num_labels] = class_queries_logits.dims;\n    num_labels -= 1; // Remove last class (background)\n\n    if (target_sizes !== null && target_sizes.length !== batch_size) {\n      throw Error(\"Make sure that you pass in as many target sizes as the batch dimension of the logits\");\n    }\n    let toReturn = [];\n    for (let i = 0; i < batch_size; ++i) {\n      let target_size = target_sizes !== null ? target_sizes[i] : null;\n      let class_logits = class_queries_logits[i];\n      let mask_logits = mask_probs[i];\n      let [mask_probs_item, pred_scores_item, pred_labels_item] = this.remove_low_and_no_objects(class_logits, mask_logits, threshold, num_labels);\n      if (pred_labels_item.length === 0) {\n        // No mask found\n        let [height, width] = target_size ?? mask_logits.dims.slice(-2);\n        let segmentation = new Tensor('int32', new Int32Array(height * width).fill(-1), [height, width]);\n        toReturn.push({\n          segmentation: segmentation,\n          segments_info: []\n        });\n        continue;\n      }\n\n      // Get segmentation map and segment information of batch item\n      let [segmentation, segments] = this.compute_segments(mask_probs_item, pred_scores_item, pred_labels_item, mask_threshold, overlap_mask_area_threshold, label_ids_to_fuse, target_size);\n      toReturn.push({\n        segmentation: segmentation,\n        segments_info: segments\n      });\n    }\n    return toReturn;\n  }\n  post_process_instance_segmentation() {\n    // TODO\n    throw Error(\"Not implemented yet\");\n  }\n}\nexport class YolosFeatureExtractor extends ImageFeatureExtractor {\n  /** @type {post_process_object_detection} */\n  post_process_object_detection(...args) {\n    return post_process_object_detection(...args);\n  }\n}\n\n/**\n * @typedef {object} SamImageProcessorResult\n * @property {Tensor} pixel_values\n * @property {HeightWidth[]} original_sizes\n * @property {HeightWidth[]} reshaped_input_sizes\n * @property {Tensor} [input_points]\n * @property {Tensor} [input_labels]\n */\n\nexport class SamImageProcessor extends ImageFeatureExtractor {\n  /**\n   * \n   * @param {any} input_points \n   * @param {HeightWidth[]} original_sizes \n   * @param {HeightWidth[]} reshaped_input_sizes \n   * @returns {Tensor}\n   */\n  reshape_input_points(input_points, original_sizes, reshaped_input_sizes) {\n    // Make deep copy to avoid altering user's input\n    input_points = structuredClone(input_points);\n    let shape = calculateDimensions(input_points);\n\n    // TODO: add support for 2D input_points\n    if (shape.length === 3) {\n      // Correct user's input\n      shape = [1, ...shape];\n      input_points = [input_points];\n    } else if (shape.length !== 4) {\n      throw Error(\"The input_points must be a 4D tensor of shape `batch_size`, `point_batch_size`, `nb_points_per_image`, `2`.\");\n    }\n\n    // Reshape input points\n    for (let i = 0; i < input_points.length; ++i) {\n      // batch_size\n      let originalImageSize = original_sizes[i];\n      let reshapedImageSize = reshaped_input_sizes[i];\n      let resizeFactors = [reshapedImageSize[0] / originalImageSize[0], reshapedImageSize[1] / originalImageSize[1]];\n      for (let j = 0; j < input_points[i].length; ++j) {\n        // point_batch_size\n        for (let k = 0; k < input_points[i][j].length; ++k) {\n          // nb_points_per_image\n          for (let w = 0; w < input_points[i][j][k].length; ++w) {\n            // 2\n            input_points[i][j][k][w] *= resizeFactors[w];\n          }\n        }\n      }\n    }\n    return new Tensor('float32', Float32Array.from(input_points.flat(Infinity)), shape);\n  }\n\n  /**\n   * \n   * @param {any} input_labels \n   * @param {Tensor} input_points \n   * @returns {Tensor}\n   */\n  add_input_labels(input_labels, input_points) {\n    let shape = calculateDimensions(input_labels);\n    if (shape.length === 2) {\n      // Correct user's input\n      shape = [1, ...shape];\n      input_labels = [input_labels];\n    } else if (shape.length !== 3) {\n      throw Error(\"The input_points must be a 4D tensor of shape `batch_size`, `point_batch_size`, `nb_points_per_image`, `2`.\");\n    }\n    if (shape.some((x, i) => x !== input_points.dims[i])) {\n      throw Error(`The first ${shape.length} dimensions of 'input_points' and 'input_labels' must be the same.`);\n    }\n    return new Tensor('int64', input_labels.flat(Infinity).map(BigInt), shape);\n  }\n  /**\n   * @param {any[]} images The URL(s) of the image(s) to extract features from.\n   * @param {any} [input_points] A 3D or 4D array, representing the input points provided by the user.\n   * - 3D: `[point_batch_size, nb_points_per_image, 2]`. In this case, `batch_size` is assumed to be 1.\n   * - 4D: `[batch_size, point_batch_size, nb_points_per_image, 2]`.\n   * @param {any} [input_labels] A 2D or 3D array, representing the input labels for the points, used by the prompt encoder to encode the prompt.\n   * - 2D: `[point_batch_size, nb_points_per_image]`. In this case, `batch_size` is assumed to be 1.\n   * - 3D: `[batch_size, point_batch_size, nb_points_per_image]`.\n   * @returns {Promise<SamImageProcessorResult>}\n   */\n  async _call(images, input_points = null, input_labels = null) {\n    // TODO allow user to use preprocessed images\n    /** @type {SamImageProcessorResult} */\n    const processed = await super._call(images);\n    if (input_points) {\n      processed.input_points = this.reshape_input_points(input_points, processed.original_sizes, processed.reshaped_input_sizes);\n    }\n    if (input_labels) {\n      if (!processed.input_points) {\n        throw Error(\"`input_points` must be provided if `input_labels` are provided.\");\n      }\n      processed.input_labels = this.add_input_labels(input_labels, processed.input_points);\n    }\n    return processed;\n  }\n\n  /**\n   * Remove padding and upscale masks to the original image size.\n   * @param {Tensor} masks Batched masks from the mask_decoder in (batch_size, num_channels, height, width) format.\n   * @param {number[][]} original_sizes The original sizes of each image before it was resized to the model's expected input shape, in (height, width) format.\n   * @param {number[][]} reshaped_input_sizes The size of each image as it is fed to the model, in (height, width) format. Used to remove padding.\n   * @param {Object} options Optional parameters for post-processing.\n   * @param {number} [options.mask_threshold] The threshold to use for binarizing the masks.\n   * @param {boolean} [options.binarize] Whether to binarize the masks.\n   * @param {Object} [options.pad_size] The target size the images were padded to before being passed to the model. If `null`, the target size is assumed to be the processor's `pad_size`.\n   * @param {number} [options.pad_size.height] The height the images were padded to.\n   * @param {number} [options.pad_size.width] The width the images were padded to.\n   * @returns {Tensor[]} Batched masks in batch_size, num_channels, height, width) format, where (height, width) is given by original_size.\n   */\n  post_process_masks(masks, original_sizes, reshaped_input_sizes, {\n    mask_threshold = 0.0,\n    binarize = true,\n    pad_size = null\n  } = {}) {\n    // masks: [1, 1, 3, 256, 256]\n\n    const output_masks = [];\n    pad_size = pad_size ?? this.pad_size;\n    const target_image_size = [pad_size.height, pad_size.width];\n    for (let i = 0; i < original_sizes.length; ++i) {\n      const original_size = original_sizes[i];\n      const reshaped_input_size = reshaped_input_sizes[i];\n      const mask = masks[i]; // [b, c, h, w]\n\n      // TODO: improve\n      const interpolated_masks = [];\n      for (let j = 0; j < mask.dims[0]; ++j) {\n        const m = mask[j]; // 3d tensor\n\n        // Upscale mask to padded size\n        let interpolated_mask = interpolate(m, target_image_size, 'bilinear', false);\n\n        // Crop mask\n        interpolated_mask = interpolated_mask.slice(null, [0, reshaped_input_size[0]], [0, reshaped_input_size[1]]);\n\n        // Downscale mask\n        interpolated_mask = interpolate(interpolated_mask, original_size, 'bilinear', false);\n        if (binarize) {\n          const binarizedMaskData = new Uint8Array(interpolated_mask.data.length);\n          for (let i = 0; i < interpolated_mask.data.length; ++i) {\n            if (interpolated_mask.data[i] > mask_threshold) {\n              binarizedMaskData[i] = 1;\n            }\n          }\n          interpolated_mask = new Tensor('bool', binarizedMaskData, interpolated_mask.dims);\n        }\n        interpolated_masks.push(interpolated_mask);\n      }\n      output_masks.push(stack(interpolated_masks));\n    }\n    return output_masks;\n  }\n}\nexport class Swin2SRImageProcessor extends ImageFeatureExtractor {\n  pad_image(pixelData, imgDims, padSize, options = {}) {\n    // NOTE: In this case, `padSize` represents the size of the sliding window for the local attention.\n    // In other words, the image is padded so that its width and height are multiples of `padSize`.\n    const [imageHeight, imageWidth, imageChannels] = imgDims;\n    return super.pad_image(pixelData, imgDims, {\n      // NOTE: For Swin2SR models, the original python implementation adds padding even when the image's width/height is already\n      // a multiple of `pad_size`. However, this is most likely a bug (PR: https://github.com/mv-lab/swin2sr/pull/19).\n      // For this reason, we only add padding when the image's width/height is not a multiple of `pad_size`.\n      width: imageWidth + (padSize - imageWidth % padSize) % padSize,\n      height: imageHeight + (padSize - imageHeight % padSize) % padSize\n    }, {\n      mode: 'symmetric',\n      center: false,\n      constant_values: -1,\n      ...options\n    });\n  }\n}\nexport class VitMatteImageProcessor extends ImageFeatureExtractor {\n  /**\n   * Calls the feature extraction process on an array of images, preprocesses\n   * each image, and concatenates the resulting features into a single Tensor.\n   * @param {RawImage[]} images The image(s) to extract features from.\n   * @param {RawImage[]} trimaps The trimaps(s) to extract features from.\n   * @returns {Promise<ImageFeatureExtractorResult>} An object containing the concatenated pixel values of the preprocessed images.\n   */\n  async _call(images, trimaps) {\n    if (!Array.isArray(images)) {\n      images = [images];\n    }\n    if (!Array.isArray(trimaps)) {\n      trimaps = [trimaps];\n    }\n    const imageData = await Promise.all(images.map(x => this.preprocess(x)));\n    const trimapData = await Promise.all(trimaps.map(x => this.preprocess(x, {\n      do_normalize: false,\n      do_convert_rgb: false,\n      do_convert_grayscale: true\n    })));\n\n    // Stack pixel values\n    const pixel_values = stack(imageData.map(\n    // Concatenate images and trimaps\n    (x, i) => cat([x.pixel_values, trimapData[i].pixel_values], 0)), 0);\n    return {\n      pixel_values: pixel_values,\n      // Original sizes of images\n      original_sizes: imageData.map(x => x.original_size),\n      // Reshaped sizes of images, before padding or cropping\n      reshaped_input_sizes: imageData.map(x => x.reshaped_input_size)\n    };\n  }\n}\nexport class WhisperFeatureExtractor extends FeatureExtractor {\n  constructor(config) {\n    super(config);\n\n    // Prefer given `mel_filters` from preprocessor_config.json, or calculate them if they don't exist.\n    this.config.mel_filters ??= mel_filter_bank(Math.floor(1 + this.config.n_fft / 2),\n    // num_frequency_bins\n    this.config.feature_size,\n    // num_mel_filters\n    0.0,\n    // min_frequency\n    8000.0,\n    // max_frequency\n    this.config.sampling_rate,\n    // sampling_rate\n    \"slaney\",\n    // norm\n    \"slaney\" // mel_scale\n    );\n    this.window = window_function(this.config.n_fft, 'hann');\n  }\n\n  /**\n   * Computes the log-Mel spectrogram of the provided audio waveform.\n   * @param {Float32Array|Float64Array} waveform The audio waveform to process.\n   * @returns {{data: Float32Array, dims: number[]}} An object containing the log-Mel spectrogram data as a Float32Array and its dimensions as an array of numbers.\n   */\n  _extract_fbank_features(waveform) {\n    const {\n      data,\n      dims\n    } = spectrogram(waveform, this.window,\n    // window\n    this.config.n_fft,\n    // frame_length\n    this.config.hop_length,\n    // hop_length\n    {\n      power: 2.0,\n      mel_filters: this.config.mel_filters,\n      log_mel: 'log10',\n      // Custom\n      max_num_frames: this.config.nb_max_frames // 3000\n    });\n    const maxValue = max(data)[0];\n    for (let i = 0; i < data.length; ++i) {\n      data[i] = (Math.max(data[i], maxValue - 8.0) + 4.0) / 4.0;\n    }\n    return {\n      data,\n      dims\n    };\n  }\n\n  /**\n   * Asynchronously extracts features from a given audio using the provided configuration.\n   * @param {Float32Array|Float64Array} audio The audio data as a Float32Array/Float64Array.\n   * @returns {Promise<{ input_features: Tensor }>} A Promise resolving to an object containing the extracted input features as a Tensor.\n   */\n  async _call(audio) {\n    validate_audio_inputs(audio, 'WhisperFeatureExtractor');\n    let waveform;\n    if (audio.length > this.config.n_samples) {\n      console.warn(\"Attempting to extract features for audio longer than 30 seconds. \" + \"If using a pipeline to extract transcript from a long audio clip, \" + \"remember to specify `chunk_length_s` and/or `stride_length_s`.\");\n      waveform = audio.slice(0, this.config.n_samples);\n    } else {\n      // pad with zeros\n      waveform = new Float32Array(this.config.n_samples);\n      waveform.set(audio);\n    }\n    const {\n      data,\n      dims\n    } = this._extract_fbank_features(waveform);\n    return {\n      input_features: new Tensor('float32', data, [1, ...dims])\n    };\n  }\n}\nexport class Wav2Vec2FeatureExtractor extends FeatureExtractor {\n  /**\n   * @param {Float32Array} input_values \n   * @returns {Float32Array} \n   */\n  _zero_mean_unit_var_norm(input_values) {\n    // TODO support batch?\n    const sum = input_values.reduce((a, b) => a + b, 0);\n    const mean = sum / input_values.length;\n    const variance = input_values.reduce((a, b) => a + (b - mean) ** 2, 0) / input_values.length;\n    return input_values.map(x => (x - mean) / Math.sqrt(variance + 1e-7));\n  }\n\n  /**\n   * Asynchronously extracts features from a given audio using the provided configuration.\n   * @param {Float32Array|Float64Array} audio The audio data as a Float32Array/Float64Array.\n   * @returns {Promise<{ input_values: Tensor; attention_mask: Tensor }>} A Promise resolving to an object containing the extracted input features and attention mask as Tensors.\n   */\n  async _call(audio) {\n    validate_audio_inputs(audio, 'Wav2Vec2FeatureExtractor');\n    if (audio instanceof Float64Array) {\n      audio = new Float32Array(audio);\n    }\n    let input_values = audio;\n\n    // zero-mean and unit-variance normalization\n    if (this.config.do_normalize) {\n      input_values = this._zero_mean_unit_var_norm(input_values);\n    }\n\n    // TODO: allow user to pass in attention mask\n    const shape = [1, input_values.length];\n    return {\n      input_values: new Tensor('float32', input_values, shape),\n      attention_mask: new Tensor('int64', new BigInt64Array(input_values.length).fill(1n), shape)\n    };\n  }\n}\nexport class SeamlessM4TFeatureExtractor extends FeatureExtractor {\n  constructor(config) {\n    super(config);\n    const sampling_rate = this.config.sampling_rate;\n    const mel_filters = mel_filter_bank(256,\n    // num_frequency_bins\n    this.config.num_mel_bins,\n    // num_mel_filters\n    20,\n    // min_frequency\n    Math.floor(sampling_rate / 2),\n    // max_frequency\n    sampling_rate,\n    // sampling_rate\n    null,\n    // norm\n    \"kaldi\",\n    // mel_scale\n    true // triangularize_in_mel_space\n    );\n\n    // Do padding:\n    for (let i = 0; i < mel_filters.length; ++i) {\n      mel_filters[i].push(0);\n    }\n    this.mel_filters = mel_filters;\n    this.window = window_function(400, 'povey', {\n      periodic: false\n    });\n  }\n\n  /**\n   * Computes the log-Mel spectrogram of the provided audio waveform.\n   * @param {Float32Array|Float64Array} waveform The audio waveform to process.\n   * @param {number} max_length The maximum number of frames to return.\n   * @returns {{data: Float32Array, dims: number[]}} An object containing the log-Mel spectrogram data as a Float32Array and its dimensions as an array of numbers.\n   */\n  _extract_fbank_features(waveform, max_length) {\n    // NOTE: We don't pad/truncate since that is passed in as `max_num_frames`\n\n    // Kaldi compliance: 16-bit signed integers\n    // 32768 == 2 ** 15\n    waveform = waveform.map((/** @type {number} */x) => x * 32768);\n    return spectrogram(waveform, this.window,\n    // window\n    400,\n    // frame_length\n    160,\n    // hop_length\n    {\n      fft_length: 512,\n      power: 2.0,\n      center: false,\n      preemphasis: 0.97,\n      mel_filters: this.mel_filters,\n      log_mel: 'log',\n      mel_floor: 1.192092955078125e-07,\n      remove_dc_offset: true,\n      // Custom\n      max_num_frames: max_length,\n      transpose: true\n    });\n  }\n\n  /**\n   * Asynchronously extracts features from a given audio using the provided configuration.\n   * @param {Float32Array|Float64Array} audio The audio data as a Float32Array/Float64Array.\n   * @param {Object} options Optional parameters for feature extraction.\n   * @param {boolean} [options.padding=true] Whether to pad the sequence to a multiple of `pad_to_multiple_of`.\n   * @param {number} [options.pad_to_multiple_of=2] The number to pad the sequence to a multiple of.\n   * @param {boolean} [options.do_normalize_per_mel_bins=true] Whether or not to zero-mean unit-variance normalize the input per mel-channel.\n   * @param {boolean} [options.return_attention_mask=true] Whether to return the attention mask.\n   * @returns {Promise<{ input_features: Tensor, attention_mask?: Tensor }>} A Promise resolving to an object containing the extracted input features and attention masks as Tensors.\n   */\n  async _call(audio, {\n    padding = true,\n    pad_to_multiple_of = 2,\n    do_normalize_per_mel_bins = true,\n    return_attention_mask = true\n  } = {}) {\n    validate_audio_inputs(audio, 'SeamlessM4TFeatureExtractor');\n    let features = this._extract_fbank_features(audio, this.config.max_length);\n    if (do_normalize_per_mel_bins) {\n      const [num_features, feature_size] = features.dims;\n      for (let i = 0; i < feature_size; ++i) {\n        let sum = 0;\n        for (let j = 0; j < num_features; ++j) {\n          sum += features.data[j * feature_size + i];\n        }\n        const mean = sum / num_features;\n        let variance = 0;\n        for (let j = 0; j < num_features; ++j) {\n          variance += (features.data[j * feature_size + i] - mean) ** 2;\n        }\n        variance /= num_features - 1; // NOTE: We use ddof=1\n\n        const std = Math.sqrt(variance + 1e-7);\n        for (let j = 0; j < num_features; ++j) {\n          const index = j * feature_size + i;\n          features.data[index] = (features.data[index] - mean) / std;\n        }\n      }\n    }\n    let padded_attention_mask;\n    if (padding) {\n      const [num_frames, num_channels] = features.dims;\n      const pad_size = num_frames % pad_to_multiple_of;\n      if (pad_size > 0) {\n        const padded_data = new Float32Array(num_channels * (num_frames + pad_size));\n        padded_data.set(features.data);\n        padded_data.fill(this.config.padding_value, features.data.length);\n        const numPaddedFrames = num_frames + pad_size;\n        features = {\n          data: padded_data,\n          dims: [numPaddedFrames, num_channels]\n        };\n        if (return_attention_mask) {\n          padded_attention_mask = new Tensor('int64', new BigInt64Array(numPaddedFrames), [1, numPaddedFrames]);\n          padded_attention_mask.data.fill(1n, 0, num_frames);\n        }\n      }\n    }\n    const [num_frames, num_channels] = features.dims;\n    const stride = this.config.stride;\n    const remainder = num_frames % stride;\n    if (remainder !== 0) {\n      throw new Error(`The number of frames (${num_frames}) must be a multiple of the stride (${stride}).`);\n    }\n    const input_features = new Tensor('float32', features.data, features.dims).view(1, Math.floor(num_frames / stride), num_channels * stride);\n    const result = {\n      input_features\n    };\n    if (return_attention_mask) {\n      const reshapedNumFrames = input_features.dims[1];\n      const attention_mask = new Tensor('int64', new BigInt64Array(reshapedNumFrames), [1, reshapedNumFrames]);\n      if (padded_attention_mask) {\n        for (let i = 1, j = 0; i < num_frames; i += stride, ++j) {\n          attention_mask.data[j] = padded_attention_mask.data[i];\n        }\n      } else {\n        attention_mask.data.fill(1n);\n      }\n      result.attention_mask = attention_mask;\n    }\n    return result;\n  }\n}\nexport class ASTFeatureExtractor extends FeatureExtractor {\n  constructor(config) {\n    super(config);\n    const sampling_rate = this.config.sampling_rate;\n    const mel_filters = mel_filter_bank(256,\n    // num_frequency_bins\n    this.config.num_mel_bins,\n    // num_mel_filters\n    20,\n    // min_frequency\n    Math.floor(sampling_rate / 2),\n    // max_frequency\n    sampling_rate,\n    // sampling_rate\n    null,\n    // norm\n    \"kaldi\",\n    // mel_scale\n    true // triangularize_in_mel_space\n    );\n\n    // Do padding:\n    for (let i = 0; i < mel_filters.length; ++i) {\n      mel_filters[i].push(0);\n    }\n    this.mel_filters = mel_filters;\n    this.window = window_function(400, 'hann', {\n      periodic: false\n    });\n    this.mean = this.config.mean;\n    this.std = this.config.std;\n  }\n\n  /**\n   * Computes the log-Mel spectrogram of the provided audio waveform.\n   * @param {Float32Array|Float64Array} waveform The audio waveform to process.\n   * @param {number} max_length The maximum number of frames to return.\n   * @returns {{data: Float32Array, dims: number[]}} An object containing the log-Mel spectrogram data as a Float32Array and its dimensions as an array of numbers.\n   */\n  _extract_fbank_features(waveform, max_length) {\n    // NOTE: We don't pad/truncate since that is passed in as `max_num_frames`\n    return spectrogram(waveform, this.window,\n    // window\n    400,\n    // frame_length\n    160,\n    // hop_length\n    {\n      fft_length: 512,\n      power: 2.0,\n      center: false,\n      preemphasis: 0.97,\n      mel_filters: this.mel_filters,\n      log_mel: 'log',\n      mel_floor: 1.192092955078125e-07,\n      remove_dc_offset: true,\n      // Custom\n      max_num_frames: max_length,\n      transpose: true\n    });\n  }\n\n  /**\n   * Asynchronously extracts features from a given audio using the provided configuration.\n   * @param {Float32Array|Float64Array} audio The audio data as a Float32Array/Float64Array.\n   * @returns {Promise<{ input_values: Tensor }>} A Promise resolving to an object containing the extracted input features as a Tensor.\n   */\n  async _call(audio) {\n    validate_audio_inputs(audio, 'ASTFeatureExtractor');\n    const features = this._extract_fbank_features(audio, this.config.max_length);\n    if (this.config.do_normalize) {\n      // Normalize the input audio spectrogram to have mean=0, std=0.5\n      const denom = this.std * 2;\n      for (let i = 0; i < features.data.length; ++i) {\n        features.data[i] = (features.data[i] - this.mean) / denom;\n      }\n    }\n    return {\n      input_values: new Tensor('float32', features.data, [1, ...features.dims])\n    };\n  }\n}\nexport class ClapFeatureExtractor extends FeatureExtractor {\n  constructor(config) {\n    super(config);\n    this.mel_filters = mel_filter_bank(this.config.nb_frequency_bins,\n    // num_frequency_bins\n    this.config.feature_size,\n    // num_mel_filters\n    this.config.frequency_min,\n    // min_frequency\n    this.config.frequency_max,\n    // max_frequency\n    this.config.sampling_rate,\n    // sampling_rate\n    null,\n    // norm\n    \"htk\" // mel_scale\n    );\n    this.mel_filters_slaney = mel_filter_bank(this.config.nb_frequency_bins,\n    // num_frequency_bins\n    this.config.feature_size,\n    // num_mel_filters\n    this.config.frequency_min,\n    // min_frequency\n    this.config.frequency_max,\n    // max_frequency\n    this.config.sampling_rate,\n    // sampling_rate\n    \"slaney\",\n    // norm\n    \"slaney\" // mel_scale\n    );\n    this.window = window_function(this.config.fft_window_size, 'hann');\n  }\n\n  /**\n   * Extracts the mel spectrogram and prepares it for the mode based on the `truncation` and `padding` arguments.\n   * \n   * Four different path are possible:\n   *   - `truncation=\"fusion\"` and the length of the waveform is greater than the max length: the mel spectrogram\n   *     will be computed on the entire audio. 3 random crops and a dowsampled version of the full mel spectrogram\n   *     are then stacked together. They will later be used for `feature_fusion`.\n   *   - `truncation=\"rand_trunc\"` and the length of the waveform is smaller than the max length: the audio is\n   *     padded based on `padding`.\n   *   - `truncation=\"fusion\"` and the length of the waveform is smaller than the max length: the audio is padded\n   *     based on `padding`, and is repeated `4` times.\n   *   - `truncation=\"rand_trunc\"` and the length of the waveform is greater than the max length: the mel\n   *     spectrogram will be computed on a random crop of the waveform.\n   * \n   * @param {Float32Array|Float64Array} waveform The input waveform.\n   * @param {number} max_length The maximum length of the waveform.\n   * @param {string} truncation The truncation strategy to use.\n   * @param {string} padding The padding strategy to use.\n   * @returns {{ data: Float32Array; dims: number[]; longer: boolean; }} An object containing the mel spectrogram data as a Float32Array, its dimensions as an array of numbers, and a boolean indicating whether the waveform was longer than the max length.\n   */\n  _get_input_mel(waveform, max_length, truncation, padding) {\n    /** @type {{ data: Float32Array; dims: number[]}} */\n    let input_mel;\n    let longer = false;\n    const diff = waveform.length - max_length;\n    if (diff > 0) {\n      if (truncation === 'rand_trunc') {\n        longer = true;\n        const idx = Math.floor(Math.random() * (diff + 1));\n        waveform = waveform.subarray(idx, idx + max_length);\n        input_mel = this._extract_fbank_features(waveform, this.mel_filters_slaney, this.config.nb_max_samples);\n        input_mel.dims = [1, ...input_mel.dims]; // \"unsqueeze\"\n      } else {\n        // TODO implement fusion strategy\n        throw new Error(`Truncation strategy \"${truncation}\" not implemented`);\n      }\n    } else {\n      if (diff < 0) {\n        let padded = new Float64Array(max_length); // already padded with zeros\n        padded.set(waveform);\n        if (padding === 'repeat') {\n          for (let i = waveform.length; i < max_length; i += waveform.length) {\n            padded.set(waveform.subarray(0, Math.min(waveform.length, max_length - i)), i);\n          }\n        } else if (padding === 'repeatpad') {\n          for (let i = waveform.length; i < -diff; i += waveform.length) {\n            padded.set(waveform, i);\n          }\n        }\n        waveform = padded;\n      }\n      if (truncation === 'fusion') {\n        throw new Error(`Truncation strategy \"${truncation}\" not implemented`);\n      }\n      input_mel = this._extract_fbank_features(waveform, this.mel_filters_slaney, this.config.nb_max_samples);\n      input_mel.dims = [1, ...input_mel.dims]; // \"unsqueeze\"\n    }\n    return {\n      ...input_mel,\n      longer\n    };\n  }\n\n  /**\n   * Compute the log-mel spectrogram of the provided `waveform` using the Hann window.\n   * In CLAP, two different filter banks are used depending on the truncation pattern:\n   *  - `self.mel_filters`: they correspond to the default parameters of `torchaudio` which can be obtained from\n   *    calling `torchaudio.transforms.MelSpectrogram().mel_scale.fb`. These filters are used when `truncation`\n   *    is set to `\"fusion\"`.\n   *  - `self.mel_filteres_slaney` : they correspond to the default parameters of `librosa` which used\n   *    `librosa.filters.mel` when computing the mel spectrogram. These filters were only used in the original\n   *    implementation when the truncation mode is not `\"fusion\"`.\n   * \n   * @param {Float32Array|Float64Array} waveform The audio waveform to process.\n   * @param {number[][]} mel_filters The mel filters to use.\n   * @param {number} [max_length=null] The maximum number of frames to return.\n   * @returns {{data: Float32Array, dims: number[]}} An object containing the log-Mel spectrogram data as a Float32Array and its dimensions as an array of numbers.\n   */\n  _extract_fbank_features(waveform, mel_filters, max_length = null) {\n    // NOTE: We don't pad/truncate since that is passed in as `max_num_frames`\n    return spectrogram(waveform, this.window,\n    // window\n    this.config.fft_window_size,\n    // frame_length\n    this.config.hop_length,\n    // hop_length\n    {\n      power: 2.0,\n      mel_filters,\n      log_mel: 'dB',\n      // Custom\n      max_num_frames: max_length,\n      do_pad: false,\n      transpose: true\n    });\n  }\n\n  /**\n   * Asynchronously extracts features from a given audio using the provided configuration.\n   * @param {Float32Array|Float64Array} audio The audio data as a Float32Array/Float64Array.\n   * @returns {Promise<{ input_features: Tensor }>} A Promise resolving to an object containing the extracted input features as a Tensor.\n   */\n  async _call(audio, {\n    max_length = null\n  } = {}) {\n    validate_audio_inputs(audio, 'ClapFeatureExtractor');\n\n    // convert to mel spectrogram, truncate and pad if needed.\n    const padded_inputs = this._get_input_mel(audio, max_length ?? this.config.nb_max_samples, this.config.truncation, this.config.padding);\n    return {\n      input_features: new Tensor('float32', padded_inputs.data, [1, ...padded_inputs.dims])\n    };\n  }\n}\nexport class SpeechT5FeatureExtractor extends FeatureExtractor {}\n\n/**\n * Represents a Processor that extracts features from an input.\n * @extends Callable\n */\nexport class Processor extends Callable {\n  /**\n   * Creates a new Processor with the given feature extractor.\n   * @param {FeatureExtractor} feature_extractor The function used to extract features from the input.\n   */\n  constructor(feature_extractor) {\n    super();\n    this.feature_extractor = feature_extractor;\n    // TODO use tokenizer here?\n  }\n\n  /**\n   * Calls the feature_extractor function with the given input.\n   * @param {any} input The input to extract features from.\n   * @param {...any} args Additional arguments.\n   * @returns {Promise<any>} A Promise that resolves with the extracted features.\n   */\n  async _call(input, ...args) {\n    return await this.feature_extractor(input, ...args);\n  }\n}\nexport class SamProcessor extends Processor {\n  /**\n   * @borrows SamImageProcessor#_call as _call\n   */\n  async _call(...args) {\n    return await this.feature_extractor(...args);\n  }\n\n  /**\n   * @borrows SamImageProcessor#post_process_masks as post_process_masks\n   */\n  post_process_masks(...args) {\n    // @ts-ignore\n    return this.feature_extractor.post_process_masks(...args);\n  }\n  /**\n   * @borrows SamImageProcessor#reshape_input_points as reshape_input_points\n   */\n  reshape_input_points(...args) {\n    // @ts-ignore\n    return this.feature_extractor.reshape_input_points(...args);\n  }\n}\n\n/**\n * Represents a WhisperProcessor that extracts features from an audio input.\n * @extends Processor\n */\nexport class WhisperProcessor extends Processor {\n  /**\n   * Calls the feature_extractor function with the given audio input.\n   * @param {any} audio The audio input to extract features from.\n   * @returns {Promise<any>} A Promise that resolves with the extracted features.\n   */\n  async _call(audio) {\n    return await this.feature_extractor(audio);\n  }\n}\nexport class Wav2Vec2ProcessorWithLM extends Processor {\n  /**\n   * Calls the feature_extractor function with the given audio input.\n   * @param {any} audio The audio input to extract features from.\n   * @returns {Promise<any>} A Promise that resolves with the extracted features.\n   */\n  async _call(audio) {\n    return await this.feature_extractor(audio);\n  }\n}\nexport class SpeechT5Processor extends Processor {\n  /**\n   * Calls the feature_extractor function with the given input.\n   * @param {any} input The input to extract features from.\n   * @returns {Promise<any>} A Promise that resolves with the extracted features.\n   */\n  async _call(input) {\n    return await this.feature_extractor(input);\n  }\n}\nexport class OwlViTProcessor extends Processor {}\n\n//////////////////////////////////////////////////\n/**\n * Helper class which is used to instantiate pretrained processors with the `from_pretrained` function.\n * The chosen processor class is determined by the type specified in the processor config.\n * \n * **Example:** Load a processor using `from_pretrained`.\n * ```javascript\n * let processor = await AutoProcessor.from_pretrained('openai/whisper-tiny.en');\n * ```\n * \n * **Example:** Run an image through a processor.\n * ```javascript\n * let processor = await AutoProcessor.from_pretrained('Xenova/clip-vit-base-patch16');\n * let image = await RawImage.read('https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/football-match.jpg');\n * let image_inputs = await processor(image);\n * // {\n * //   \"pixel_values\": {\n * //     \"dims\": [ 1, 3, 224, 224 ],\n * //     \"type\": \"float32\",\n * //     \"data\": Float32Array [ -1.558687686920166, -1.558687686920166, -1.5440893173217773, ... ],\n * //     \"size\": 150528\n * //   },\n * //   \"original_sizes\": [\n * //     [ 533, 800 ]\n * //   ],\n * //   \"reshaped_input_sizes\": [\n * //     [ 224, 224 ]\n * //   ]\n * // }\n * ```\n */\nexport class AutoProcessor {\n  static FEATURE_EXTRACTOR_CLASS_MAPPING = {\n    ImageFeatureExtractor,\n    WhisperFeatureExtractor,\n    ViTFeatureExtractor,\n    MobileViTFeatureExtractor,\n    MobileViTImageProcessor,\n    OwlViTFeatureExtractor,\n    Owlv2ImageProcessor,\n    CLIPFeatureExtractor,\n    ChineseCLIPFeatureExtractor,\n    SiglipImageProcessor,\n    ConvNextFeatureExtractor,\n    ConvNextImageProcessor,\n    SegformerFeatureExtractor,\n    BitImageProcessor,\n    DPTImageProcessor,\n    DPTFeatureExtractor,\n    GLPNFeatureExtractor,\n    BeitFeatureExtractor,\n    DeiTFeatureExtractor,\n    DetrFeatureExtractor,\n    YolosFeatureExtractor,\n    DonutFeatureExtractor,\n    NougatImageProcessor,\n    EfficientNetImageProcessor,\n    ViTImageProcessor,\n    VitMatteImageProcessor,\n    SamImageProcessor,\n    Swin2SRImageProcessor,\n    Wav2Vec2FeatureExtractor,\n    SeamlessM4TFeatureExtractor,\n    SpeechT5FeatureExtractor,\n    ASTFeatureExtractor,\n    ClapFeatureExtractor\n  };\n  static PROCESSOR_CLASS_MAPPING = {\n    WhisperProcessor,\n    Wav2Vec2ProcessorWithLM,\n    SamProcessor,\n    SpeechT5Processor,\n    OwlViTProcessor\n  };\n\n  /**\n   * Instantiate one of the processor classes of the library from a pretrained model.\n   * \n   * The processor class to instantiate is selected based on the `feature_extractor_type` property of the config object\n   * (either passed as an argument or loaded from `pretrained_model_name_or_path` if possible)\n   * \n   * @param {string} pretrained_model_name_or_path The name or path of the pretrained model. Can be either:\n   * - A string, the *model id* of a pretrained processor hosted inside a model repo on huggingface.co.\n   *   Valid model ids can be located at the root-level, like `bert-base-uncased`, or namespaced under a\n   *   user or organization name, like `dbmdz/bert-base-german-cased`.\n   * - A path to a *directory* containing processor files, e.g., `./my_model_directory/`.\n   * @param {import('./utils/hub.js').PretrainedOptions} options Additional options for loading the processor.\n   * \n   * @returns {Promise<Processor>} A new instance of the Processor class.\n   */\n  static async from_pretrained(pretrained_model_name_or_path, {\n    progress_callback = null,\n    config = null,\n    cache_dir = null,\n    local_files_only = false,\n    revision = 'main'\n  } = {}) {\n    let preprocessorConfig = config ?? (await getModelJSON(pretrained_model_name_or_path, 'preprocessor_config.json', true, {\n      progress_callback,\n      config,\n      cache_dir,\n      local_files_only,\n      revision\n    }));\n\n    // Determine feature extractor class\n    // TODO: Ensure backwards compatibility with old configs\n    let key = preprocessorConfig.feature_extractor_type ?? preprocessorConfig.image_processor_type;\n    let feature_extractor_class = this.FEATURE_EXTRACTOR_CLASS_MAPPING[key];\n    if (!feature_extractor_class) {\n      if (preprocessorConfig.size !== undefined) {\n        // Assume ImageFeatureExtractor\n        console.warn(`Feature extractor type \"${key}\" not found, assuming ImageFeatureExtractor due to size parameter in config.`);\n        feature_extractor_class = ImageFeatureExtractor;\n      } else {\n        throw new Error(`Unknown Feature Extractor type: ${key}`);\n      }\n    }\n\n    // If no associated processor class, use default\n    let processor_class = this.PROCESSOR_CLASS_MAPPING[preprocessorConfig.processor_class] ?? Processor;\n\n    // Instantiate processor and feature extractor\n    let feature_extractor = new feature_extractor_class(preprocessorConfig);\n    return new processor_class(feature_extractor);\n  }\n}\n//////////////////////////////////////////////////","map":{"version":3,"names":["Callable","calculateDimensions","calculateReflectOffset","getModelJSON","min","max","softmax","bankers_round","Tensor","permute","cat","interpolate","stack","RawImage","window_function","spectrogram","mel_filter_bank","center_to_corners_format","centerX","centerY","width","height","post_process_object_detection","outputs","threshold","target_sizes","is_zero_shot","out_logits","logits","out_bbox","pred_boxes","batch_size","num_boxes","num_classes","dims","length","Error","toReturn","i","target_size","info","boxes","classes","scores","bbox","j","logit","indices","probs","sigmoid","data","k","push","maxIndex","index","box","map","x","validate_audio_inputs","audio","feature_extractor","Float32Array","Float64Array","constructor","name","constraint_to_multiple_of","val","multiple","minVal","maxVal","a","Math","floor","ceil","enforce_size_divisibility","divisor","FeatureExtractor","config","ImageFeatureExtractor","image_mean","mean","image_std","std","resample","do_rescale","rescale_factor","do_normalize","do_resize","do_thumbnail","size","size_divisibility","size_divisor","do_center_crop","crop_size","do_convert_rgb","do_crop_margin","pad_size","do_pad","undefined","do_flip_channel_order","thumbnail","image","input_height","input_width","output_height","output_width","resize","crop_margin","gray_threshold","gray_image","clone","grayscale","minValue","maxValue","diff","x_min","y_min","x_max","y_max","row","crop","pad_image","pixelData","imgDims","padSize","mode","center","constant_values","imageHeight","imageWidth","imageChannels","paddedImageWidth","paddedImageHeight","paddedPixelData","Array","isArray","fill","left","top","b","c","d","h1","w1","rescale","get_resize_output_image_size","srcWidth","srcHeight","shortest_edge","longest_edge","Number","isInteger","max_size","shortResizeFactor","newWidth","newHeight","longResizeFactor","finalWidth","toFixed","finalHeight","keep_aspect_ratio","ensure_multiple_of","scale_height","scale_width","abs","JSON","stringify","preprocess","do_convert_grayscale","rgb","crop_width","crop_height","center_crop","reshaped_input_size","from","channels","padded","paddedWidth","paddedHeight","temp","pixel_values","original_size","_call","images","args","imageData","Promise","all","original_sizes","reshaped_input_sizes","SegformerFeatureExtractor","post_process_semantic_segmentation","slice","segmentation","Int32Array","buffer","hasLabel","out","labels","filter","DPTFeatureExtractor","DPTImageProcessor","BitImageProcessor","GLPNFeatureExtractor","CLIPFeatureExtractor","ChineseCLIPFeatureExtractor","SiglipImageProcessor","ConvNextFeatureExtractor","crop_pct","resize_shortest_edge","ConvNextImageProcessor","ViTFeatureExtractor","ViTImageProcessor","EfficientNetImageProcessor","include_top","MobileViTFeatureExtractor","MobileViTImageProcessor","OwlViTFeatureExtractor","Owlv2ImageProcessor","DeiTFeatureExtractor","BeitFeatureExtractor","DonutFeatureExtractor","options","NougatImageProcessor","DetrFeatureExtractor","result","maskSize","pixel_mask","BigInt64Array","reduce","remove_low_and_no_objects","class_logits","mask_logits","object_mask_threshold","num_labels","mask_probs_item","pred_scores_item","pred_labels_item","cls","mask","pred_label","pred_score","check_segment_validity","mask_labels","mask_probs","mask_threshold","overlap_mask_area_threshold","mask_k","mask_k_area","original_area","mask_exists","area_ratio","compute_segments","pred_scores","pred_labels","label_ids_to_fuse","segments","bestScores","score","current_segment_id","pred_class","id","label_id","post_process_panoptic_segmentation","console","warn","Set","class_queries_logits","masks_queries_logits","pred_masks","num_queries","segments_info","post_process_instance_segmentation","YolosFeatureExtractor","SamImageProcessor","reshape_input_points","input_points","structuredClone","shape","originalImageSize","reshapedImageSize","resizeFactors","w","flat","Infinity","add_input_labels","input_labels","some","BigInt","processed","post_process_masks","masks","binarize","output_masks","target_image_size","interpolated_masks","m","interpolated_mask","binarizedMaskData","Uint8Array","Swin2SRImageProcessor","VitMatteImageProcessor","trimaps","trimapData","WhisperFeatureExtractor","mel_filters","n_fft","feature_size","sampling_rate","window","_extract_fbank_features","waveform","hop_length","power","log_mel","max_num_frames","nb_max_frames","n_samples","set","input_features","Wav2Vec2FeatureExtractor","_zero_mean_unit_var_norm","input_values","sum","variance","sqrt","attention_mask","SeamlessM4TFeatureExtractor","num_mel_bins","periodic","max_length","fft_length","preemphasis","mel_floor","remove_dc_offset","transpose","padding","pad_to_multiple_of","do_normalize_per_mel_bins","return_attention_mask","features","num_features","padded_attention_mask","num_frames","num_channels","padded_data","padding_value","numPaddedFrames","stride","remainder","view","reshapedNumFrames","ASTFeatureExtractor","denom","ClapFeatureExtractor","nb_frequency_bins","frequency_min","frequency_max","mel_filters_slaney","fft_window_size","_get_input_mel","truncation","input_mel","longer","idx","random","subarray","nb_max_samples","padded_inputs","SpeechT5FeatureExtractor","Processor","input","SamProcessor","WhisperProcessor","Wav2Vec2ProcessorWithLM","SpeechT5Processor","OwlViTProcessor","AutoProcessor","FEATURE_EXTRACTOR_CLASS_MAPPING","PROCESSOR_CLASS_MAPPING","from_pretrained","pretrained_model_name_or_path","progress_callback","cache_dir","local_files_only","revision","preprocessorConfig","key","feature_extractor_type","image_processor_type","feature_extractor_class","processor_class"],"sources":["/Users/lorryrio/Project/calico/node_modules/@xenova/transformers/src/processors.js"],"sourcesContent":["\n/**\n * @file Processors are used to prepare non-textual inputs (e.g., image or audio) for a model.\n * \n * **Example:** Using a `WhisperProcessor` to prepare an audio input for a model.\n * ```javascript\n * import { AutoProcessor, read_audio } from '@xenova/transformers';\n *\n * let processor = await AutoProcessor.from_pretrained('openai/whisper-tiny.en');\n * let audio = await read_audio('https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac', 16000);\n * let { input_features } = await processor(audio);\n * // Tensor {\n * //   data: Float32Array(240000) [0.4752984642982483, 0.5597258806228638, 0.56434166431427, ...],\n * //   dims: [1, 80, 3000],\n * //   type: 'float32',\n * //   size: 240000,\n * // }\n * ```\n * \n * @module processors\n */\nimport {\n    Callable,\n    calculateDimensions,\n    calculateReflectOffset,\n} from './utils/core.js';\n\nimport {\n    getModelJSON,\n} from './utils/hub.js';\n\nimport {\n    min,\n    max,\n    softmax,\n    bankers_round,\n} from './utils/maths.js';\n\n\nimport { Tensor, permute, cat, interpolate, stack } from './utils/tensor.js';\n\nimport { RawImage } from './utils/image.js';\nimport {\n    window_function,\n    spectrogram,\n    mel_filter_bank,\n} from './utils/audio.js';\n\n\n// Helper functions\n\n/**\n * Converts bounding boxes from center format to corners format.\n * \n * @param {number[]} arr The coordinate for the center of the box and its width, height dimensions (center_x, center_y, width, height)\n * @returns {number[]} The coodinates for the top-left and bottom-right corners of the box (top_left_x, top_left_y, bottom_right_x, bottom_right_y)\n */\nfunction center_to_corners_format([centerX, centerY, width, height]) {\n    return [\n        centerX - width / 2,\n        centerY - height / 2,\n        centerX + width / 2,\n        centerY + height / 2\n    ];\n}\n\n/**\n * Post-processes the outputs of the model (for object detection).\n * @param {Object} outputs The outputs of the model that must be post-processed\n * @param {Tensor} outputs.logits The logits\n * @param {Tensor} outputs.pred_boxes The predicted boxes.\n * @param {number} [threshold=0.5] The threshold to use for the scores.\n * @param {number[][]} [target_sizes=null] The sizes of the original images.\n * @param {boolean} [is_zero_shot=false] Whether zero-shot object detection was performed.\n * @return {Object[]} An array of objects containing the post-processed outputs.\n * @private\n */\nfunction post_process_object_detection(outputs, threshold = 0.5, target_sizes = null, is_zero_shot = false) {\n    const out_logits = outputs.logits;\n    const out_bbox = outputs.pred_boxes;\n    const [batch_size, num_boxes, num_classes] = out_logits.dims;\n\n    if (target_sizes !== null && target_sizes.length !== batch_size) {\n        throw Error(\"Make sure that you pass in as many target sizes as the batch dimension of the logits\")\n    }\n    let toReturn = [];\n    for (let i = 0; i < batch_size; ++i) {\n        let target_size = target_sizes !== null ? target_sizes[i] : null;\n        let info = {\n            boxes: [],\n            classes: [],\n            scores: []\n        }\n        let logits = out_logits[i];\n        let bbox = out_bbox[i];\n\n        for (let j = 0; j < num_boxes; ++j) {\n            let logit = logits[j];\n\n            let indices = [];\n            let probs;\n            if (is_zero_shot) {\n                // Get indices of classes with high enough probability\n                probs = logit.sigmoid().data;\n                for (let k = 0; k < probs.length; ++k) {\n                    if (probs[k] > threshold) {\n                        indices.push(k);\n                    }\n                }\n\n            } else {\n                // Get most probable class\n                let maxIndex = max(logit.data)[1];\n\n                if (maxIndex === num_classes - 1) {\n                    // This is the background class, skip it\n                    continue;\n                }\n                indices.push(maxIndex);\n\n                // Compute softmax over classes\n                probs = softmax(logit.data);\n            }\n\n            for (const index of indices) {\n\n                // Some class has a high enough probability\n                /** @type {number[]} */\n                let box = bbox[j].data;\n\n                // convert to [x0, y0, x1, y1] format\n                box = center_to_corners_format(box)\n                if (target_size !== null) {\n                    box = box.map((x, i) => x * target_size[(i + 1) % 2])\n                }\n\n                info.boxes.push(box);\n                info.classes.push(index);\n                info.scores.push(probs[index]);\n            }\n        }\n        toReturn.push(info);\n    }\n    return toReturn;\n}\n\n/**\n * Named tuple to indicate the order we are using is (height x width), even though\n * the Graphicsâ€™ industry standard is (width x height).\n * @typedef {[height: number, width: number]} HeightWidth\n */\n\n/**\n * Helper function to validate audio inputs.\n * @param {any} audio The audio data.\n * @param {string} feature_extractor The name of the feature extractor.\n * @private\n */\nfunction validate_audio_inputs(audio, feature_extractor) {\n    if (!(audio instanceof Float32Array || audio instanceof Float64Array)) {\n        throw new Error(\n            `${feature_extractor} expects input to be a Float32Array or a Float64Array, but got ${audio?.constructor?.name ?? typeof audio} instead. ` +\n            `If using the feature extractor directly, remember to use \\`read_audio(url, sampling_rate)\\` to obtain the raw audio data of the file/url.`\n        )\n    }\n}\n\n/**\n * Helper function to constrain a value to be a multiple of a number.\n * @param {number} val The value to constrain.\n * @param {number} multiple The number to constrain to.\n * @param {number} [minVal=0] The minimum value to constrain to.\n * @param {number} [maxVal=null] The maximum value to constrain to.\n * @returns {number} The constrained value.\n * @private\n */\nfunction constraint_to_multiple_of(val, multiple, minVal = 0, maxVal = null) {\n    const a = val / multiple;\n    let x = bankers_round(a) * multiple;\n\n    if (maxVal !== null && x > maxVal) {\n        x = Math.floor(a) * multiple;\n    }\n\n    if (x < minVal) {\n        x = Math.ceil(a) * multiple;\n    }\n\n    return x;\n}\n\n/**\n * Rounds the height and width down to the closest multiple of size_divisibility\n * @param {[number, number]} size The size of the image\n * @param {number} divisor The divisor to use.\n * @returns {[number, number]} The rounded size.\n */\nfunction enforce_size_divisibility([width, height], divisor) {\n    return [\n        Math.max(Math.floor(width / divisor), 1) * divisor,\n        Math.max(Math.floor(height / divisor), 1) * divisor\n    ];\n}\n\n\n/**\n * Base class for feature extractors.\n *\n * @extends Callable\n */\nexport class FeatureExtractor extends Callable {\n    /**\n     * Constructs a new FeatureExtractor instance.\n     *\n     * @param {Object} config The configuration for the feature extractor.\n     */\n    constructor(config) {\n        super();\n        this.config = config\n    }\n}\n\n/**\n * @typedef {object} ImageFeatureExtractorResult\n * @property {Tensor} pixel_values The pixel values of the batched preprocessed images.\n * @property {HeightWidth[]} original_sizes Array of two-dimensional tuples like [[480, 640]].\n * @property {HeightWidth[]} reshaped_input_sizes Array of two-dimensional tuples like [[1000, 1330]].\n */\n\n/**\n * Feature extractor for image models.\n *\n * @extends FeatureExtractor\n */\nexport class ImageFeatureExtractor extends FeatureExtractor {\n\n    /**\n     * Constructs a new ImageFeatureExtractor instance.\n     *\n     * @param {Object} config The configuration for the feature extractor.\n     * @param {number[]} config.image_mean The mean values for image normalization.\n     * @param {number[]} config.image_std The standard deviation values for image normalization.\n     * @param {boolean} config.do_rescale Whether to rescale the image pixel values to the [0,1] range.\n     * @param {number} config.rescale_factor The factor to use for rescaling the image pixel values.\n     * @param {boolean} config.do_normalize Whether to normalize the image pixel values.\n     * @param {boolean} config.do_resize Whether to resize the image.\n     * @param {number} config.resample What method to use for resampling.\n     * @param {number|Object} config.size The size to resize the image to.\n     * @param {boolean} [config.do_flip_channel_order=false] Whether to flip the color channels from RGB to BGR.\n     * Can be overridden by the `do_flip_channel_order` parameter in the `preprocess` method.\n     */\n    constructor(config) {\n        super(config);\n\n        this.image_mean = this.config.image_mean ?? this.config.mean;\n        this.image_std = this.config.image_std ?? this.config.std;\n\n        this.resample = this.config.resample ?? 2; // 2 => bilinear\n        this.do_rescale = this.config.do_rescale ?? true;\n        this.rescale_factor = this.config.rescale_factor ?? (1 / 255);\n        this.do_normalize = this.config.do_normalize;\n\n        this.do_resize = this.config.do_resize;\n        this.do_thumbnail = this.config.do_thumbnail;\n        this.size = this.config.size;\n        this.size_divisibility = this.config.size_divisibility ?? this.config.size_divisor;\n\n        this.do_center_crop = this.config.do_center_crop;\n        this.crop_size = this.config.crop_size;\n        this.do_convert_rgb = this.config.do_convert_rgb ?? true;\n        this.do_crop_margin = this.config.do_crop_margin;\n\n        this.pad_size = this.config.pad_size;\n        this.do_pad = this.config.do_pad;\n\n        if (this.do_pad && !this.pad_size && this.size && this.size.width !== undefined && this.size.height !== undefined) {\n            // Should pad, but no pad size specified\n            // We infer the pad size from the resize size\n            this.pad_size = this.size\n        }\n\n        this.do_flip_channel_order = this.config.do_flip_channel_order ?? false;\n    }\n\n    /**\n     * Resize the image to make a thumbnail. The image is resized so that no dimension is larger than any\n     * corresponding dimension of the specified size.\n     * @param {RawImage} image The image to be resized.\n     * @param {{height:number, width:number}} size The size `{\"height\": h, \"width\": w}` to resize the image to.\n     * @param {string | 0 | 1 | 2 | 3 | 4 | 5} [resample=2] The resampling filter to use.\n     * @returns {Promise<RawImage>} The resized image.\n     */\n    async thumbnail(image, size, resample = 2) {\n        const input_height = image.height;\n        const input_width = image.width;\n\n        const output_height = size.height;\n        const output_width = size.width;\n\n        // We always resize to the smallest of either the input or output size.\n        let height = Math.min(input_height, output_height)\n        let width = Math.min(input_width, output_width)\n\n        if (height === input_height && width === input_width) {\n            return image;\n        }\n        if (input_height > input_width) {\n            width = Math.floor(input_width * height / input_height);\n        } else if (input_width > input_height) {\n            height = Math.floor(input_height * width / input_width);\n        }\n        return await image.resize(width, height, { resample });\n    }\n\n\n    /**\n     * Crops the margin of the image. Gray pixels are considered margin (i.e., pixels with a value below the threshold).\n     * @param {RawImage} image The image to be cropped.\n     * @param {number} gray_threshold Value below which pixels are considered to be gray.\n     * @returns {Promise<RawImage>} The cropped image.\n     */\n    async crop_margin(image, gray_threshold = 200) {\n\n        const gray_image = image.clone().grayscale();\n\n        const minValue = min(gray_image.data)[0];\n        const maxValue = max(gray_image.data)[0];\n        const diff = maxValue - minValue;\n\n        if (diff === 0) {\n            return image;\n        }\n\n        const threshold = gray_threshold / 255;\n\n        let x_min = gray_image.width, y_min = gray_image.height, x_max = 0, y_max = 0;\n        for (let j = 0; j < gray_image.height; ++j) {\n            const row = j * gray_image.width;\n            for (let i = 0; i < gray_image.width; ++i) {\n                if ((gray_image.data[row + i] - minValue) / diff < threshold) {\n                    // We have a non-zero pixel, so we update the min/max values accordingly\n                    x_min = Math.min(x_min, i);\n                    y_min = Math.min(y_min, j);\n                    x_max = Math.max(x_max, i);\n                    y_max = Math.max(y_max, j);\n                }\n            }\n        }\n\n        image = await image.crop([x_min, y_min, x_max, y_max]);\n        return image;\n    }\n\n    /**\n     * Pad the image by a certain amount.\n     * @param {Float32Array} pixelData The pixel data to pad.\n     * @param {number[]} imgDims The dimensions of the image (height, width, channels).\n     * @param {{width:number; height:number}|number} padSize The dimensions of the padded image.\n     * @param {Object} options The options for padding.\n     * @param {'constant'|'symmetric'} [options.mode='constant'] The type of padding to add.\n     * @param {boolean} [options.center=false] Whether to center the image.\n     * @param {number} [options.constant_values=0] The constant value to use for padding.\n     * @returns {[Float32Array, number[]]} The padded pixel data and image dimensions.\n     */\n    pad_image(pixelData, imgDims, padSize, {\n        mode = 'constant',\n        center = false,\n        constant_values = 0,\n    } = {}) {\n        const [imageHeight, imageWidth, imageChannels] = imgDims;\n\n        let paddedImageWidth, paddedImageHeight;\n        if (typeof padSize === 'number') {\n            paddedImageWidth = padSize;\n            paddedImageHeight = padSize;\n        } else {\n            paddedImageWidth = padSize.width;\n            paddedImageHeight = padSize.height;\n        }\n\n        // Only add padding if there is a difference in size\n        if (paddedImageWidth !== imageWidth || paddedImageHeight !== imageHeight) {\n            const paddedPixelData = new Float32Array(paddedImageWidth * paddedImageHeight * imageChannels);\n            if (Array.isArray(constant_values)) {\n                // Fill with constant values, cycling through the array\n                for (let i = 0; i < paddedPixelData.length; ++i) {\n                    paddedPixelData[i] = constant_values[i % imageChannels];\n                }\n            } else if (constant_values !== 0) {\n                paddedPixelData.fill(constant_values);\n            }\n\n            const [left, top] = center\n                ? [Math.floor((paddedImageWidth - imageWidth) / 2), Math.floor((paddedImageHeight - imageHeight) / 2)]\n                : [0, 0];\n\n            // Copy the original image into the padded image\n            for (let i = 0; i < imageHeight; ++i) {\n                const a = (i + top) * paddedImageWidth;\n                const b = i * imageWidth;\n                for (let j = 0; j < imageWidth; ++j) {\n                    const c = (a + j + left) * imageChannels;\n                    const d = (b + j) * imageChannels;\n                    for (let k = 0; k < imageChannels; ++k) {\n                        paddedPixelData[c + k] = pixelData[d + k];\n                    }\n                }\n            }\n\n            if (mode === 'symmetric') {\n                if (center) {\n                    throw new Error('`center` padding is not supported when `mode` is set to `symmetric`.');\n                    // TODO: Implement this\n                }\n                const h1 = imageHeight - 1;\n                const w1 = imageWidth - 1;\n                for (let i = 0; i < paddedImageHeight; ++i) {\n                    const a = i * paddedImageWidth;\n                    const b = calculateReflectOffset(i, h1) * imageWidth;\n\n                    for (let j = 0; j < paddedImageWidth; ++j) {\n                        if (i < imageHeight && j < imageWidth) continue; // Do not overwrite original image\n                        const c = (a + j) * imageChannels;\n                        const d = (b + calculateReflectOffset(j, w1)) * imageChannels;\n\n                        // Copy channel-wise\n                        for (let k = 0; k < imageChannels; ++k) {\n                            paddedPixelData[c + k] = pixelData[d + k];\n                        }\n                    }\n                }\n            }\n\n\n            // Update pixel data and image dimensions\n            pixelData = paddedPixelData;\n            imgDims = [paddedImageHeight, paddedImageWidth, imageChannels]\n        }\n        return [pixelData, imgDims];\n    }\n\n    /**\n     * Rescale the image' pixel values by `this.rescale_factor`.\n     * @param {Float32Array} pixelData The pixel data to rescale.\n     * @returns {void}\n     */\n    rescale(pixelData) {\n        for (let i = 0; i < pixelData.length; ++i) {\n            pixelData[i] = this.rescale_factor * pixelData[i];\n        }\n    }\n\n    /**\n     * Find the target (width, height) dimension of the output image after\n     * resizing given the input image and the desired size.\n     * @param {RawImage} image The image to resize.\n     * @param {any} size The size to use for resizing the image. \n     * @returns {[number, number]} The target (width, height) dimension of the output image after resizing.\n     */\n    get_resize_output_image_size(image, size) {\n        // `size` comes in many forms, so we need to handle them all here:\n        // 1. `size` is an integer, in which case we resize the image to be a square \n\n        const [srcWidth, srcHeight] = image.size;\n\n        let shortest_edge;\n        let longest_edge;\n\n        if (this.do_thumbnail) {\n            // NOTE: custom logic for `Donut` models\n            const { height, width } = size;\n            shortest_edge = Math.min(height, width)\n        }\n        // Support both formats for backwards compatibility\n        else if (Number.isInteger(size)) {\n            shortest_edge = size;\n            longest_edge = this.config.max_size ?? shortest_edge;\n\n        } else if (size !== undefined) {\n            // Extract known properties from `size`\n            shortest_edge = size.shortest_edge;\n            longest_edge = size.longest_edge;\n        }\n\n        // If `longest_edge` and `shortest_edge` are set, maintain aspect ratio and resize to `shortest_edge`\n        // while keeping the largest dimension <= `longest_edge`\n        if (shortest_edge !== undefined || longest_edge !== undefined) {\n            // http://opensourcehacker.com/2011/12/01/calculate-aspect-ratio-conserving-resize-for-images-in-javascript/\n            // Try resize so that shortest edge is `shortest_edge` (target)\n            const shortResizeFactor = shortest_edge === undefined\n                ? 1 // If `shortest_edge` is not set, don't upscale\n                : Math.max(shortest_edge / srcWidth, shortest_edge / srcHeight);\n\n            const newWidth = srcWidth * shortResizeFactor;\n            const newHeight = srcHeight * shortResizeFactor;\n\n            // The new width and height might be greater than `longest_edge`, so\n            // we downscale again to ensure the largest dimension is `longest_edge` \n            const longResizeFactor = longest_edge === undefined\n                ? 1 // If `longest_edge` is not set, don't downscale\n                : Math.min(longest_edge / newWidth, longest_edge / newHeight);\n\n            // To avoid certain floating point precision issues, we round to 2 decimal places\n            let finalWidth = Math.floor(Number((newWidth * longResizeFactor).toFixed(2)));\n            let finalHeight = Math.floor(Number((newHeight * longResizeFactor).toFixed(2)));\n\n            if (this.size_divisibility !== undefined) {\n                [finalWidth, finalHeight] = enforce_size_divisibility([finalWidth, finalHeight], this.size_divisibility)\n            }\n            return [finalWidth, finalHeight];\n\n        } else if (size !== undefined && size.width !== undefined && size.height !== undefined) {\n            // If `width` and `height` are set, resize to those dimensions\n\n            let newWidth = size.width;\n            let newHeight = size.height;\n\n            // Custom for DPT models\n            if (this.config.keep_aspect_ratio && this.config.ensure_multiple_of) {\n\n                // determine new height and width\n                let scale_height = newHeight / srcHeight;\n                let scale_width = newWidth / srcWidth;\n\n                // scale as little as possible\n                if (Math.abs(1 - scale_width) < Math.abs(1 - scale_height)) {\n                    // fit width\n                    scale_height = scale_width;\n                } else {\n                    // fit height\n                    scale_width = scale_height;\n                }\n\n                newHeight = constraint_to_multiple_of(scale_height * srcHeight, this.config.ensure_multiple_of);\n                newWidth = constraint_to_multiple_of(scale_width * srcWidth, this.config.ensure_multiple_of);\n            }\n\n            return [newWidth, newHeight];\n\n        } else if (this.size_divisibility !== undefined) {\n            return enforce_size_divisibility([srcWidth, srcHeight], this.size_divisibility);\n        } else {\n            throw new Error(`Could not resize image due to unsupported \\`this.size\\` option in config: ${JSON.stringify(size)}`);\n        }\n    }\n\n    /**\n     * Resizes the image.\n     * @param {RawImage} image The image to resize.\n     * @returns {Promise<RawImage>} The resized image.\n     */\n    async resize(image) {\n        const [newWidth, newHeight] = this.get_resize_output_image_size(image, this.size);\n        return await image.resize(newWidth, newHeight, {\n            resample: this.resample,\n        });\n    }\n\n    /**\n     * @typedef {object} PreprocessedImage\n     * @property {HeightWidth} original_size The original size of the image.\n     * @property {HeightWidth} reshaped_input_size The reshaped input size of the image.\n     * @property {Tensor} pixel_values The pixel values of the preprocessed image.\n     */\n\n    /**\n     * Preprocesses the given image.\n     *\n     * @param {RawImage} image The image to preprocess.\n     * @param {Object} overrides The overrides for the preprocessing options.\n     * @returns {Promise<PreprocessedImage>} The preprocessed image.\n     */\n    async preprocess(image, {\n        do_normalize = null,\n        do_pad = null,\n        do_convert_rgb = null,\n        do_convert_grayscale = null,\n        do_flip_channel_order = null,\n    } = {}) {\n        if (this.do_crop_margin) {\n            // NOTE: Specific to nougat processors. This is done before resizing,\n            // and can be interpreted as a pre-preprocessing step.\n            image = await this.crop_margin(image);\n        }\n\n        const [srcWidth, srcHeight] = image.size; // original image size\n\n        // Convert image to RGB if specified in config.\n        if (do_convert_rgb ?? this.do_convert_rgb) {\n            image = image.rgb();\n        } else if (do_convert_grayscale) {\n            image = image.grayscale();\n        }\n\n        // TODO:\n        // For efficiency reasons, it might be best to merge the resize and center crop operations into one.\n\n        // Resize all images\n        if (this.do_resize) {\n            image = await this.resize(image);\n        }\n\n        // Resize the image using thumbnail method.\n        if (this.do_thumbnail) {\n            image = await this.thumbnail(image, this.size, this.resample);\n        }\n\n        if (this.do_center_crop) {\n\n            let crop_width;\n            let crop_height;\n            if (Number.isInteger(this.crop_size)) {\n                crop_width = this.crop_size;\n                crop_height = this.crop_size;\n            } else {\n                crop_width = this.crop_size.width;\n                crop_height = this.crop_size.height;\n            }\n\n            image = await image.center_crop(crop_width, crop_height);\n        }\n\n        /** @type {HeightWidth} */\n        const reshaped_input_size = [image.height, image.width];\n\n        // NOTE: All pixel-level manipulation (i.e., modifying `pixelData`)\n        // occurs with data in the hwc format (height, width, channels), \n        // to emulate the behavior of the original Python code (w/ numpy).\n        let pixelData = Float32Array.from(image.data);\n        let imgDims = [image.height, image.width, image.channels];\n\n        if (this.do_rescale) {\n            this.rescale(pixelData);\n        }\n\n        if (do_normalize ?? this.do_normalize) {\n            let image_mean = this.image_mean;\n            if (!Array.isArray(this.image_mean)) {\n                image_mean = new Array(image.channels).fill(image_mean);\n            }\n\n            let image_std = this.image_std;\n            if (!Array.isArray(this.image_std)) {\n                image_std = new Array(image.channels).fill(image_mean);\n            }\n\n            if (image_mean.length !== image.channels || image_std.length !== image.channels) {\n                throw new Error(`When set to arrays, the length of \\`image_mean\\` (${image_mean.length}) and \\`image_std\\` (${image_std.length}) must match the number of channels in the image (${image.channels}).`);\n            }\n\n            for (let i = 0; i < pixelData.length; i += image.channels) {\n                for (let j = 0; j < image.channels; ++j) {\n                    pixelData[i + j] = (pixelData[i + j] - image_mean[j]) / image_std[j];\n                }\n            }\n        }\n\n        // do padding after rescaling/normalizing\n        if (do_pad ?? this.do_pad) {\n            if (this.pad_size) {\n                const padded = this.pad_image(pixelData, [image.height, image.width, image.channels], this.pad_size);\n                [pixelData, imgDims] = padded; // Update pixel data and image dimensions\n            } else if (this.size_divisibility) {\n                const [paddedWidth, paddedHeight] = enforce_size_divisibility([imgDims[1], imgDims[0]], this.size_divisibility);\n                [pixelData, imgDims] = this.pad_image(pixelData, imgDims, { width: paddedWidth, height: paddedHeight });\n            }\n        }\n\n        if (do_flip_channel_order ?? this.do_flip_channel_order) {\n            if (imgDims[2] !== 3) {\n                throw new Error('Flipping channel order is only supported for RGB images.');\n            }\n            // Convert RGB to BGR\n            for (let i = 0; i < pixelData.length; i += 3) {\n                const temp = pixelData[i];\n                pixelData[i] = pixelData[i + 2];\n                pixelData[i + 2] = temp;\n            }\n        }\n\n        const pixel_values = new Tensor('float32', pixelData, imgDims)\n            .permute(2, 0, 1); // convert to channel dimension format (hwc -> chw)\n\n        return {\n            original_size: [srcHeight, srcWidth],\n            reshaped_input_size: reshaped_input_size,\n            pixel_values: pixel_values,\n        }\n    }\n\n    /**\n     * Calls the feature extraction process on an array of images,\n     * preprocesses each image, and concatenates the resulting\n     * features into a single Tensor.\n     * @param {RawImage[]} images The image(s) to extract features from.\n     * @param {...any} args Additional arguments.\n     * @returns {Promise<ImageFeatureExtractorResult>} An object containing the concatenated pixel values (and other metadata) of the preprocessed images.\n     */\n    async _call(images, ...args) {\n        if (!Array.isArray(images)) {\n            images = [images];\n        }\n        /** @type {PreprocessedImage[]} */\n        const imageData = await Promise.all(images.map(x => this.preprocess(x)));\n\n        // Stack pixel values\n        const pixel_values = stack(imageData.map(x => x.pixel_values), 0);\n\n        return {\n            pixel_values: pixel_values,\n\n            // Original sizes of images\n            original_sizes: imageData.map(x => x.original_size),\n\n            // Reshaped sizes of images, before padding or cropping\n            reshaped_input_sizes: imageData.map(x => x.reshaped_input_size),\n        }\n    }\n\n}\n\nexport class SegformerFeatureExtractor extends ImageFeatureExtractor {\n\n    /**\n     * Converts the output of `SegformerForSemanticSegmentation` into semantic segmentation maps.\n     * @param {*} outputs Raw outputs of the model.\n     * @param {number[][]} [target_sizes=null] List of tuples corresponding to the requested final size\n     * (height, width) of each prediction. If unset, predictions will not be resized.\n     * @returns {{segmentation: Tensor; labels: number[]}[]} The semantic segmentation maps.\n     */\n    post_process_semantic_segmentation(outputs, target_sizes = null) {\n\n        const logits = outputs.logits;\n        const batch_size = logits.dims[0];\n\n        if (target_sizes !== null && target_sizes.length !== batch_size) {\n            throw Error(\"Make sure that you pass in as many target sizes as the batch dimension of the logits\")\n        }\n\n        const toReturn = [];\n        for (let i = 0; i < batch_size; ++i) {\n            const target_size = target_sizes !== null ? target_sizes[i] : null;\n\n            let data = logits[i];\n\n            // 1. If target_size is not null, we need to resize the masks to the target size\n            if (target_size !== null) {\n                // resize the masks to the target size\n                data = interpolate(data, target_size, 'bilinear', false);\n            }\n            const [height, width] = target_size ?? data.dims.slice(-2);\n\n            const segmentation = new Tensor(\n                'int32',\n                new Int32Array(height * width),\n                [height, width]\n            );\n\n            // Buffer to store current largest value\n            const buffer = data[0].data;\n            for (let j = 1; j < data.dims[0]; ++j) {\n                const row = data[j].data;\n                for (let k = 0; k < row.length; ++k) {\n                    if (row[k] > buffer[k]) {\n                        buffer[k] = row[k];\n                        segmentation.data[k] = j;\n                    }\n                }\n            }\n\n            // Store which objects have labels\n            // This is much more efficient that creating a set of the final values\n            const hasLabel = new Array(data.dims[0]);\n            const out = segmentation.data;\n            for (let j = 0; j < out.length; ++j) {\n                const index = out[j];\n                hasLabel[index] = index;\n            }\n            /** @type {number[]} The unique list of labels that were detected */\n            const labels = hasLabel.filter(x => x !== undefined);\n\n            toReturn.push({ segmentation, labels });\n        }\n        return toReturn;\n    }\n}\nexport class DPTFeatureExtractor extends ImageFeatureExtractor { }\nexport class DPTImageProcessor extends DPTFeatureExtractor { } // NOTE: extends DPTFeatureExtractor\nexport class BitImageProcessor extends ImageFeatureExtractor { }\nexport class GLPNFeatureExtractor extends ImageFeatureExtractor { }\nexport class CLIPFeatureExtractor extends ImageFeatureExtractor { }\nexport class ChineseCLIPFeatureExtractor extends ImageFeatureExtractor { }\nexport class SiglipImageProcessor extends ImageFeatureExtractor { }\nexport class ConvNextFeatureExtractor extends ImageFeatureExtractor {\n    constructor(config) {\n        super(config);\n\n        /**\n         * Percentage of the image to crop. Only has an effect if this.size < 384.\n         */\n        this.crop_pct = this.config.crop_pct ?? (224 / 256);\n    }\n\n    async resize(image) {\n        const shortest_edge = this.size?.shortest_edge;\n        if (shortest_edge === undefined) {\n            throw new Error(`Size dictionary must contain 'shortest_edge' key.`);\n        }\n\n        if (shortest_edge < 384) {\n            // maintain same ratio, resizing shortest edge to shortest_edge/crop_pct\n            const resize_shortest_edge = Math.floor(shortest_edge / this.crop_pct);\n\n            const [newWidth, newHeight] = this.get_resize_output_image_size(image, {\n                shortest_edge: resize_shortest_edge,\n            });\n\n            image = await image.resize(newWidth, newHeight, {\n                resample: this.resample,\n            });\n\n            // then crop to (shortest_edge, shortest_edge)\n            image = await image.center_crop(shortest_edge, shortest_edge);\n        } else {\n            // warping (no cropping) when evaluated at 384 or larger\n            image = await image.resize(shortest_edge, shortest_edge, {\n                resample: this.resample,\n            });\n        }\n\n        return image;\n    }\n}\nexport class ConvNextImageProcessor extends ConvNextFeatureExtractor { }  // NOTE extends ConvNextFeatureExtractor\nexport class ViTFeatureExtractor extends ImageFeatureExtractor { }\nexport class ViTImageProcessor extends ImageFeatureExtractor { }\n\nexport class EfficientNetImageProcessor extends ImageFeatureExtractor {\n    constructor(config) {\n        super(config);\n        this.include_top = this.config.include_top ?? true;\n        if (this.include_top) {\n            this.image_std = this.image_std.map(x => x * x);\n        }\n    }\n}\n\n\nexport class MobileViTFeatureExtractor extends ImageFeatureExtractor { }\nexport class MobileViTImageProcessor extends MobileViTFeatureExtractor { } // NOTE extends MobileViTFeatureExtractor\nexport class OwlViTFeatureExtractor extends ImageFeatureExtractor {\n    /** @type {post_process_object_detection} */\n    post_process_object_detection(...args) {\n        return post_process_object_detection(...args);\n    }\n}\nexport class Owlv2ImageProcessor extends OwlViTFeatureExtractor { } // NOTE extends OwlViTFeatureExtractor\n\nexport class DeiTFeatureExtractor extends ImageFeatureExtractor { }\nexport class BeitFeatureExtractor extends ImageFeatureExtractor { }\nexport class DonutFeatureExtractor extends ImageFeatureExtractor {\n    pad_image(pixelData, imgDims, padSize, options = {}) {\n        const [imageHeight, imageWidth, imageChannels] = imgDims;\n\n        let image_mean = this.image_mean;\n        if (!Array.isArray(this.image_mean)) {\n            image_mean = new Array(imageChannels).fill(image_mean);\n        }\n\n        let image_std = this.image_std;\n        if (!Array.isArray(image_std)) {\n            image_std = new Array(imageChannels).fill(image_mean);\n        }\n\n        const constant_values = image_mean.map((x, i) => - x / image_std[i]);\n\n        return super.pad_image(pixelData, imgDims, padSize, {\n            center: true,\n\n            // Since normalization is done after padding, we need to use certain constant values to ensure the same behaviour is observed.\n            // For more information, see https://github.com/huggingface/transformers/blob/main/src/transformers/models/donut/image_processing_donut.py#L433-L451\n            constant_values: constant_values,\n            ...options,\n        });\n    }\n}\nexport class NougatImageProcessor extends DonutFeatureExtractor { } // NOTE extends DonutFeatureExtractor\n\n/**\n * @typedef {object} DetrFeatureExtractorResultProps\n * @property {Tensor} pixel_mask\n * @typedef {ImageFeatureExtractorResult & DetrFeatureExtractorResultProps} DetrFeatureExtractorResult\n */\n\n/**\n * Detr Feature Extractor.\n *\n * @extends ImageFeatureExtractor\n */\nexport class DetrFeatureExtractor extends ImageFeatureExtractor {\n    /**\n     * Calls the feature extraction process on an array of images, preprocesses\n     * each image, and concatenates the resulting features into a single Tensor.\n     * @param {RawImage[]} images The image(s) to extract features from.\n     * @returns {Promise<DetrFeatureExtractorResult>} An object containing the concatenated pixel values of the preprocessed images.\n     */\n    async _call(images) {\n        const result = await super._call(images);\n\n        // TODO support differently-sized images, for now assume all images are the same size.\n        // TODO support different mask sizes (not just 64x64)\n        // Currently, just fill pixel mask with 1s\n        const maskSize = [result.pixel_values.dims[0], 64, 64];\n        const pixel_mask = new Tensor(\n            'int64',\n            new BigInt64Array(maskSize.reduce((a, b) => a * b)).fill(1n),\n            maskSize\n        );\n\n        return { ...result, pixel_mask };\n    }\n\n    /**\n     * Post-processes the outputs of the model (for object detection).\n     * @param {Object} outputs The outputs of the model that must be post-processed\n     * @param {Tensor} outputs.logits The logits\n     * @param {Tensor} outputs.pred_boxes The predicted boxes.\n     * @return {Object[]} An array of objects containing the post-processed outputs.\n     */\n\n    /** @type {post_process_object_detection} */\n    post_process_object_detection(...args) {\n        return post_process_object_detection(...args);\n    }\n\n    /**\n     * Binarize the given masks using `object_mask_threshold`, it returns the associated values of `masks`, `scores` and `labels`.\n     * @param {Tensor} class_logits The class logits.\n     * @param {Tensor} mask_logits The mask logits.\n     * @param {number} object_mask_threshold A number between 0 and 1 used to binarize the masks.\n     * @param {number} num_labels The number of labels.\n     * @returns {[Tensor[], number[], number[]]} The binarized masks, the scores, and the labels.\n     */\n    remove_low_and_no_objects(class_logits, mask_logits, object_mask_threshold, num_labels) {\n\n        let mask_probs_item = [];\n        let pred_scores_item = [];\n        let pred_labels_item = [];\n\n        for (let j = 0; j < class_logits.dims[0]; ++j) {\n            let cls = class_logits[j];\n            let mask = mask_logits[j];\n\n            let pred_label = max(cls.data)[1];\n            if (pred_label === num_labels) {\n                // Is the background, so we ignore it\n                continue;\n            }\n\n            let scores = softmax(cls.data);\n            let pred_score = scores[pred_label];\n            if (pred_score > object_mask_threshold) {\n                mask_probs_item.push(mask);\n                pred_scores_item.push(pred_score);\n                pred_labels_item.push(pred_label);\n            }\n        }\n\n        return [mask_probs_item, pred_scores_item, pred_labels_item];\n\n    }\n\n    /**\n     * Checks whether the segment is valid or not.\n     * @param {Int32Array} mask_labels Labels for each pixel in the mask.\n     * @param {Tensor[]} mask_probs Probabilities for each pixel in the masks.\n     * @param {number} k The class id of the segment.\n     * @param {number} mask_threshold The mask threshold.\n     * @param {number} overlap_mask_area_threshold The overlap mask area threshold.\n     * @returns {[boolean, number[]]} Whether the segment is valid or not, and the indices of the valid labels.\n     */\n    check_segment_validity(\n        mask_labels,\n        mask_probs,\n        k,\n        mask_threshold = 0.5,\n        overlap_mask_area_threshold = 0.8\n    ) {\n        // mask_k is a 1D array of indices, indicating where the mask is equal to k\n        let mask_k = [];\n        let mask_k_area = 0;\n        let original_area = 0;\n\n        // Compute the area of all the stuff in query k\n        for (let i = 0; i < mask_labels.length; ++i) {\n            if (mask_labels[i] === k) {\n                mask_k.push(i);\n                ++mask_k_area;\n            }\n\n            if (mask_probs[k].data[i] >= mask_threshold) {\n                ++original_area;\n            }\n        }\n        let mask_exists = mask_k_area > 0 && original_area > 0;\n\n        // Eliminate disconnected tiny segments\n        if (mask_exists) {\n            // Perform additional check\n            let area_ratio = mask_k_area / original_area;\n            mask_exists = area_ratio > overlap_mask_area_threshold;\n        }\n\n        return [mask_exists, mask_k]\n    }\n\n    /**\n     * Computes the segments.\n     * @param {Tensor[]} mask_probs The mask probabilities.\n     * @param {number[]} pred_scores The predicted scores.\n     * @param {number[]} pred_labels The predicted labels.\n     * @param {number} mask_threshold The mask threshold.\n     * @param {number} overlap_mask_area_threshold The overlap mask area threshold.\n     * @param {Set<number>} label_ids_to_fuse The label ids to fuse.\n     * @param {number[]} target_size The target size of the image.\n     * @returns {[Tensor, Array<{id: number, label_id: number, score: number}>]} The computed segments.\n     */\n    compute_segments(\n        mask_probs,\n        pred_scores,\n        pred_labels,\n        mask_threshold,\n        overlap_mask_area_threshold,\n        label_ids_to_fuse = null,\n        target_size = null,\n    ) {\n        let [height, width] = target_size ?? mask_probs[0].dims;\n\n        let segmentation = new Tensor(\n            'int32',\n            new Int32Array(height * width),\n            [height, width]\n        );\n        let segments = [];\n\n        // 1. If target_size is not null, we need to resize the masks to the target size\n        if (target_size !== null) {\n            // resize the masks to the target size\n            for (let i = 0; i < mask_probs.length; ++i) {\n                mask_probs[i] = interpolate(mask_probs[i], target_size, 'bilinear', false);\n            }\n        }\n\n        // 2. Weigh each mask by its prediction score\n        // NOTE: `mask_probs` is updated in-place\n        // \n        // Temporary storage for the best label/scores for each pixel ([height, width]):\n        let mask_labels = new Int32Array(mask_probs[0].data.length);\n        let bestScores = new Float32Array(mask_probs[0].data.length);\n\n        for (let i = 0; i < mask_probs.length; ++i) {\n            let score = pred_scores[i];\n\n            for (let j = 0; j < mask_probs[i].data.length; ++j) {\n                mask_probs[i].data[j] *= score\n                if (mask_probs[i].data[j] > bestScores[j]) {\n                    mask_labels[j] = i;\n                    bestScores[j] = mask_probs[i].data[j];\n                }\n            }\n        }\n\n        let current_segment_id = 0;\n\n        // let stuff_memory_list = {}\n        for (let k = 0; k < pred_labels.length; ++k) {\n            let pred_class = pred_labels[k];\n\n            // TODO add `should_fuse`\n            // let should_fuse = pred_class in label_ids_to_fuse\n\n            // Check if mask exists and large enough to be a segment\n            let [mask_exists, mask_k] = this.check_segment_validity(\n                mask_labels,\n                mask_probs,\n                k,\n                mask_threshold,\n                overlap_mask_area_threshold\n            )\n\n            if (!mask_exists) {\n                // Nothing to see here\n                continue;\n            }\n\n            // TODO\n            // if (pred_class in stuff_memory_list) {\n            //     current_segment_id = stuff_memory_list[pred_class]\n            // } else {\n            //     current_segment_id += 1;\n            // }\n            ++current_segment_id;\n\n\n            // Add current object segment to final segmentation map\n            for (let index of mask_k) {\n                segmentation.data[index] = current_segment_id;\n            }\n\n            segments.push({\n                id: current_segment_id,\n                label_id: pred_class,\n                // was_fused: should_fuse, TODO\n                score: pred_scores[k],\n            })\n\n            // TODO\n            // if(should_fuse){\n            //     stuff_memory_list[pred_class] = current_segment_id\n            // }\n        }\n\n        return [segmentation, segments];\n    }\n\n    /**\n     * Post-process the model output to generate the final panoptic segmentation.\n     * @param {*} outputs The model output to post process\n     * @param {number} [threshold=0.5] The probability score threshold to keep predicted instance masks.\n     * @param {number} [mask_threshold=0.5] Threshold to use when turning the predicted masks into binary values.\n     * @param {number} [overlap_mask_area_threshold=0.8] The overlap mask area threshold to merge or discard small disconnected parts within each binary instance mask.\n     * @param {Set<number>} [label_ids_to_fuse=null] The labels in this state will have all their instances be fused together.\n     * @param {number[][]} [target_sizes=null] The target sizes to resize the masks to.\n     * @returns {Array<{ segmentation: Tensor, segments_info: Array<{id: number, label_id: number, score: number}>}>}\n     */\n    post_process_panoptic_segmentation(\n        outputs,\n        threshold = 0.5,\n        mask_threshold = 0.5,\n        overlap_mask_area_threshold = 0.8,\n        label_ids_to_fuse = null,\n        target_sizes = null,\n    ) {\n        if (label_ids_to_fuse === null) {\n            console.warn(\"`label_ids_to_fuse` unset. No instance will be fused.\")\n            label_ids_to_fuse = new Set();\n        }\n\n        const class_queries_logits = outputs.logits; // [batch_size, num_queries, num_classes+1]\n        const masks_queries_logits = outputs.pred_masks; // [batch_size, num_queries, height, width]\n\n        const mask_probs = masks_queries_logits.sigmoid()  // [batch_size, num_queries, height, width]\n\n        let [batch_size, num_queries, num_labels] = class_queries_logits.dims;\n        num_labels -= 1; // Remove last class (background)\n\n        if (target_sizes !== null && target_sizes.length !== batch_size) {\n            throw Error(\"Make sure that you pass in as many target sizes as the batch dimension of the logits\")\n        }\n\n        let toReturn = [];\n        for (let i = 0; i < batch_size; ++i) {\n            let target_size = target_sizes !== null ? target_sizes[i] : null;\n\n            let class_logits = class_queries_logits[i];\n            let mask_logits = mask_probs[i];\n\n            let [mask_probs_item, pred_scores_item, pred_labels_item] = this.remove_low_and_no_objects(class_logits, mask_logits, threshold, num_labels);\n\n            if (pred_labels_item.length === 0) {\n                // No mask found\n                let [height, width] = target_size ?? mask_logits.dims.slice(-2);\n\n                let segmentation = new Tensor(\n                    'int32',\n                    new Int32Array(height * width).fill(-1),\n                    [height, width]\n                )\n                toReturn.push({\n                    segmentation: segmentation,\n                    segments_info: []\n                });\n                continue;\n            }\n\n\n            // Get segmentation map and segment information of batch item\n            let [segmentation, segments] = this.compute_segments(\n                mask_probs_item,\n                pred_scores_item,\n                pred_labels_item,\n                mask_threshold,\n                overlap_mask_area_threshold,\n                label_ids_to_fuse,\n                target_size,\n            )\n\n            toReturn.push({\n                segmentation: segmentation,\n                segments_info: segments\n            })\n        }\n\n        return toReturn;\n    }\n\n    post_process_instance_segmentation() {\n        // TODO\n        throw Error(\"Not implemented yet\");\n    }\n}\n\nexport class YolosFeatureExtractor extends ImageFeatureExtractor {\n    /** @type {post_process_object_detection} */\n    post_process_object_detection(...args) {\n        return post_process_object_detection(...args);\n    }\n}\n\n/**\n * @typedef {object} SamImageProcessorResult\n * @property {Tensor} pixel_values\n * @property {HeightWidth[]} original_sizes\n * @property {HeightWidth[]} reshaped_input_sizes\n * @property {Tensor} [input_points]\n * @property {Tensor} [input_labels]\n */\n\nexport class SamImageProcessor extends ImageFeatureExtractor {\n\n    /**\n     * \n     * @param {any} input_points \n     * @param {HeightWidth[]} original_sizes \n     * @param {HeightWidth[]} reshaped_input_sizes \n     * @returns {Tensor}\n     */\n    reshape_input_points(input_points, original_sizes, reshaped_input_sizes) {\n\n        // Make deep copy to avoid altering user's input\n        input_points = structuredClone(input_points);\n        let shape = calculateDimensions(input_points);\n\n        // TODO: add support for 2D input_points\n        if (shape.length === 3) {\n            // Correct user's input\n            shape = [1, ...shape];\n            input_points = [input_points];\n        } else if (shape.length !== 4) {\n            throw Error(\"The input_points must be a 4D tensor of shape `batch_size`, `point_batch_size`, `nb_points_per_image`, `2`.\")\n        }\n\n        // Reshape input points\n        for (let i = 0; i < input_points.length; ++i) { // batch_size\n            let originalImageSize = original_sizes[i];\n            let reshapedImageSize = reshaped_input_sizes[i];\n\n            let resizeFactors = [\n                reshapedImageSize[0] / originalImageSize[0],\n                reshapedImageSize[1] / originalImageSize[1]\n            ]\n\n            for (let j = 0; j < input_points[i].length; ++j) { // point_batch_size\n                for (let k = 0; k < input_points[i][j].length; ++k) { // nb_points_per_image\n                    for (let w = 0; w < input_points[i][j][k].length; ++w) { // 2\n                        input_points[i][j][k][w] *= resizeFactors[w];\n                    }\n                }\n            }\n        }\n\n        return new Tensor(\n            'float32',\n            Float32Array.from(input_points.flat(Infinity)),\n            shape\n        )\n\n    }\n\n    /**\n     * \n     * @param {any} input_labels \n     * @param {Tensor} input_points \n     * @returns {Tensor}\n     */\n    add_input_labels(input_labels, input_points) {\n        let shape = calculateDimensions(input_labels);\n        if (shape.length === 2) {\n            // Correct user's input\n            shape = [1, ...shape];\n            input_labels = [input_labels];\n        } else if (shape.length !== 3) {\n            throw Error(\"The input_points must be a 4D tensor of shape `batch_size`, `point_batch_size`, `nb_points_per_image`, `2`.\")\n        }\n\n        if (shape.some((x, i) => x !== input_points.dims[i])) {\n            throw Error(`The first ${shape.length} dimensions of 'input_points' and 'input_labels' must be the same.`)\n        }\n        return new Tensor(\n            'int64',\n            input_labels.flat(Infinity).map(BigInt),\n            shape,\n        )\n    }\n    /**\n     * @param {any[]} images The URL(s) of the image(s) to extract features from.\n     * @param {any} [input_points] A 3D or 4D array, representing the input points provided by the user.\n     * - 3D: `[point_batch_size, nb_points_per_image, 2]`. In this case, `batch_size` is assumed to be 1.\n     * - 4D: `[batch_size, point_batch_size, nb_points_per_image, 2]`.\n     * @param {any} [input_labels] A 2D or 3D array, representing the input labels for the points, used by the prompt encoder to encode the prompt.\n     * - 2D: `[point_batch_size, nb_points_per_image]`. In this case, `batch_size` is assumed to be 1.\n     * - 3D: `[batch_size, point_batch_size, nb_points_per_image]`.\n     * @returns {Promise<SamImageProcessorResult>}\n     */\n    async _call(images, input_points = null, input_labels = null) {\n        // TODO allow user to use preprocessed images\n        /** @type {SamImageProcessorResult} */\n        const processed = await super._call(images);\n\n        if (input_points) {\n            processed.input_points = this.reshape_input_points(\n                input_points, processed.original_sizes, processed.reshaped_input_sizes\n            );\n        }\n\n        if (input_labels) {\n            if (!processed.input_points) {\n                throw Error(\"`input_points` must be provided if `input_labels` are provided.\")\n            }\n            processed.input_labels = this.add_input_labels(input_labels, processed.input_points);\n        }\n\n        return processed;\n    }\n\n    /**\n     * Remove padding and upscale masks to the original image size.\n     * @param {Tensor} masks Batched masks from the mask_decoder in (batch_size, num_channels, height, width) format.\n     * @param {number[][]} original_sizes The original sizes of each image before it was resized to the model's expected input shape, in (height, width) format.\n     * @param {number[][]} reshaped_input_sizes The size of each image as it is fed to the model, in (height, width) format. Used to remove padding.\n     * @param {Object} options Optional parameters for post-processing.\n     * @param {number} [options.mask_threshold] The threshold to use for binarizing the masks.\n     * @param {boolean} [options.binarize] Whether to binarize the masks.\n     * @param {Object} [options.pad_size] The target size the images were padded to before being passed to the model. If `null`, the target size is assumed to be the processor's `pad_size`.\n     * @param {number} [options.pad_size.height] The height the images were padded to.\n     * @param {number} [options.pad_size.width] The width the images were padded to.\n     * @returns {Tensor[]} Batched masks in batch_size, num_channels, height, width) format, where (height, width) is given by original_size.\n     */\n    post_process_masks(masks, original_sizes, reshaped_input_sizes, {\n        mask_threshold = 0.0,\n        binarize = true,\n        pad_size = null,\n    } = {}) {\n        // masks: [1, 1, 3, 256, 256]\n\n        const output_masks = [];\n\n        pad_size = pad_size ?? this.pad_size;\n\n        const target_image_size = [pad_size.height, pad_size.width];\n\n        for (let i = 0; i < original_sizes.length; ++i) {\n            const original_size = original_sizes[i];\n            const reshaped_input_size = reshaped_input_sizes[i];\n\n            const mask = masks[i]; // [b, c, h, w]\n\n            // TODO: improve\n            const interpolated_masks = [];\n            for (let j = 0; j < mask.dims[0]; ++j) {\n                const m = mask[j]; // 3d tensor\n\n                // Upscale mask to padded size\n                let interpolated_mask = interpolate(m, target_image_size, 'bilinear', false);\n\n                // Crop mask\n                interpolated_mask = interpolated_mask.slice(null, [0, reshaped_input_size[0]], [0, reshaped_input_size[1]]);\n\n                // Downscale mask\n                interpolated_mask = interpolate(interpolated_mask, original_size, 'bilinear', false);\n\n                if (binarize) {\n                    const binarizedMaskData = new Uint8Array(interpolated_mask.data.length);\n                    for (let i = 0; i < interpolated_mask.data.length; ++i) {\n                        if (interpolated_mask.data[i] > mask_threshold) {\n                            binarizedMaskData[i] = 1;\n                        }\n                    }\n                    interpolated_mask = new Tensor(\n                        'bool',\n                        binarizedMaskData,\n                        interpolated_mask.dims\n                    )\n                }\n\n                interpolated_masks.push(interpolated_mask);\n            }\n\n            output_masks.push(stack(interpolated_masks));\n        }\n\n        return output_masks;\n    }\n}\n\nexport class Swin2SRImageProcessor extends ImageFeatureExtractor {\n    pad_image(pixelData, imgDims, padSize, options = {}) {\n        // NOTE: In this case, `padSize` represents the size of the sliding window for the local attention.\n        // In other words, the image is padded so that its width and height are multiples of `padSize`.\n        const [imageHeight, imageWidth, imageChannels] = imgDims;\n\n        return super.pad_image(pixelData, imgDims, {\n            // NOTE: For Swin2SR models, the original python implementation adds padding even when the image's width/height is already\n            // a multiple of `pad_size`. However, this is most likely a bug (PR: https://github.com/mv-lab/swin2sr/pull/19).\n            // For this reason, we only add padding when the image's width/height is not a multiple of `pad_size`.\n            width: imageWidth + (padSize - imageWidth % padSize) % padSize,\n            height: imageHeight + (padSize - imageHeight % padSize) % padSize,\n        }, {\n            mode: 'symmetric',\n            center: false,\n            constant_values: -1,\n            ...options,\n        })\n    }\n}\n\nexport class VitMatteImageProcessor extends ImageFeatureExtractor {\n    /**\n     * Calls the feature extraction process on an array of images, preprocesses\n     * each image, and concatenates the resulting features into a single Tensor.\n     * @param {RawImage[]} images The image(s) to extract features from.\n     * @param {RawImage[]} trimaps The trimaps(s) to extract features from.\n     * @returns {Promise<ImageFeatureExtractorResult>} An object containing the concatenated pixel values of the preprocessed images.\n     */\n    async _call(images, trimaps) {\n        if (!Array.isArray(images)) {\n            images = [images];\n        }\n        if (!Array.isArray(trimaps)) {\n            trimaps = [trimaps];\n        }\n\n        const imageData = await Promise.all(images.map(x => this.preprocess(x)));\n        const trimapData = await Promise.all(trimaps.map(x => this.preprocess(x, {\n            do_normalize: false,\n            do_convert_rgb: false,\n            do_convert_grayscale: true,\n        })));\n\n\n        // Stack pixel values\n        const pixel_values = stack(imageData.map(\n            // Concatenate images and trimaps\n            (x, i) => cat([x.pixel_values, trimapData[i].pixel_values], 0)\n        ), 0);\n\n        return {\n            pixel_values: pixel_values,\n\n            // Original sizes of images\n            original_sizes: imageData.map(x => x.original_size),\n\n            // Reshaped sizes of images, before padding or cropping\n            reshaped_input_sizes: imageData.map(x => x.reshaped_input_size),\n        }\n    }\n}\n\nexport class WhisperFeatureExtractor extends FeatureExtractor {\n\n    constructor(config) {\n        super(config);\n\n        // Prefer given `mel_filters` from preprocessor_config.json, or calculate them if they don't exist.\n        this.config.mel_filters ??= mel_filter_bank(\n            Math.floor(1 + this.config.n_fft / 2), // num_frequency_bins\n            this.config.feature_size, // num_mel_filters\n            0.0, // min_frequency\n            8000.0, // max_frequency\n            this.config.sampling_rate, // sampling_rate\n            \"slaney\", // norm\n            \"slaney\", // mel_scale\n        );\n\n        this.window = window_function(this.config.n_fft, 'hann');\n    }\n\n    /**\n     * Computes the log-Mel spectrogram of the provided audio waveform.\n     * @param {Float32Array|Float64Array} waveform The audio waveform to process.\n     * @returns {{data: Float32Array, dims: number[]}} An object containing the log-Mel spectrogram data as a Float32Array and its dimensions as an array of numbers.\n     */\n    _extract_fbank_features(waveform) {\n        const { data, dims } = spectrogram(\n            waveform,\n            this.window, // window\n            this.config.n_fft, // frame_length\n            this.config.hop_length, // hop_length\n            {\n                power: 2.0,\n                mel_filters: this.config.mel_filters,\n                log_mel: 'log10',\n\n                // Custom\n                max_num_frames: this.config.nb_max_frames, // 3000\n            }\n        )\n\n        const maxValue = max(data)[0];\n\n        for (let i = 0; i < data.length; ++i) {\n            data[i] = (Math.max(data[i], maxValue - 8.0) + 4.0) / 4.0;\n        }\n\n        return { data, dims };\n    }\n\n    /**\n     * Asynchronously extracts features from a given audio using the provided configuration.\n     * @param {Float32Array|Float64Array} audio The audio data as a Float32Array/Float64Array.\n     * @returns {Promise<{ input_features: Tensor }>} A Promise resolving to an object containing the extracted input features as a Tensor.\n     */\n    async _call(audio) {\n        validate_audio_inputs(audio, 'WhisperFeatureExtractor');\n\n        let waveform;\n        if (audio.length > this.config.n_samples) {\n            console.warn(\n                \"Attempting to extract features for audio longer than 30 seconds. \" +\n                \"If using a pipeline to extract transcript from a long audio clip, \" +\n                \"remember to specify `chunk_length_s` and/or `stride_length_s`.\"\n            );\n            waveform = audio.slice(0, this.config.n_samples);\n        } else {\n            // pad with zeros\n            waveform = new Float32Array(this.config.n_samples);\n            waveform.set(audio);\n        }\n\n        const { data, dims } = this._extract_fbank_features(waveform);\n\n        return {\n            input_features: new Tensor('float32',\n                data,\n                [1, ...dims]\n            )\n        };\n    }\n}\n\nexport class Wav2Vec2FeatureExtractor extends FeatureExtractor {\n\n    /**\n     * @param {Float32Array} input_values \n     * @returns {Float32Array} \n     */\n    _zero_mean_unit_var_norm(input_values) {\n        // TODO support batch?\n        const sum = input_values.reduce((a, b) => a + b, 0);\n        const mean = sum / input_values.length;\n        const variance = input_values.reduce((a, b) => a + (b - mean) ** 2, 0) / input_values.length;\n        return input_values.map(x => (x - mean) / Math.sqrt(variance + 1e-7));\n    }\n\n    /**\n     * Asynchronously extracts features from a given audio using the provided configuration.\n     * @param {Float32Array|Float64Array} audio The audio data as a Float32Array/Float64Array.\n     * @returns {Promise<{ input_values: Tensor; attention_mask: Tensor }>} A Promise resolving to an object containing the extracted input features and attention mask as Tensors.\n     */\n    async _call(audio) {\n        validate_audio_inputs(audio, 'Wav2Vec2FeatureExtractor');\n\n        if (audio instanceof Float64Array) {\n            audio = new Float32Array(audio);\n        }\n\n        let input_values = audio;\n\n        // zero-mean and unit-variance normalization\n        if (this.config.do_normalize) {\n            input_values = this._zero_mean_unit_var_norm(input_values);\n        }\n\n        // TODO: allow user to pass in attention mask\n        const shape = [1, input_values.length];\n        return {\n            input_values: new Tensor('float32', input_values, shape),\n            attention_mask: new Tensor('int64', new BigInt64Array(input_values.length).fill(1n), shape)\n        };\n    }\n}\n\nexport class SeamlessM4TFeatureExtractor extends FeatureExtractor {\n\n    constructor(config) {\n        super(config);\n\n        const sampling_rate = this.config.sampling_rate;\n        const mel_filters = mel_filter_bank(\n            256, // num_frequency_bins\n            this.config.num_mel_bins, // num_mel_filters\n            20, // min_frequency\n            Math.floor(sampling_rate / 2), // max_frequency\n            sampling_rate, // sampling_rate\n            null, // norm\n            \"kaldi\", // mel_scale\n            true, // triangularize_in_mel_space\n        );\n\n        // Do padding:\n        for (let i = 0; i < mel_filters.length; ++i) {\n            mel_filters[i].push(0);\n        }\n        this.mel_filters = mel_filters;\n\n        this.window = window_function(400, 'povey', {\n            periodic: false,\n        })\n    }\n\n    /**\n     * Computes the log-Mel spectrogram of the provided audio waveform.\n     * @param {Float32Array|Float64Array} waveform The audio waveform to process.\n     * @param {number} max_length The maximum number of frames to return.\n     * @returns {{data: Float32Array, dims: number[]}} An object containing the log-Mel spectrogram data as a Float32Array and its dimensions as an array of numbers.\n     */\n    _extract_fbank_features(waveform, max_length) {\n        // NOTE: We don't pad/truncate since that is passed in as `max_num_frames`\n\n        // Kaldi compliance: 16-bit signed integers\n        // 32768 == 2 ** 15\n        waveform = waveform.map((/** @type {number} */ x) => x * 32768)\n\n        return spectrogram(\n            waveform,\n            this.window, // window\n            400, // frame_length\n            160, // hop_length\n            {\n                fft_length: 512,\n                power: 2.0,\n                center: false,\n                preemphasis: 0.97,\n                mel_filters: this.mel_filters,\n                log_mel: 'log',\n                mel_floor: 1.192092955078125e-07,\n                remove_dc_offset: true,\n\n                // Custom\n                max_num_frames: max_length,\n                transpose: true,\n            }\n        )\n    }\n\n    /**\n     * Asynchronously extracts features from a given audio using the provided configuration.\n     * @param {Float32Array|Float64Array} audio The audio data as a Float32Array/Float64Array.\n     * @param {Object} options Optional parameters for feature extraction.\n     * @param {boolean} [options.padding=true] Whether to pad the sequence to a multiple of `pad_to_multiple_of`.\n     * @param {number} [options.pad_to_multiple_of=2] The number to pad the sequence to a multiple of.\n     * @param {boolean} [options.do_normalize_per_mel_bins=true] Whether or not to zero-mean unit-variance normalize the input per mel-channel.\n     * @param {boolean} [options.return_attention_mask=true] Whether to return the attention mask.\n     * @returns {Promise<{ input_features: Tensor, attention_mask?: Tensor }>} A Promise resolving to an object containing the extracted input features and attention masks as Tensors.\n     */\n    async _call(audio, {\n        padding = true,\n        pad_to_multiple_of = 2,\n        do_normalize_per_mel_bins = true,\n        return_attention_mask = true,\n    } = {}) {\n        validate_audio_inputs(audio, 'SeamlessM4TFeatureExtractor');\n\n        let features = this._extract_fbank_features(audio, this.config.max_length);\n\n        if (do_normalize_per_mel_bins) {\n            const [num_features, feature_size] = features.dims;\n            for (let i = 0; i < feature_size; ++i) {\n                let sum = 0;\n                for (let j = 0; j < num_features; ++j) {\n                    sum += features.data[j * feature_size + i];\n                }\n\n                const mean = sum / num_features;\n\n                let variance = 0;\n                for (let j = 0; j < num_features; ++j) {\n                    variance += (features.data[j * feature_size + i] - mean) ** 2;\n                }\n                variance /= num_features - 1; // NOTE: We use ddof=1\n\n                const std = Math.sqrt(variance + 1e-7);\n                for (let j = 0; j < num_features; ++j) {\n                    const index = j * feature_size + i;\n                    features.data[index] = (features.data[index] - mean) / std;\n                }\n            }\n        }\n\n        let padded_attention_mask;\n        if (padding) {\n            const [num_frames, num_channels] = features.dims;\n\n            const pad_size = num_frames % pad_to_multiple_of;\n            if (pad_size > 0) {\n                const padded_data = new Float32Array(num_channels * (num_frames + pad_size));\n                padded_data.set(features.data)\n                padded_data.fill(this.config.padding_value, features.data.length)\n\n                const numPaddedFrames = num_frames + pad_size;\n                features = {\n                    data: padded_data,\n                    dims: [numPaddedFrames, num_channels],\n                }\n\n                if (return_attention_mask) {\n                    padded_attention_mask = new Tensor(\n                        'int64',\n                        new BigInt64Array(numPaddedFrames),\n                        [1, numPaddedFrames],\n                    )\n                    padded_attention_mask.data.fill(1n, 0, num_frames);\n                }\n            }\n        }\n\n        const [num_frames, num_channels] = features.dims;\n\n        const stride = this.config.stride;\n        const remainder = num_frames % stride;\n        if (remainder !== 0) {\n            throw new Error(`The number of frames (${num_frames}) must be a multiple of the stride (${stride}).`)\n        }\n\n        const input_features = new Tensor('float32',\n            features.data,\n            features.dims,\n        ).view(\n            1,\n            Math.floor(num_frames / stride),\n            num_channels * stride,\n        );\n\n        const result = { input_features }\n\n        if (return_attention_mask) {\n            const reshapedNumFrames = input_features.dims[1];\n\n            const attention_mask = new Tensor(\n                'int64',\n                new BigInt64Array(reshapedNumFrames),\n                [1, reshapedNumFrames],\n            );\n            if (padded_attention_mask) {\n                for (let i = 1, j = 0; i < num_frames; i += stride, ++j) {\n                    attention_mask.data[j] = padded_attention_mask.data[i];\n                }\n            } else {\n                attention_mask.data.fill(1n);\n            }\n\n            result.attention_mask = attention_mask;\n        }\n\n        return result;\n    }\n}\n\nexport class ASTFeatureExtractor extends FeatureExtractor {\n\n\n    constructor(config) {\n        super(config);\n\n        const sampling_rate = this.config.sampling_rate;\n        const mel_filters = mel_filter_bank(\n            256, // num_frequency_bins\n            this.config.num_mel_bins, // num_mel_filters\n            20, // min_frequency\n            Math.floor(sampling_rate / 2), // max_frequency\n            sampling_rate, // sampling_rate\n            null, // norm\n            \"kaldi\", // mel_scale\n            true, // triangularize_in_mel_space\n        );\n\n        // Do padding:\n        for (let i = 0; i < mel_filters.length; ++i) {\n            mel_filters[i].push(0);\n        }\n        this.mel_filters = mel_filters;\n\n        this.window = window_function(400, 'hann', {\n            periodic: false,\n        })\n\n        this.mean = this.config.mean;\n        this.std = this.config.std;\n    }\n\n    /**\n     * Computes the log-Mel spectrogram of the provided audio waveform.\n     * @param {Float32Array|Float64Array} waveform The audio waveform to process.\n     * @param {number} max_length The maximum number of frames to return.\n     * @returns {{data: Float32Array, dims: number[]}} An object containing the log-Mel spectrogram data as a Float32Array and its dimensions as an array of numbers.\n     */\n    _extract_fbank_features(waveform, max_length) {\n        // NOTE: We don't pad/truncate since that is passed in as `max_num_frames`\n        return spectrogram(\n            waveform,\n            this.window, // window\n            400, // frame_length\n            160, // hop_length\n            {\n                fft_length: 512,\n                power: 2.0,\n                center: false,\n                preemphasis: 0.97,\n                mel_filters: this.mel_filters,\n                log_mel: 'log',\n                mel_floor: 1.192092955078125e-07,\n                remove_dc_offset: true,\n\n                // Custom\n                max_num_frames: max_length,\n                transpose: true,\n            }\n        )\n    }\n\n\n    /**\n     * Asynchronously extracts features from a given audio using the provided configuration.\n     * @param {Float32Array|Float64Array} audio The audio data as a Float32Array/Float64Array.\n     * @returns {Promise<{ input_values: Tensor }>} A Promise resolving to an object containing the extracted input features as a Tensor.\n     */\n    async _call(audio) {\n        validate_audio_inputs(audio, 'ASTFeatureExtractor');\n\n        const features = this._extract_fbank_features(audio, this.config.max_length);\n        if (this.config.do_normalize) {\n            // Normalize the input audio spectrogram to have mean=0, std=0.5\n            const denom = this.std * 2;\n            for (let i = 0; i < features.data.length; ++i) {\n                features.data[i] = (features.data[i] - this.mean) / denom;\n            }\n        }\n\n        return {\n            input_values: new Tensor('float32',\n                features.data,\n                [1, ...features.dims]\n            )\n        };\n    }\n}\n\nexport class ClapFeatureExtractor extends FeatureExtractor {\n\n    constructor(config) {\n        super(config);\n\n        this.mel_filters = mel_filter_bank(\n            this.config.nb_frequency_bins, // num_frequency_bins\n            this.config.feature_size, // num_mel_filters\n            this.config.frequency_min, // min_frequency\n            this.config.frequency_max, // max_frequency\n            this.config.sampling_rate, // sampling_rate\n            null, // norm\n            \"htk\", // mel_scale\n        );\n\n        this.mel_filters_slaney = mel_filter_bank(\n            this.config.nb_frequency_bins, // num_frequency_bins\n            this.config.feature_size, // num_mel_filters\n            this.config.frequency_min, // min_frequency\n            this.config.frequency_max, // max_frequency\n            this.config.sampling_rate, // sampling_rate\n            \"slaney\", // norm\n            \"slaney\", // mel_scale\n        );\n\n        this.window = window_function(this.config.fft_window_size, 'hann')\n\n    }\n\n\n    /**\n     * Extracts the mel spectrogram and prepares it for the mode based on the `truncation` and `padding` arguments.\n     * \n     * Four different path are possible:\n     *   - `truncation=\"fusion\"` and the length of the waveform is greater than the max length: the mel spectrogram\n     *     will be computed on the entire audio. 3 random crops and a dowsampled version of the full mel spectrogram\n     *     are then stacked together. They will later be used for `feature_fusion`.\n     *   - `truncation=\"rand_trunc\"` and the length of the waveform is smaller than the max length: the audio is\n     *     padded based on `padding`.\n     *   - `truncation=\"fusion\"` and the length of the waveform is smaller than the max length: the audio is padded\n     *     based on `padding`, and is repeated `4` times.\n     *   - `truncation=\"rand_trunc\"` and the length of the waveform is greater than the max length: the mel\n     *     spectrogram will be computed on a random crop of the waveform.\n     * \n     * @param {Float32Array|Float64Array} waveform The input waveform.\n     * @param {number} max_length The maximum length of the waveform.\n     * @param {string} truncation The truncation strategy to use.\n     * @param {string} padding The padding strategy to use.\n     * @returns {{ data: Float32Array; dims: number[]; longer: boolean; }} An object containing the mel spectrogram data as a Float32Array, its dimensions as an array of numbers, and a boolean indicating whether the waveform was longer than the max length.\n     */\n    _get_input_mel(waveform, max_length, truncation, padding) {\n\n        /** @type {{ data: Float32Array; dims: number[]}} */\n        let input_mel;\n        let longer = false;\n        const diff = waveform.length - max_length;\n        if (diff > 0) {\n            if (truncation === 'rand_trunc') {\n                longer = true;\n                const idx = Math.floor(Math.random() * (diff + 1));\n                waveform = waveform.subarray(idx, idx + max_length);\n\n                input_mel = this._extract_fbank_features(waveform, this.mel_filters_slaney, this.config.nb_max_samples);\n                input_mel.dims = [1, ...input_mel.dims]; // \"unsqueeze\"\n            } else {\n                // TODO implement fusion strategy\n                throw new Error(`Truncation strategy \"${truncation}\" not implemented`)\n            }\n        } else {\n            if (diff < 0) {\n                let padded = new Float64Array(max_length); // already padded with zeros\n                padded.set(waveform);\n\n                if (padding === 'repeat') {\n                    for (let i = waveform.length; i < max_length; i += waveform.length) {\n                        padded.set(waveform.subarray(0, Math.min(waveform.length, max_length - i)), i);\n                    }\n                } else if (padding === 'repeatpad') {\n                    for (let i = waveform.length; i < -diff; i += waveform.length) {\n                        padded.set(waveform, i);\n                    }\n                }\n                waveform = padded;\n            }\n\n            if (truncation === 'fusion') {\n                throw new Error(`Truncation strategy \"${truncation}\" not implemented`)\n            }\n\n            input_mel = this._extract_fbank_features(waveform, this.mel_filters_slaney, this.config.nb_max_samples);\n            input_mel.dims = [1, ...input_mel.dims]; // \"unsqueeze\"\n        }\n\n        return {\n            ...input_mel,\n            longer,\n        }\n    }\n\n    /**\n     * Compute the log-mel spectrogram of the provided `waveform` using the Hann window.\n     * In CLAP, two different filter banks are used depending on the truncation pattern:\n     *  - `self.mel_filters`: they correspond to the default parameters of `torchaudio` which can be obtained from\n     *    calling `torchaudio.transforms.MelSpectrogram().mel_scale.fb`. These filters are used when `truncation`\n     *    is set to `\"fusion\"`.\n     *  - `self.mel_filteres_slaney` : they correspond to the default parameters of `librosa` which used\n     *    `librosa.filters.mel` when computing the mel spectrogram. These filters were only used in the original\n     *    implementation when the truncation mode is not `\"fusion\"`.\n     * \n     * @param {Float32Array|Float64Array} waveform The audio waveform to process.\n     * @param {number[][]} mel_filters The mel filters to use.\n     * @param {number} [max_length=null] The maximum number of frames to return.\n     * @returns {{data: Float32Array, dims: number[]}} An object containing the log-Mel spectrogram data as a Float32Array and its dimensions as an array of numbers.\n     */\n    _extract_fbank_features(waveform, mel_filters, max_length = null) {\n        // NOTE: We don't pad/truncate since that is passed in as `max_num_frames`\n        return spectrogram(\n            waveform,\n            this.window, // window\n            this.config.fft_window_size, // frame_length\n            this.config.hop_length, // hop_length\n            {\n                power: 2.0,\n                mel_filters,\n                log_mel: 'dB',\n\n                // Custom\n                max_num_frames: max_length,\n                do_pad: false,\n                transpose: true,\n            }\n        )\n    }\n\n\n    /**\n     * Asynchronously extracts features from a given audio using the provided configuration.\n     * @param {Float32Array|Float64Array} audio The audio data as a Float32Array/Float64Array.\n     * @returns {Promise<{ input_features: Tensor }>} A Promise resolving to an object containing the extracted input features as a Tensor.\n     */\n    async _call(audio, {\n        max_length = null,\n    } = {}) {\n        validate_audio_inputs(audio, 'ClapFeatureExtractor');\n\n        // convert to mel spectrogram, truncate and pad if needed.\n        const padded_inputs = this._get_input_mel(\n            audio,\n            max_length ?? this.config.nb_max_samples,\n            this.config.truncation,\n            this.config.padding,\n        );\n\n\n        return {\n            input_features: new Tensor('float32',\n                padded_inputs.data,\n                [1, ...padded_inputs.dims]\n            )\n        };\n    }\n}\n\n\n\nexport class SpeechT5FeatureExtractor extends FeatureExtractor { }\n\n/**\n * Represents a Processor that extracts features from an input.\n * @extends Callable\n */\nexport class Processor extends Callable {\n    /**\n     * Creates a new Processor with the given feature extractor.\n     * @param {FeatureExtractor} feature_extractor The function used to extract features from the input.\n     */\n    constructor(feature_extractor) {\n        super();\n        this.feature_extractor = feature_extractor;\n        // TODO use tokenizer here?\n    }\n\n    /**\n     * Calls the feature_extractor function with the given input.\n     * @param {any} input The input to extract features from.\n     * @param {...any} args Additional arguments.\n     * @returns {Promise<any>} A Promise that resolves with the extracted features.\n     */\n    async _call(input, ...args) {\n        return await this.feature_extractor(input, ...args);\n    }\n}\n\nexport class SamProcessor extends Processor {\n    /**\n     * @borrows SamImageProcessor#_call as _call\n     */\n    async _call(...args) {\n        return await this.feature_extractor(...args);\n    }\n\n    /**\n     * @borrows SamImageProcessor#post_process_masks as post_process_masks\n     */\n    post_process_masks(...args) {\n        // @ts-ignore\n        return this.feature_extractor.post_process_masks(...args);\n    }\n    /**\n     * @borrows SamImageProcessor#reshape_input_points as reshape_input_points\n     */\n    reshape_input_points(...args) {\n        // @ts-ignore\n        return this.feature_extractor.reshape_input_points(...args);\n    }\n}\n\n/**\n * Represents a WhisperProcessor that extracts features from an audio input.\n * @extends Processor\n */\nexport class WhisperProcessor extends Processor {\n    /**\n     * Calls the feature_extractor function with the given audio input.\n     * @param {any} audio The audio input to extract features from.\n     * @returns {Promise<any>} A Promise that resolves with the extracted features.\n     */\n    async _call(audio) {\n        return await this.feature_extractor(audio)\n    }\n}\n\n\nexport class Wav2Vec2ProcessorWithLM extends Processor {\n    /**\n     * Calls the feature_extractor function with the given audio input.\n     * @param {any} audio The audio input to extract features from.\n     * @returns {Promise<any>} A Promise that resolves with the extracted features.\n     */\n    async _call(audio) {\n        return await this.feature_extractor(audio)\n    }\n}\n\nexport class SpeechT5Processor extends Processor {\n    /**\n     * Calls the feature_extractor function with the given input.\n     * @param {any} input The input to extract features from.\n     * @returns {Promise<any>} A Promise that resolves with the extracted features.\n     */\n    async _call(input) {\n        return await this.feature_extractor(input)\n    }\n}\n\nexport class OwlViTProcessor extends Processor { }\n\n\n//////////////////////////////////////////////////\n/**\n * Helper class which is used to instantiate pretrained processors with the `from_pretrained` function.\n * The chosen processor class is determined by the type specified in the processor config.\n * \n * **Example:** Load a processor using `from_pretrained`.\n * ```javascript\n * let processor = await AutoProcessor.from_pretrained('openai/whisper-tiny.en');\n * ```\n * \n * **Example:** Run an image through a processor.\n * ```javascript\n * let processor = await AutoProcessor.from_pretrained('Xenova/clip-vit-base-patch16');\n * let image = await RawImage.read('https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/football-match.jpg');\n * let image_inputs = await processor(image);\n * // {\n * //   \"pixel_values\": {\n * //     \"dims\": [ 1, 3, 224, 224 ],\n * //     \"type\": \"float32\",\n * //     \"data\": Float32Array [ -1.558687686920166, -1.558687686920166, -1.5440893173217773, ... ],\n * //     \"size\": 150528\n * //   },\n * //   \"original_sizes\": [\n * //     [ 533, 800 ]\n * //   ],\n * //   \"reshaped_input_sizes\": [\n * //     [ 224, 224 ]\n * //   ]\n * // }\n * ```\n */\nexport class AutoProcessor {\n    static FEATURE_EXTRACTOR_CLASS_MAPPING = {\n        ImageFeatureExtractor,\n        WhisperFeatureExtractor,\n        ViTFeatureExtractor,\n        MobileViTFeatureExtractor,\n        MobileViTImageProcessor,\n        OwlViTFeatureExtractor,\n        Owlv2ImageProcessor,\n        CLIPFeatureExtractor,\n        ChineseCLIPFeatureExtractor,\n        SiglipImageProcessor,\n        ConvNextFeatureExtractor,\n        ConvNextImageProcessor,\n        SegformerFeatureExtractor,\n        BitImageProcessor,\n        DPTImageProcessor,\n        DPTFeatureExtractor,\n        GLPNFeatureExtractor,\n        BeitFeatureExtractor,\n        DeiTFeatureExtractor,\n        DetrFeatureExtractor,\n        YolosFeatureExtractor,\n        DonutFeatureExtractor,\n        NougatImageProcessor,\n        EfficientNetImageProcessor,\n\n        ViTImageProcessor,\n        VitMatteImageProcessor,\n        SamImageProcessor,\n        Swin2SRImageProcessor,\n        Wav2Vec2FeatureExtractor,\n        SeamlessM4TFeatureExtractor,\n        SpeechT5FeatureExtractor,\n        ASTFeatureExtractor,\n        ClapFeatureExtractor,\n    }\n\n    static PROCESSOR_CLASS_MAPPING = {\n        WhisperProcessor,\n        Wav2Vec2ProcessorWithLM,\n        SamProcessor,\n        SpeechT5Processor,\n        OwlViTProcessor,\n    }\n\n    /**\n     * Instantiate one of the processor classes of the library from a pretrained model.\n     * \n     * The processor class to instantiate is selected based on the `feature_extractor_type` property of the config object\n     * (either passed as an argument or loaded from `pretrained_model_name_or_path` if possible)\n     * \n     * @param {string} pretrained_model_name_or_path The name or path of the pretrained model. Can be either:\n     * - A string, the *model id* of a pretrained processor hosted inside a model repo on huggingface.co.\n     *   Valid model ids can be located at the root-level, like `bert-base-uncased`, or namespaced under a\n     *   user or organization name, like `dbmdz/bert-base-german-cased`.\n     * - A path to a *directory* containing processor files, e.g., `./my_model_directory/`.\n     * @param {import('./utils/hub.js').PretrainedOptions} options Additional options for loading the processor.\n     * \n     * @returns {Promise<Processor>} A new instance of the Processor class.\n     */\n    static async from_pretrained(pretrained_model_name_or_path, {\n        progress_callback = null,\n        config = null,\n        cache_dir = null,\n        local_files_only = false,\n        revision = 'main',\n    } = {}) {\n\n        let preprocessorConfig = config ?? await getModelJSON(pretrained_model_name_or_path, 'preprocessor_config.json', true, {\n            progress_callback,\n            config,\n            cache_dir,\n            local_files_only,\n            revision,\n        })\n\n        // Determine feature extractor class\n        // TODO: Ensure backwards compatibility with old configs\n        let key = preprocessorConfig.feature_extractor_type ?? preprocessorConfig.image_processor_type;\n        let feature_extractor_class = this.FEATURE_EXTRACTOR_CLASS_MAPPING[key];\n\n        if (!feature_extractor_class) {\n            if (preprocessorConfig.size !== undefined) {\n                // Assume ImageFeatureExtractor\n                console.warn(`Feature extractor type \"${key}\" not found, assuming ImageFeatureExtractor due to size parameter in config.`);\n                feature_extractor_class = ImageFeatureExtractor;\n            } else {\n                throw new Error(`Unknown Feature Extractor type: ${key}`);\n            }\n        }\n\n        // If no associated processor class, use default\n        let processor_class = this.PROCESSOR_CLASS_MAPPING[preprocessorConfig.processor_class] ?? Processor;\n\n        // Instantiate processor and feature extractor\n        let feature_extractor = new feature_extractor_class(preprocessorConfig);\n        return new processor_class(feature_extractor);\n    }\n}\n//////////////////////////////////////////////////\n\n"],"mappings":"AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SACIA,QAAQ,EACRC,mBAAmB,EACnBC,sBAAsB,QACnB,iBAAiB;AAExB,SACIC,YAAY,QACT,gBAAgB;AAEvB,SACIC,GAAG,EACHC,GAAG,EACHC,OAAO,EACPC,aAAa,QACV,kBAAkB;AAGzB,SAASC,MAAM,EAAEC,OAAO,EAAEC,GAAG,EAAEC,WAAW,EAAEC,KAAK,QAAQ,mBAAmB;AAE5E,SAASC,QAAQ,QAAQ,kBAAkB;AAC3C,SACIC,eAAe,EACfC,WAAW,EACXC,eAAe,QACZ,kBAAkB;;AAGzB;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,SAASC,wBAAwBA,CAAC,CAACC,OAAO,EAAEC,OAAO,EAAEC,KAAK,EAAEC,MAAM,CAAC,EAAE;EACjE,OAAO,CACHH,OAAO,GAAGE,KAAK,GAAG,CAAC,EACnBD,OAAO,GAAGE,MAAM,GAAG,CAAC,EACpBH,OAAO,GAAGE,KAAK,GAAG,CAAC,EACnBD,OAAO,GAAGE,MAAM,GAAG,CAAC,CACvB;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAASC,6BAA6BA,CAACC,OAAO,EAAEC,SAAS,GAAG,GAAG,EAAEC,YAAY,GAAG,IAAI,EAAEC,YAAY,GAAG,KAAK,EAAE;EACxG,MAAMC,UAAU,GAAGJ,OAAO,CAACK,MAAM;EACjC,MAAMC,QAAQ,GAAGN,OAAO,CAACO,UAAU;EACnC,MAAM,CAACC,UAAU,EAAEC,SAAS,EAAEC,WAAW,CAAC,GAAGN,UAAU,CAACO,IAAI;EAE5D,IAAIT,YAAY,KAAK,IAAI,IAAIA,YAAY,CAACU,MAAM,KAAKJ,UAAU,EAAE;IAC7D,MAAMK,KAAK,CAAC,sFAAsF,CAAC;EACvG;EACA,IAAIC,QAAQ,GAAG,EAAE;EACjB,KAAK,IAAIC,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGP,UAAU,EAAE,EAAEO,CAAC,EAAE;IACjC,IAAIC,WAAW,GAAGd,YAAY,KAAK,IAAI,GAAGA,YAAY,CAACa,CAAC,CAAC,GAAG,IAAI;IAChE,IAAIE,IAAI,GAAG;MACPC,KAAK,EAAE,EAAE;MACTC,OAAO,EAAE,EAAE;MACXC,MAAM,EAAE;IACZ,CAAC;IACD,IAAIf,MAAM,GAAGD,UAAU,CAACW,CAAC,CAAC;IAC1B,IAAIM,IAAI,GAAGf,QAAQ,CAACS,CAAC,CAAC;IAEtB,KAAK,IAAIO,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGb,SAAS,EAAE,EAAEa,CAAC,EAAE;MAChC,IAAIC,KAAK,GAAGlB,MAAM,CAACiB,CAAC,CAAC;MAErB,IAAIE,OAAO,GAAG,EAAE;MAChB,IAAIC,KAAK;MACT,IAAItB,YAAY,EAAE;QACd;QACAsB,KAAK,GAAGF,KAAK,CAACG,OAAO,CAAC,CAAC,CAACC,IAAI;QAC5B,KAAK,IAAIC,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGH,KAAK,CAACb,MAAM,EAAE,EAAEgB,CAAC,EAAE;UACnC,IAAIH,KAAK,CAACG,CAAC,CAAC,GAAG3B,SAAS,EAAE;YACtBuB,OAAO,CAACK,IAAI,CAACD,CAAC,CAAC;UACnB;QACJ;MAEJ,CAAC,MAAM;QACH;QACA,IAAIE,QAAQ,GAAGhD,GAAG,CAACyC,KAAK,CAACI,IAAI,CAAC,CAAC,CAAC,CAAC;QAEjC,IAAIG,QAAQ,KAAKpB,WAAW,GAAG,CAAC,EAAE;UAC9B;UACA;QACJ;QACAc,OAAO,CAACK,IAAI,CAACC,QAAQ,CAAC;;QAEtB;QACAL,KAAK,GAAG1C,OAAO,CAACwC,KAAK,CAACI,IAAI,CAAC;MAC/B;MAEA,KAAK,MAAMI,KAAK,IAAIP,OAAO,EAAE;QAEzB;QACA;QACA,IAAIQ,GAAG,GAAGX,IAAI,CAACC,CAAC,CAAC,CAACK,IAAI;;QAEtB;QACAK,GAAG,GAAGtC,wBAAwB,CAACsC,GAAG,CAAC;QACnC,IAAIhB,WAAW,KAAK,IAAI,EAAE;UACtBgB,GAAG,GAAGA,GAAG,CAACC,GAAG,CAAC,CAACC,CAAC,EAAEnB,CAAC,KAAKmB,CAAC,GAAGlB,WAAW,CAAC,CAACD,CAAC,GAAG,CAAC,IAAI,CAAC,CAAC,CAAC;QACzD;QAEAE,IAAI,CAACC,KAAK,CAACW,IAAI,CAACG,GAAG,CAAC;QACpBf,IAAI,CAACE,OAAO,CAACU,IAAI,CAACE,KAAK,CAAC;QACxBd,IAAI,CAACG,MAAM,CAACS,IAAI,CAACJ,KAAK,CAACM,KAAK,CAAC,CAAC;MAClC;IACJ;IACAjB,QAAQ,CAACe,IAAI,CAACZ,IAAI,CAAC;EACvB;EACA,OAAOH,QAAQ;AACnB;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,SAASqB,qBAAqBA,CAACC,KAAK,EAAEC,iBAAiB,EAAE;EACrD,IAAI,EAAED,KAAK,YAAYE,YAAY,IAAIF,KAAK,YAAYG,YAAY,CAAC,EAAE;IACnE,MAAM,IAAI1B,KAAK,CACX,GAAGwB,iBAAiB,kEAAkED,KAAK,EAAEI,WAAW,EAAEC,IAAI,IAAI,OAAOL,KAAK,YAAY,GAC1I,2IACJ,CAAC;EACL;AACJ;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAASM,yBAAyBA,CAACC,GAAG,EAAEC,QAAQ,EAAEC,MAAM,GAAG,CAAC,EAAEC,MAAM,GAAG,IAAI,EAAE;EACzE,MAAMC,CAAC,GAAGJ,GAAG,GAAGC,QAAQ;EACxB,IAAIV,CAAC,GAAGlD,aAAa,CAAC+D,CAAC,CAAC,GAAGH,QAAQ;EAEnC,IAAIE,MAAM,KAAK,IAAI,IAAIZ,CAAC,GAAGY,MAAM,EAAE;IAC/BZ,CAAC,GAAGc,IAAI,CAACC,KAAK,CAACF,CAAC,CAAC,GAAGH,QAAQ;EAChC;EAEA,IAAIV,CAAC,GAAGW,MAAM,EAAE;IACZX,CAAC,GAAGc,IAAI,CAACE,IAAI,CAACH,CAAC,CAAC,GAAGH,QAAQ;EAC/B;EAEA,OAAOV,CAAC;AACZ;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,SAASiB,yBAAyBA,CAAC,CAACtD,KAAK,EAAEC,MAAM,CAAC,EAAEsD,OAAO,EAAE;EACzD,OAAO,CACHJ,IAAI,CAAClE,GAAG,CAACkE,IAAI,CAACC,KAAK,CAACpD,KAAK,GAAGuD,OAAO,CAAC,EAAE,CAAC,CAAC,GAAGA,OAAO,EAClDJ,IAAI,CAAClE,GAAG,CAACkE,IAAI,CAACC,KAAK,CAACnD,MAAM,GAAGsD,OAAO,CAAC,EAAE,CAAC,CAAC,GAAGA,OAAO,CACtD;AACL;;AAGA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMC,gBAAgB,SAAS5E,QAAQ,CAAC;EAC3C;AACJ;AACA;AACA;AACA;EACI+D,WAAWA,CAACc,MAAM,EAAE;IAChB,KAAK,CAAC,CAAC;IACP,IAAI,CAACA,MAAM,GAAGA,MAAM;EACxB;AACJ;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMC,qBAAqB,SAASF,gBAAgB,CAAC;EAExD;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACIb,WAAWA,CAACc,MAAM,EAAE;IAChB,KAAK,CAACA,MAAM,CAAC;IAEb,IAAI,CAACE,UAAU,GAAG,IAAI,CAACF,MAAM,CAACE,UAAU,IAAI,IAAI,CAACF,MAAM,CAACG,IAAI;IAC5D,IAAI,CAACC,SAAS,GAAG,IAAI,CAACJ,MAAM,CAACI,SAAS,IAAI,IAAI,CAACJ,MAAM,CAACK,GAAG;IAEzD,IAAI,CAACC,QAAQ,GAAG,IAAI,CAACN,MAAM,CAACM,QAAQ,IAAI,CAAC,CAAC,CAAC;IAC3C,IAAI,CAACC,UAAU,GAAG,IAAI,CAACP,MAAM,CAACO,UAAU,IAAI,IAAI;IAChD,IAAI,CAACC,cAAc,GAAG,IAAI,CAACR,MAAM,CAACQ,cAAc,IAAK,CAAC,GAAG,GAAI;IAC7D,IAAI,CAACC,YAAY,GAAG,IAAI,CAACT,MAAM,CAACS,YAAY;IAE5C,IAAI,CAACC,SAAS,GAAG,IAAI,CAACV,MAAM,CAACU,SAAS;IACtC,IAAI,CAACC,YAAY,GAAG,IAAI,CAACX,MAAM,CAACW,YAAY;IAC5C,IAAI,CAACC,IAAI,GAAG,IAAI,CAACZ,MAAM,CAACY,IAAI;IAC5B,IAAI,CAACC,iBAAiB,GAAG,IAAI,CAACb,MAAM,CAACa,iBAAiB,IAAI,IAAI,CAACb,MAAM,CAACc,YAAY;IAElF,IAAI,CAACC,cAAc,GAAG,IAAI,CAACf,MAAM,CAACe,cAAc;IAChD,IAAI,CAACC,SAAS,GAAG,IAAI,CAAChB,MAAM,CAACgB,SAAS;IACtC,IAAI,CAACC,cAAc,GAAG,IAAI,CAACjB,MAAM,CAACiB,cAAc,IAAI,IAAI;IACxD,IAAI,CAACC,cAAc,GAAG,IAAI,CAAClB,MAAM,CAACkB,cAAc;IAEhD,IAAI,CAACC,QAAQ,GAAG,IAAI,CAACnB,MAAM,CAACmB,QAAQ;IACpC,IAAI,CAACC,MAAM,GAAG,IAAI,CAACpB,MAAM,CAACoB,MAAM;IAEhC,IAAI,IAAI,CAACA,MAAM,IAAI,CAAC,IAAI,CAACD,QAAQ,IAAI,IAAI,CAACP,IAAI,IAAI,IAAI,CAACA,IAAI,CAACrE,KAAK,KAAK8E,SAAS,IAAI,IAAI,CAACT,IAAI,CAACpE,MAAM,KAAK6E,SAAS,EAAE;MAC/G;MACA;MACA,IAAI,CAACF,QAAQ,GAAG,IAAI,CAACP,IAAI;IAC7B;IAEA,IAAI,CAACU,qBAAqB,GAAG,IAAI,CAACtB,MAAM,CAACsB,qBAAqB,IAAI,KAAK;EAC3E;;EAEA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;EACI,MAAMC,SAASA,CAACC,KAAK,EAAEZ,IAAI,EAAEN,QAAQ,GAAG,CAAC,EAAE;IACvC,MAAMmB,YAAY,GAAGD,KAAK,CAAChF,MAAM;IACjC,MAAMkF,WAAW,GAAGF,KAAK,CAACjF,KAAK;IAE/B,MAAMoF,aAAa,GAAGf,IAAI,CAACpE,MAAM;IACjC,MAAMoF,YAAY,GAAGhB,IAAI,CAACrE,KAAK;;IAE/B;IACA,IAAIC,MAAM,GAAGkD,IAAI,CAACnE,GAAG,CAACkG,YAAY,EAAEE,aAAa,CAAC;IAClD,IAAIpF,KAAK,GAAGmD,IAAI,CAACnE,GAAG,CAACmG,WAAW,EAAEE,YAAY,CAAC;IAE/C,IAAIpF,MAAM,KAAKiF,YAAY,IAAIlF,KAAK,KAAKmF,WAAW,EAAE;MAClD,OAAOF,KAAK;IAChB;IACA,IAAIC,YAAY,GAAGC,WAAW,EAAE;MAC5BnF,KAAK,GAAGmD,IAAI,CAACC,KAAK,CAAC+B,WAAW,GAAGlF,MAAM,GAAGiF,YAAY,CAAC;IAC3D,CAAC,MAAM,IAAIC,WAAW,GAAGD,YAAY,EAAE;MACnCjF,MAAM,GAAGkD,IAAI,CAACC,KAAK,CAAC8B,YAAY,GAAGlF,KAAK,GAAGmF,WAAW,CAAC;IAC3D;IACA,OAAO,MAAMF,KAAK,CAACK,MAAM,CAACtF,KAAK,EAAEC,MAAM,EAAE;MAAE8D;IAAS,CAAC,CAAC;EAC1D;;EAGA;AACJ;AACA;AACA;AACA;AACA;EACI,MAAMwB,WAAWA,CAACN,KAAK,EAAEO,cAAc,GAAG,GAAG,EAAE;IAE3C,MAAMC,UAAU,GAAGR,KAAK,CAACS,KAAK,CAAC,CAAC,CAACC,SAAS,CAAC,CAAC;IAE5C,MAAMC,QAAQ,GAAG5G,GAAG,CAACyG,UAAU,CAAC3D,IAAI,CAAC,CAAC,CAAC,CAAC;IACxC,MAAM+D,QAAQ,GAAG5G,GAAG,CAACwG,UAAU,CAAC3D,IAAI,CAAC,CAAC,CAAC,CAAC;IACxC,MAAMgE,IAAI,GAAGD,QAAQ,GAAGD,QAAQ;IAEhC,IAAIE,IAAI,KAAK,CAAC,EAAE;MACZ,OAAOb,KAAK;IAChB;IAEA,MAAM7E,SAAS,GAAGoF,cAAc,GAAG,GAAG;IAEtC,IAAIO,KAAK,GAAGN,UAAU,CAACzF,KAAK;MAAEgG,KAAK,GAAGP,UAAU,CAACxF,MAAM;MAAEgG,KAAK,GAAG,CAAC;MAAEC,KAAK,GAAG,CAAC;IAC7E,KAAK,IAAIzE,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGgE,UAAU,CAACxF,MAAM,EAAE,EAAEwB,CAAC,EAAE;MACxC,MAAM0E,GAAG,GAAG1E,CAAC,GAAGgE,UAAU,CAACzF,KAAK;MAChC,KAAK,IAAIkB,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGuE,UAAU,CAACzF,KAAK,EAAE,EAAEkB,CAAC,EAAE;QACvC,IAAI,CAACuE,UAAU,CAAC3D,IAAI,CAACqE,GAAG,GAAGjF,CAAC,CAAC,GAAG0E,QAAQ,IAAIE,IAAI,GAAG1F,SAAS,EAAE;UAC1D;UACA2F,KAAK,GAAG5C,IAAI,CAACnE,GAAG,CAAC+G,KAAK,EAAE7E,CAAC,CAAC;UAC1B8E,KAAK,GAAG7C,IAAI,CAACnE,GAAG,CAACgH,KAAK,EAAEvE,CAAC,CAAC;UAC1BwE,KAAK,GAAG9C,IAAI,CAAClE,GAAG,CAACgH,KAAK,EAAE/E,CAAC,CAAC;UAC1BgF,KAAK,GAAG/C,IAAI,CAAClE,GAAG,CAACiH,KAAK,EAAEzE,CAAC,CAAC;QAC9B;MACJ;IACJ;IAEAwD,KAAK,GAAG,MAAMA,KAAK,CAACmB,IAAI,CAAC,CAACL,KAAK,EAAEC,KAAK,EAAEC,KAAK,EAAEC,KAAK,CAAC,CAAC;IACtD,OAAOjB,KAAK;EAChB;;EAEA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACIoB,SAASA,CAACC,SAAS,EAAEC,OAAO,EAAEC,OAAO,EAAE;IACnCC,IAAI,GAAG,UAAU;IACjBC,MAAM,GAAG,KAAK;IACdC,eAAe,GAAG;EACtB,CAAC,GAAG,CAAC,CAAC,EAAE;IACJ,MAAM,CAACC,WAAW,EAAEC,UAAU,EAAEC,aAAa,CAAC,GAAGP,OAAO;IAExD,IAAIQ,gBAAgB,EAAEC,iBAAiB;IACvC,IAAI,OAAOR,OAAO,KAAK,QAAQ,EAAE;MAC7BO,gBAAgB,GAAGP,OAAO;MAC1BQ,iBAAiB,GAAGR,OAAO;IAC/B,CAAC,MAAM;MACHO,gBAAgB,GAAGP,OAAO,CAACxG,KAAK;MAChCgH,iBAAiB,GAAGR,OAAO,CAACvG,MAAM;IACtC;;IAEA;IACA,IAAI8G,gBAAgB,KAAKF,UAAU,IAAIG,iBAAiB,KAAKJ,WAAW,EAAE;MACtE,MAAMK,eAAe,GAAG,IAAIxE,YAAY,CAACsE,gBAAgB,GAAGC,iBAAiB,GAAGF,aAAa,CAAC;MAC9F,IAAII,KAAK,CAACC,OAAO,CAACR,eAAe,CAAC,EAAE;QAChC;QACA,KAAK,IAAIzF,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG+F,eAAe,CAAClG,MAAM,EAAE,EAAEG,CAAC,EAAE;UAC7C+F,eAAe,CAAC/F,CAAC,CAAC,GAAGyF,eAAe,CAACzF,CAAC,GAAG4F,aAAa,CAAC;QAC3D;MACJ,CAAC,MAAM,IAAIH,eAAe,KAAK,CAAC,EAAE;QAC9BM,eAAe,CAACG,IAAI,CAACT,eAAe,CAAC;MACzC;MAEA,MAAM,CAACU,IAAI,EAAEC,GAAG,CAAC,GAAGZ,MAAM,GACpB,CAACvD,IAAI,CAACC,KAAK,CAAC,CAAC2D,gBAAgB,GAAGF,UAAU,IAAI,CAAC,CAAC,EAAE1D,IAAI,CAACC,KAAK,CAAC,CAAC4D,iBAAiB,GAAGJ,WAAW,IAAI,CAAC,CAAC,CAAC,GACpG,CAAC,CAAC,EAAE,CAAC,CAAC;;MAEZ;MACA,KAAK,IAAI1F,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG0F,WAAW,EAAE,EAAE1F,CAAC,EAAE;QAClC,MAAMgC,CAAC,GAAG,CAAChC,CAAC,GAAGoG,GAAG,IAAIP,gBAAgB;QACtC,MAAMQ,CAAC,GAAGrG,CAAC,GAAG2F,UAAU;QACxB,KAAK,IAAIpF,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGoF,UAAU,EAAE,EAAEpF,CAAC,EAAE;UACjC,MAAM+F,CAAC,GAAG,CAACtE,CAAC,GAAGzB,CAAC,GAAG4F,IAAI,IAAIP,aAAa;UACxC,MAAMW,CAAC,GAAG,CAACF,CAAC,GAAG9F,CAAC,IAAIqF,aAAa;UACjC,KAAK,IAAI/E,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG+E,aAAa,EAAE,EAAE/E,CAAC,EAAE;YACpCkF,eAAe,CAACO,CAAC,GAAGzF,CAAC,CAAC,GAAGuE,SAAS,CAACmB,CAAC,GAAG1F,CAAC,CAAC;UAC7C;QACJ;MACJ;MAEA,IAAI0E,IAAI,KAAK,WAAW,EAAE;QACtB,IAAIC,MAAM,EAAE;UACR,MAAM,IAAI1F,KAAK,CAAC,sEAAsE,CAAC;UACvF;QACJ;QACA,MAAM0G,EAAE,GAAGd,WAAW,GAAG,CAAC;QAC1B,MAAMe,EAAE,GAAGd,UAAU,GAAG,CAAC;QACzB,KAAK,IAAI3F,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG8F,iBAAiB,EAAE,EAAE9F,CAAC,EAAE;UACxC,MAAMgC,CAAC,GAAGhC,CAAC,GAAG6F,gBAAgB;UAC9B,MAAMQ,CAAC,GAAGzI,sBAAsB,CAACoC,CAAC,EAAEwG,EAAE,CAAC,GAAGb,UAAU;UAEpD,KAAK,IAAIpF,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGsF,gBAAgB,EAAE,EAAEtF,CAAC,EAAE;YACvC,IAAIP,CAAC,GAAG0F,WAAW,IAAInF,CAAC,GAAGoF,UAAU,EAAE,SAAS,CAAC;YACjD,MAAMW,CAAC,GAAG,CAACtE,CAAC,GAAGzB,CAAC,IAAIqF,aAAa;YACjC,MAAMW,CAAC,GAAG,CAACF,CAAC,GAAGzI,sBAAsB,CAAC2C,CAAC,EAAEkG,EAAE,CAAC,IAAIb,aAAa;;YAE7D;YACA,KAAK,IAAI/E,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG+E,aAAa,EAAE,EAAE/E,CAAC,EAAE;cACpCkF,eAAe,CAACO,CAAC,GAAGzF,CAAC,CAAC,GAAGuE,SAAS,CAACmB,CAAC,GAAG1F,CAAC,CAAC;YAC7C;UACJ;QACJ;MACJ;;MAGA;MACAuE,SAAS,GAAGW,eAAe;MAC3BV,OAAO,GAAG,CAACS,iBAAiB,EAAED,gBAAgB,EAAED,aAAa,CAAC;IAClE;IACA,OAAO,CAACR,SAAS,EAAEC,OAAO,CAAC;EAC/B;;EAEA;AACJ;AACA;AACA;AACA;EACIqB,OAAOA,CAACtB,SAAS,EAAE;IACf,KAAK,IAAIpF,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGoF,SAAS,CAACvF,MAAM,EAAE,EAAEG,CAAC,EAAE;MACvCoF,SAAS,CAACpF,CAAC,CAAC,GAAG,IAAI,CAAC+C,cAAc,GAAGqC,SAAS,CAACpF,CAAC,CAAC;IACrD;EACJ;;EAEA;AACJ;AACA;AACA;AACA;AACA;AACA;EACI2G,4BAA4BA,CAAC5C,KAAK,EAAEZ,IAAI,EAAE;IACtC;IACA;;IAEA,MAAM,CAACyD,QAAQ,EAAEC,SAAS,CAAC,GAAG9C,KAAK,CAACZ,IAAI;IAExC,IAAI2D,aAAa;IACjB,IAAIC,YAAY;IAEhB,IAAI,IAAI,CAAC7D,YAAY,EAAE;MACnB;MACA,MAAM;QAAEnE,MAAM;QAAED;MAAM,CAAC,GAAGqE,IAAI;MAC9B2D,aAAa,GAAG7E,IAAI,CAACnE,GAAG,CAACiB,MAAM,EAAED,KAAK,CAAC;IAC3C;IACA;IAAA,KACK,IAAIkI,MAAM,CAACC,SAAS,CAAC9D,IAAI,CAAC,EAAE;MAC7B2D,aAAa,GAAG3D,IAAI;MACpB4D,YAAY,GAAG,IAAI,CAACxE,MAAM,CAAC2E,QAAQ,IAAIJ,aAAa;IAExD,CAAC,MAAM,IAAI3D,IAAI,KAAKS,SAAS,EAAE;MAC3B;MACAkD,aAAa,GAAG3D,IAAI,CAAC2D,aAAa;MAClCC,YAAY,GAAG5D,IAAI,CAAC4D,YAAY;IACpC;;IAEA;IACA;IACA,IAAID,aAAa,KAAKlD,SAAS,IAAImD,YAAY,KAAKnD,SAAS,EAAE;MAC3D;MACA;MACA,MAAMuD,iBAAiB,GAAGL,aAAa,KAAKlD,SAAS,GAC/C,CAAC,CAAC;MAAA,EACF3B,IAAI,CAAClE,GAAG,CAAC+I,aAAa,GAAGF,QAAQ,EAAEE,aAAa,GAAGD,SAAS,CAAC;MAEnE,MAAMO,QAAQ,GAAGR,QAAQ,GAAGO,iBAAiB;MAC7C,MAAME,SAAS,GAAGR,SAAS,GAAGM,iBAAiB;;MAE/C;MACA;MACA,MAAMG,gBAAgB,GAAGP,YAAY,KAAKnD,SAAS,GAC7C,CAAC,CAAC;MAAA,EACF3B,IAAI,CAACnE,GAAG,CAACiJ,YAAY,GAAGK,QAAQ,EAAEL,YAAY,GAAGM,SAAS,CAAC;;MAEjE;MACA,IAAIE,UAAU,GAAGtF,IAAI,CAACC,KAAK,CAAC8E,MAAM,CAAC,CAACI,QAAQ,GAAGE,gBAAgB,EAAEE,OAAO,CAAC,CAAC,CAAC,CAAC,CAAC;MAC7E,IAAIC,WAAW,GAAGxF,IAAI,CAACC,KAAK,CAAC8E,MAAM,CAAC,CAACK,SAAS,GAAGC,gBAAgB,EAAEE,OAAO,CAAC,CAAC,CAAC,CAAC,CAAC;MAE/E,IAAI,IAAI,CAACpE,iBAAiB,KAAKQ,SAAS,EAAE;QACtC,CAAC2D,UAAU,EAAEE,WAAW,CAAC,GAAGrF,yBAAyB,CAAC,CAACmF,UAAU,EAAEE,WAAW,CAAC,EAAE,IAAI,CAACrE,iBAAiB,CAAC;MAC5G;MACA,OAAO,CAACmE,UAAU,EAAEE,WAAW,CAAC;IAEpC,CAAC,MAAM,IAAItE,IAAI,KAAKS,SAAS,IAAIT,IAAI,CAACrE,KAAK,KAAK8E,SAAS,IAAIT,IAAI,CAACpE,MAAM,KAAK6E,SAAS,EAAE;MACpF;;MAEA,IAAIwD,QAAQ,GAAGjE,IAAI,CAACrE,KAAK;MACzB,IAAIuI,SAAS,GAAGlE,IAAI,CAACpE,MAAM;;MAE3B;MACA,IAAI,IAAI,CAACwD,MAAM,CAACmF,iBAAiB,IAAI,IAAI,CAACnF,MAAM,CAACoF,kBAAkB,EAAE;QAEjE;QACA,IAAIC,YAAY,GAAGP,SAAS,GAAGR,SAAS;QACxC,IAAIgB,WAAW,GAAGT,QAAQ,GAAGR,QAAQ;;QAErC;QACA,IAAI3E,IAAI,CAAC6F,GAAG,CAAC,CAAC,GAAGD,WAAW,CAAC,GAAG5F,IAAI,CAAC6F,GAAG,CAAC,CAAC,GAAGF,YAAY,CAAC,EAAE;UACxD;UACAA,YAAY,GAAGC,WAAW;QAC9B,CAAC,MAAM;UACH;UACAA,WAAW,GAAGD,YAAY;QAC9B;QAEAP,SAAS,GAAG1F,yBAAyB,CAACiG,YAAY,GAAGf,SAAS,EAAE,IAAI,CAACtE,MAAM,CAACoF,kBAAkB,CAAC;QAC/FP,QAAQ,GAAGzF,yBAAyB,CAACkG,WAAW,GAAGjB,QAAQ,EAAE,IAAI,CAACrE,MAAM,CAACoF,kBAAkB,CAAC;MAChG;MAEA,OAAO,CAACP,QAAQ,EAAEC,SAAS,CAAC;IAEhC,CAAC,MAAM,IAAI,IAAI,CAACjE,iBAAiB,KAAKQ,SAAS,EAAE;MAC7C,OAAOxB,yBAAyB,CAAC,CAACwE,QAAQ,EAAEC,SAAS,CAAC,EAAE,IAAI,CAACzD,iBAAiB,CAAC;IACnF,CAAC,MAAM;MACH,MAAM,IAAItD,KAAK,CAAC,6EAA6EiI,IAAI,CAACC,SAAS,CAAC7E,IAAI,CAAC,EAAE,CAAC;IACxH;EACJ;;EAEA;AACJ;AACA;AACA;AACA;EACI,MAAMiB,MAAMA,CAACL,KAAK,EAAE;IAChB,MAAM,CAACqD,QAAQ,EAAEC,SAAS,CAAC,GAAG,IAAI,CAACV,4BAA4B,CAAC5C,KAAK,EAAE,IAAI,CAACZ,IAAI,CAAC;IACjF,OAAO,MAAMY,KAAK,CAACK,MAAM,CAACgD,QAAQ,EAAEC,SAAS,EAAE;MAC3CxE,QAAQ,EAAE,IAAI,CAACA;IACnB,CAAC,CAAC;EACN;;EAEA;AACJ;AACA;AACA;AACA;AACA;;EAEI;AACJ;AACA;AACA;AACA;AACA;AACA;EACI,MAAMoF,UAAUA,CAAClE,KAAK,EAAE;IACpBf,YAAY,GAAG,IAAI;IACnBW,MAAM,GAAG,IAAI;IACbH,cAAc,GAAG,IAAI;IACrB0E,oBAAoB,GAAG,IAAI;IAC3BrE,qBAAqB,GAAG;EAC5B,CAAC,GAAG,CAAC,CAAC,EAAE;IACJ,IAAI,IAAI,CAACJ,cAAc,EAAE;MACrB;MACA;MACAM,KAAK,GAAG,MAAM,IAAI,CAACM,WAAW,CAACN,KAAK,CAAC;IACzC;IAEA,MAAM,CAAC6C,QAAQ,EAAEC,SAAS,CAAC,GAAG9C,KAAK,CAACZ,IAAI,CAAC,CAAC;;IAE1C;IACA,IAAIK,cAAc,IAAI,IAAI,CAACA,cAAc,EAAE;MACvCO,KAAK,GAAGA,KAAK,CAACoE,GAAG,CAAC,CAAC;IACvB,CAAC,MAAM,IAAID,oBAAoB,EAAE;MAC7BnE,KAAK,GAAGA,KAAK,CAACU,SAAS,CAAC,CAAC;IAC7B;;IAEA;IACA;;IAEA;IACA,IAAI,IAAI,CAACxB,SAAS,EAAE;MAChBc,KAAK,GAAG,MAAM,IAAI,CAACK,MAAM,CAACL,KAAK,CAAC;IACpC;;IAEA;IACA,IAAI,IAAI,CAACb,YAAY,EAAE;MACnBa,KAAK,GAAG,MAAM,IAAI,CAACD,SAAS,CAACC,KAAK,EAAE,IAAI,CAACZ,IAAI,EAAE,IAAI,CAACN,QAAQ,CAAC;IACjE;IAEA,IAAI,IAAI,CAACS,cAAc,EAAE;MAErB,IAAI8E,UAAU;MACd,IAAIC,WAAW;MACf,IAAIrB,MAAM,CAACC,SAAS,CAAC,IAAI,CAAC1D,SAAS,CAAC,EAAE;QAClC6E,UAAU,GAAG,IAAI,CAAC7E,SAAS;QAC3B8E,WAAW,GAAG,IAAI,CAAC9E,SAAS;MAChC,CAAC,MAAM;QACH6E,UAAU,GAAG,IAAI,CAAC7E,SAAS,CAACzE,KAAK;QACjCuJ,WAAW,GAAG,IAAI,CAAC9E,SAAS,CAACxE,MAAM;MACvC;MAEAgF,KAAK,GAAG,MAAMA,KAAK,CAACuE,WAAW,CAACF,UAAU,EAAEC,WAAW,CAAC;IAC5D;;IAEA;IACA,MAAME,mBAAmB,GAAG,CAACxE,KAAK,CAAChF,MAAM,EAAEgF,KAAK,CAACjF,KAAK,CAAC;;IAEvD;IACA;IACA;IACA,IAAIsG,SAAS,GAAG7D,YAAY,CAACiH,IAAI,CAACzE,KAAK,CAACnD,IAAI,CAAC;IAC7C,IAAIyE,OAAO,GAAG,CAACtB,KAAK,CAAChF,MAAM,EAAEgF,KAAK,CAACjF,KAAK,EAAEiF,KAAK,CAAC0E,QAAQ,CAAC;IAEzD,IAAI,IAAI,CAAC3F,UAAU,EAAE;MACjB,IAAI,CAAC4D,OAAO,CAACtB,SAAS,CAAC;IAC3B;IAEA,IAAIpC,YAAY,IAAI,IAAI,CAACA,YAAY,EAAE;MACnC,IAAIP,UAAU,GAAG,IAAI,CAACA,UAAU;MAChC,IAAI,CAACuD,KAAK,CAACC,OAAO,CAAC,IAAI,CAACxD,UAAU,CAAC,EAAE;QACjCA,UAAU,GAAG,IAAIuD,KAAK,CAACjC,KAAK,CAAC0E,QAAQ,CAAC,CAACvC,IAAI,CAACzD,UAAU,CAAC;MAC3D;MAEA,IAAIE,SAAS,GAAG,IAAI,CAACA,SAAS;MAC9B,IAAI,CAACqD,KAAK,CAACC,OAAO,CAAC,IAAI,CAACtD,SAAS,CAAC,EAAE;QAChCA,SAAS,GAAG,IAAIqD,KAAK,CAACjC,KAAK,CAAC0E,QAAQ,CAAC,CAACvC,IAAI,CAACzD,UAAU,CAAC;MAC1D;MAEA,IAAIA,UAAU,CAAC5C,MAAM,KAAKkE,KAAK,CAAC0E,QAAQ,IAAI9F,SAAS,CAAC9C,MAAM,KAAKkE,KAAK,CAAC0E,QAAQ,EAAE;QAC7E,MAAM,IAAI3I,KAAK,CAAC,qDAAqD2C,UAAU,CAAC5C,MAAM,wBAAwB8C,SAAS,CAAC9C,MAAM,qDAAqDkE,KAAK,CAAC0E,QAAQ,IAAI,CAAC;MAC1M;MAEA,KAAK,IAAIzI,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGoF,SAAS,CAACvF,MAAM,EAAEG,CAAC,IAAI+D,KAAK,CAAC0E,QAAQ,EAAE;QACvD,KAAK,IAAIlI,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGwD,KAAK,CAAC0E,QAAQ,EAAE,EAAElI,CAAC,EAAE;UACrC6E,SAAS,CAACpF,CAAC,GAAGO,CAAC,CAAC,GAAG,CAAC6E,SAAS,CAACpF,CAAC,GAAGO,CAAC,CAAC,GAAGkC,UAAU,CAAClC,CAAC,CAAC,IAAIoC,SAAS,CAACpC,CAAC,CAAC;QACxE;MACJ;IACJ;;IAEA;IACA,IAAIoD,MAAM,IAAI,IAAI,CAACA,MAAM,EAAE;MACvB,IAAI,IAAI,CAACD,QAAQ,EAAE;QACf,MAAMgF,MAAM,GAAG,IAAI,CAACvD,SAAS,CAACC,SAAS,EAAE,CAACrB,KAAK,CAAChF,MAAM,EAAEgF,KAAK,CAACjF,KAAK,EAAEiF,KAAK,CAAC0E,QAAQ,CAAC,EAAE,IAAI,CAAC/E,QAAQ,CAAC;QACpG,CAAC0B,SAAS,EAAEC,OAAO,CAAC,GAAGqD,MAAM,CAAC,CAAC;MACnC,CAAC,MAAM,IAAI,IAAI,CAACtF,iBAAiB,EAAE;QAC/B,MAAM,CAACuF,WAAW,EAAEC,YAAY,CAAC,GAAGxG,yBAAyB,CAAC,CAACiD,OAAO,CAAC,CAAC,CAAC,EAAEA,OAAO,CAAC,CAAC,CAAC,CAAC,EAAE,IAAI,CAACjC,iBAAiB,CAAC;QAC/G,CAACgC,SAAS,EAAEC,OAAO,CAAC,GAAG,IAAI,CAACF,SAAS,CAACC,SAAS,EAAEC,OAAO,EAAE;UAAEvG,KAAK,EAAE6J,WAAW;UAAE5J,MAAM,EAAE6J;QAAa,CAAC,CAAC;MAC3G;IACJ;IAEA,IAAI/E,qBAAqB,IAAI,IAAI,CAACA,qBAAqB,EAAE;MACrD,IAAIwB,OAAO,CAAC,CAAC,CAAC,KAAK,CAAC,EAAE;QAClB,MAAM,IAAIvF,KAAK,CAAC,0DAA0D,CAAC;MAC/E;MACA;MACA,KAAK,IAAIE,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGoF,SAAS,CAACvF,MAAM,EAAEG,CAAC,IAAI,CAAC,EAAE;QAC1C,MAAM6I,IAAI,GAAGzD,SAAS,CAACpF,CAAC,CAAC;QACzBoF,SAAS,CAACpF,CAAC,CAAC,GAAGoF,SAAS,CAACpF,CAAC,GAAG,CAAC,CAAC;QAC/BoF,SAAS,CAACpF,CAAC,GAAG,CAAC,CAAC,GAAG6I,IAAI;MAC3B;IACJ;IAEA,MAAMC,YAAY,GAAG,IAAI5K,MAAM,CAAC,SAAS,EAAEkH,SAAS,EAAEC,OAAO,CAAC,CACzDlH,OAAO,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;;IAEvB,OAAO;MACH4K,aAAa,EAAE,CAAClC,SAAS,EAAED,QAAQ,CAAC;MACpC2B,mBAAmB,EAAEA,mBAAmB;MACxCO,YAAY,EAAEA;IAClB,CAAC;EACL;;EAEA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;EACI,MAAME,KAAKA,CAACC,MAAM,EAAE,GAAGC,IAAI,EAAE;IACzB,IAAI,CAAClD,KAAK,CAACC,OAAO,CAACgD,MAAM,CAAC,EAAE;MACxBA,MAAM,GAAG,CAACA,MAAM,CAAC;IACrB;IACA;IACA,MAAME,SAAS,GAAG,MAAMC,OAAO,CAACC,GAAG,CAACJ,MAAM,CAAC/H,GAAG,CAACC,CAAC,IAAI,IAAI,CAAC8G,UAAU,CAAC9G,CAAC,CAAC,CAAC,CAAC;;IAExE;IACA,MAAM2H,YAAY,GAAGxK,KAAK,CAAC6K,SAAS,CAACjI,GAAG,CAACC,CAAC,IAAIA,CAAC,CAAC2H,YAAY,CAAC,EAAE,CAAC,CAAC;IAEjE,OAAO;MACHA,YAAY,EAAEA,YAAY;MAE1B;MACAQ,cAAc,EAAEH,SAAS,CAACjI,GAAG,CAACC,CAAC,IAAIA,CAAC,CAAC4H,aAAa,CAAC;MAEnD;MACAQ,oBAAoB,EAAEJ,SAAS,CAACjI,GAAG,CAACC,CAAC,IAAIA,CAAC,CAACoH,mBAAmB;IAClE,CAAC;EACL;AAEJ;AAEA,OAAO,MAAMiB,yBAAyB,SAAShH,qBAAqB,CAAC;EAEjE;AACJ;AACA;AACA;AACA;AACA;AACA;EACIiH,kCAAkCA,CAACxK,OAAO,EAAEE,YAAY,GAAG,IAAI,EAAE;IAE7D,MAAMG,MAAM,GAAGL,OAAO,CAACK,MAAM;IAC7B,MAAMG,UAAU,GAAGH,MAAM,CAACM,IAAI,CAAC,CAAC,CAAC;IAEjC,IAAIT,YAAY,KAAK,IAAI,IAAIA,YAAY,CAACU,MAAM,KAAKJ,UAAU,EAAE;MAC7D,MAAMK,KAAK,CAAC,sFAAsF,CAAC;IACvG;IAEA,MAAMC,QAAQ,GAAG,EAAE;IACnB,KAAK,IAAIC,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGP,UAAU,EAAE,EAAEO,CAAC,EAAE;MACjC,MAAMC,WAAW,GAAGd,YAAY,KAAK,IAAI,GAAGA,YAAY,CAACa,CAAC,CAAC,GAAG,IAAI;MAElE,IAAIY,IAAI,GAAGtB,MAAM,CAACU,CAAC,CAAC;;MAEpB;MACA,IAAIC,WAAW,KAAK,IAAI,EAAE;QACtB;QACAW,IAAI,GAAGvC,WAAW,CAACuC,IAAI,EAAEX,WAAW,EAAE,UAAU,EAAE,KAAK,CAAC;MAC5D;MACA,MAAM,CAAClB,MAAM,EAAED,KAAK,CAAC,GAAGmB,WAAW,IAAIW,IAAI,CAAChB,IAAI,CAAC8J,KAAK,CAAC,CAAC,CAAC,CAAC;MAE1D,MAAMC,YAAY,GAAG,IAAIzL,MAAM,CAC3B,OAAO,EACP,IAAI0L,UAAU,CAAC7K,MAAM,GAAGD,KAAK,CAAC,EAC9B,CAACC,MAAM,EAAED,KAAK,CAClB,CAAC;;MAED;MACA,MAAM+K,MAAM,GAAGjJ,IAAI,CAAC,CAAC,CAAC,CAACA,IAAI;MAC3B,KAAK,IAAIL,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGK,IAAI,CAAChB,IAAI,CAAC,CAAC,CAAC,EAAE,EAAEW,CAAC,EAAE;QACnC,MAAM0E,GAAG,GAAGrE,IAAI,CAACL,CAAC,CAAC,CAACK,IAAI;QACxB,KAAK,IAAIC,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGoE,GAAG,CAACpF,MAAM,EAAE,EAAEgB,CAAC,EAAE;UACjC,IAAIoE,GAAG,CAACpE,CAAC,CAAC,GAAGgJ,MAAM,CAAChJ,CAAC,CAAC,EAAE;YACpBgJ,MAAM,CAAChJ,CAAC,CAAC,GAAGoE,GAAG,CAACpE,CAAC,CAAC;YAClB8I,YAAY,CAAC/I,IAAI,CAACC,CAAC,CAAC,GAAGN,CAAC;UAC5B;QACJ;MACJ;;MAEA;MACA;MACA,MAAMuJ,QAAQ,GAAG,IAAI9D,KAAK,CAACpF,IAAI,CAAChB,IAAI,CAAC,CAAC,CAAC,CAAC;MACxC,MAAMmK,GAAG,GAAGJ,YAAY,CAAC/I,IAAI;MAC7B,KAAK,IAAIL,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGwJ,GAAG,CAAClK,MAAM,EAAE,EAAEU,CAAC,EAAE;QACjC,MAAMS,KAAK,GAAG+I,GAAG,CAACxJ,CAAC,CAAC;QACpBuJ,QAAQ,CAAC9I,KAAK,CAAC,GAAGA,KAAK;MAC3B;MACA;MACA,MAAMgJ,MAAM,GAAGF,QAAQ,CAACG,MAAM,CAAC9I,CAAC,IAAIA,CAAC,KAAKyC,SAAS,CAAC;MAEpD7D,QAAQ,CAACe,IAAI,CAAC;QAAE6I,YAAY;QAAEK;MAAO,CAAC,CAAC;IAC3C;IACA,OAAOjK,QAAQ;EACnB;AACJ;AACA,OAAO,MAAMmK,mBAAmB,SAAS1H,qBAAqB,CAAC;AAC/D,OAAO,MAAM2H,iBAAiB,SAASD,mBAAmB,CAAC,EAAG,CAAC;AAC/D,OAAO,MAAME,iBAAiB,SAAS5H,qBAAqB,CAAC;AAC7D,OAAO,MAAM6H,oBAAoB,SAAS7H,qBAAqB,CAAC;AAChE,OAAO,MAAM8H,oBAAoB,SAAS9H,qBAAqB,CAAC;AAChE,OAAO,MAAM+H,2BAA2B,SAAS/H,qBAAqB,CAAC;AACvE,OAAO,MAAMgI,oBAAoB,SAAShI,qBAAqB,CAAC;AAChE,OAAO,MAAMiI,wBAAwB,SAASjI,qBAAqB,CAAC;EAChEf,WAAWA,CAACc,MAAM,EAAE;IAChB,KAAK,CAACA,MAAM,CAAC;;IAEb;AACR;AACA;IACQ,IAAI,CAACmI,QAAQ,GAAG,IAAI,CAACnI,MAAM,CAACmI,QAAQ,IAAK,GAAG,GAAG,GAAI;EACvD;EAEA,MAAMtG,MAAMA,CAACL,KAAK,EAAE;IAChB,MAAM+C,aAAa,GAAG,IAAI,CAAC3D,IAAI,EAAE2D,aAAa;IAC9C,IAAIA,aAAa,KAAKlD,SAAS,EAAE;MAC7B,MAAM,IAAI9D,KAAK,CAAC,mDAAmD,CAAC;IACxE;IAEA,IAAIgH,aAAa,GAAG,GAAG,EAAE;MACrB;MACA,MAAM6D,oBAAoB,GAAG1I,IAAI,CAACC,KAAK,CAAC4E,aAAa,GAAG,IAAI,CAAC4D,QAAQ,CAAC;MAEtE,MAAM,CAACtD,QAAQ,EAAEC,SAAS,CAAC,GAAG,IAAI,CAACV,4BAA4B,CAAC5C,KAAK,EAAE;QACnE+C,aAAa,EAAE6D;MACnB,CAAC,CAAC;MAEF5G,KAAK,GAAG,MAAMA,KAAK,CAACK,MAAM,CAACgD,QAAQ,EAAEC,SAAS,EAAE;QAC5CxE,QAAQ,EAAE,IAAI,CAACA;MACnB,CAAC,CAAC;;MAEF;MACAkB,KAAK,GAAG,MAAMA,KAAK,CAACuE,WAAW,CAACxB,aAAa,EAAEA,aAAa,CAAC;IACjE,CAAC,MAAM;MACH;MACA/C,KAAK,GAAG,MAAMA,KAAK,CAACK,MAAM,CAAC0C,aAAa,EAAEA,aAAa,EAAE;QACrDjE,QAAQ,EAAE,IAAI,CAACA;MACnB,CAAC,CAAC;IACN;IAEA,OAAOkB,KAAK;EAChB;AACJ;AACA,OAAO,MAAM6G,sBAAsB,SAASH,wBAAwB,CAAC,EAAG,CAAE;AAC1E,OAAO,MAAMI,mBAAmB,SAASrI,qBAAqB,CAAC;AAC/D,OAAO,MAAMsI,iBAAiB,SAAStI,qBAAqB,CAAC;AAE7D,OAAO,MAAMuI,0BAA0B,SAASvI,qBAAqB,CAAC;EAClEf,WAAWA,CAACc,MAAM,EAAE;IAChB,KAAK,CAACA,MAAM,CAAC;IACb,IAAI,CAACyI,WAAW,GAAG,IAAI,CAACzI,MAAM,CAACyI,WAAW,IAAI,IAAI;IAClD,IAAI,IAAI,CAACA,WAAW,EAAE;MAClB,IAAI,CAACrI,SAAS,GAAG,IAAI,CAACA,SAAS,CAACzB,GAAG,CAACC,CAAC,IAAIA,CAAC,GAAGA,CAAC,CAAC;IACnD;EACJ;AACJ;AAGA,OAAO,MAAM8J,yBAAyB,SAASzI,qBAAqB,CAAC;AACrE,OAAO,MAAM0I,uBAAuB,SAASD,yBAAyB,CAAC,EAAG,CAAC;AAC3E,OAAO,MAAME,sBAAsB,SAAS3I,qBAAqB,CAAC;EAC9D;EACAxD,6BAA6BA,CAAC,GAAGkK,IAAI,EAAE;IACnC,OAAOlK,6BAA6B,CAAC,GAAGkK,IAAI,CAAC;EACjD;AACJ;AACA,OAAO,MAAMkC,mBAAmB,SAASD,sBAAsB,CAAC,EAAG,CAAC;;AAEpE,OAAO,MAAME,oBAAoB,SAAS7I,qBAAqB,CAAC;AAChE,OAAO,MAAM8I,oBAAoB,SAAS9I,qBAAqB,CAAC;AAChE,OAAO,MAAM+I,qBAAqB,SAAS/I,qBAAqB,CAAC;EAC7D2C,SAASA,CAACC,SAAS,EAAEC,OAAO,EAAEC,OAAO,EAAEkG,OAAO,GAAG,CAAC,CAAC,EAAE;IACjD,MAAM,CAAC9F,WAAW,EAAEC,UAAU,EAAEC,aAAa,CAAC,GAAGP,OAAO;IAExD,IAAI5C,UAAU,GAAG,IAAI,CAACA,UAAU;IAChC,IAAI,CAACuD,KAAK,CAACC,OAAO,CAAC,IAAI,CAACxD,UAAU,CAAC,EAAE;MACjCA,UAAU,GAAG,IAAIuD,KAAK,CAACJ,aAAa,CAAC,CAACM,IAAI,CAACzD,UAAU,CAAC;IAC1D;IAEA,IAAIE,SAAS,GAAG,IAAI,CAACA,SAAS;IAC9B,IAAI,CAACqD,KAAK,CAACC,OAAO,CAACtD,SAAS,CAAC,EAAE;MAC3BA,SAAS,GAAG,IAAIqD,KAAK,CAACJ,aAAa,CAAC,CAACM,IAAI,CAACzD,UAAU,CAAC;IACzD;IAEA,MAAMgD,eAAe,GAAGhD,UAAU,CAACvB,GAAG,CAAC,CAACC,CAAC,EAAEnB,CAAC,KAAK,CAAEmB,CAAC,GAAGwB,SAAS,CAAC3C,CAAC,CAAC,CAAC;IAEpE,OAAO,KAAK,CAACmF,SAAS,CAACC,SAAS,EAAEC,OAAO,EAAEC,OAAO,EAAE;MAChDE,MAAM,EAAE,IAAI;MAEZ;MACA;MACAC,eAAe,EAAEA,eAAe;MAChC,GAAG+F;IACP,CAAC,CAAC;EACN;AACJ;AACA,OAAO,MAAMC,oBAAoB,SAASF,qBAAqB,CAAC,EAAG,CAAC;;AAEpE;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMG,oBAAoB,SAASlJ,qBAAqB,CAAC;EAC5D;AACJ;AACA;AACA;AACA;AACA;EACI,MAAMwG,KAAKA,CAACC,MAAM,EAAE;IAChB,MAAM0C,MAAM,GAAG,MAAM,KAAK,CAAC3C,KAAK,CAACC,MAAM,CAAC;;IAExC;IACA;IACA;IACA,MAAM2C,QAAQ,GAAG,CAACD,MAAM,CAAC7C,YAAY,CAAClJ,IAAI,CAAC,CAAC,CAAC,EAAE,EAAE,EAAE,EAAE,CAAC;IACtD,MAAMiM,UAAU,GAAG,IAAI3N,MAAM,CACzB,OAAO,EACP,IAAI4N,aAAa,CAACF,QAAQ,CAACG,MAAM,CAAC,CAAC/J,CAAC,EAAEqE,CAAC,KAAKrE,CAAC,GAAGqE,CAAC,CAAC,CAAC,CAACH,IAAI,CAAC,EAAE,CAAC,EAC5D0F,QACJ,CAAC;IAED,OAAO;MAAE,GAAGD,MAAM;MAAEE;IAAW,CAAC;EACpC;;EAEA;AACJ;AACA;AACA;AACA;AACA;AACA;;EAEI;EACA7M,6BAA6BA,CAAC,GAAGkK,IAAI,EAAE;IACnC,OAAOlK,6BAA6B,CAAC,GAAGkK,IAAI,CAAC;EACjD;;EAEA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;EACI8C,yBAAyBA,CAACC,YAAY,EAAEC,WAAW,EAAEC,qBAAqB,EAAEC,UAAU,EAAE;IAEpF,IAAIC,eAAe,GAAG,EAAE;IACxB,IAAIC,gBAAgB,GAAG,EAAE;IACzB,IAAIC,gBAAgB,GAAG,EAAE;IAEzB,KAAK,IAAIhM,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG0L,YAAY,CAACrM,IAAI,CAAC,CAAC,CAAC,EAAE,EAAEW,CAAC,EAAE;MAC3C,IAAIiM,GAAG,GAAGP,YAAY,CAAC1L,CAAC,CAAC;MACzB,IAAIkM,IAAI,GAAGP,WAAW,CAAC3L,CAAC,CAAC;MAEzB,IAAImM,UAAU,GAAG3O,GAAG,CAACyO,GAAG,CAAC5L,IAAI,CAAC,CAAC,CAAC,CAAC;MACjC,IAAI8L,UAAU,KAAKN,UAAU,EAAE;QAC3B;QACA;MACJ;MAEA,IAAI/L,MAAM,GAAGrC,OAAO,CAACwO,GAAG,CAAC5L,IAAI,CAAC;MAC9B,IAAI+L,UAAU,GAAGtM,MAAM,CAACqM,UAAU,CAAC;MACnC,IAAIC,UAAU,GAAGR,qBAAqB,EAAE;QACpCE,eAAe,CAACvL,IAAI,CAAC2L,IAAI,CAAC;QAC1BH,gBAAgB,CAACxL,IAAI,CAAC6L,UAAU,CAAC;QACjCJ,gBAAgB,CAACzL,IAAI,CAAC4L,UAAU,CAAC;MACrC;IACJ;IAEA,OAAO,CAACL,eAAe,EAAEC,gBAAgB,EAAEC,gBAAgB,CAAC;EAEhE;;EAEA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACIK,sBAAsBA,CAClBC,WAAW,EACXC,UAAU,EACVjM,CAAC,EACDkM,cAAc,GAAG,GAAG,EACpBC,2BAA2B,GAAG,GAAG,EACnC;IACE;IACA,IAAIC,MAAM,GAAG,EAAE;IACf,IAAIC,WAAW,GAAG,CAAC;IACnB,IAAIC,aAAa,GAAG,CAAC;;IAErB;IACA,KAAK,IAAInN,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG6M,WAAW,CAAChN,MAAM,EAAE,EAAEG,CAAC,EAAE;MACzC,IAAI6M,WAAW,CAAC7M,CAAC,CAAC,KAAKa,CAAC,EAAE;QACtBoM,MAAM,CAACnM,IAAI,CAACd,CAAC,CAAC;QACd,EAAEkN,WAAW;MACjB;MAEA,IAAIJ,UAAU,CAACjM,CAAC,CAAC,CAACD,IAAI,CAACZ,CAAC,CAAC,IAAI+M,cAAc,EAAE;QACzC,EAAEI,aAAa;MACnB;IACJ;IACA,IAAIC,WAAW,GAAGF,WAAW,GAAG,CAAC,IAAIC,aAAa,GAAG,CAAC;;IAEtD;IACA,IAAIC,WAAW,EAAE;MACb;MACA,IAAIC,UAAU,GAAGH,WAAW,GAAGC,aAAa;MAC5CC,WAAW,GAAGC,UAAU,GAAGL,2BAA2B;IAC1D;IAEA,OAAO,CAACI,WAAW,EAAEH,MAAM,CAAC;EAChC;;EAEA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACIK,gBAAgBA,CACZR,UAAU,EACVS,WAAW,EACXC,WAAW,EACXT,cAAc,EACdC,2BAA2B,EAC3BS,iBAAiB,GAAG,IAAI,EACxBxN,WAAW,GAAG,IAAI,EACpB;IACE,IAAI,CAAClB,MAAM,EAAED,KAAK,CAAC,GAAGmB,WAAW,IAAI6M,UAAU,CAAC,CAAC,CAAC,CAAClN,IAAI;IAEvD,IAAI+J,YAAY,GAAG,IAAIzL,MAAM,CACzB,OAAO,EACP,IAAI0L,UAAU,CAAC7K,MAAM,GAAGD,KAAK,CAAC,EAC9B,CAACC,MAAM,EAAED,KAAK,CAClB,CAAC;IACD,IAAI4O,QAAQ,GAAG,EAAE;;IAEjB;IACA,IAAIzN,WAAW,KAAK,IAAI,EAAE;MACtB;MACA,KAAK,IAAID,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG8M,UAAU,CAACjN,MAAM,EAAE,EAAEG,CAAC,EAAE;QACxC8M,UAAU,CAAC9M,CAAC,CAAC,GAAG3B,WAAW,CAACyO,UAAU,CAAC9M,CAAC,CAAC,EAAEC,WAAW,EAAE,UAAU,EAAE,KAAK,CAAC;MAC9E;IACJ;;IAEA;IACA;IACA;IACA;IACA,IAAI4M,WAAW,GAAG,IAAIjD,UAAU,CAACkD,UAAU,CAAC,CAAC,CAAC,CAAClM,IAAI,CAACf,MAAM,CAAC;IAC3D,IAAI8N,UAAU,GAAG,IAAIpM,YAAY,CAACuL,UAAU,CAAC,CAAC,CAAC,CAAClM,IAAI,CAACf,MAAM,CAAC;IAE5D,KAAK,IAAIG,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG8M,UAAU,CAACjN,MAAM,EAAE,EAAEG,CAAC,EAAE;MACxC,IAAI4N,KAAK,GAAGL,WAAW,CAACvN,CAAC,CAAC;MAE1B,KAAK,IAAIO,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGuM,UAAU,CAAC9M,CAAC,CAAC,CAACY,IAAI,CAACf,MAAM,EAAE,EAAEU,CAAC,EAAE;QAChDuM,UAAU,CAAC9M,CAAC,CAAC,CAACY,IAAI,CAACL,CAAC,CAAC,IAAIqN,KAAK;QAC9B,IAAId,UAAU,CAAC9M,CAAC,CAAC,CAACY,IAAI,CAACL,CAAC,CAAC,GAAGoN,UAAU,CAACpN,CAAC,CAAC,EAAE;UACvCsM,WAAW,CAACtM,CAAC,CAAC,GAAGP,CAAC;UAClB2N,UAAU,CAACpN,CAAC,CAAC,GAAGuM,UAAU,CAAC9M,CAAC,CAAC,CAACY,IAAI,CAACL,CAAC,CAAC;QACzC;MACJ;IACJ;IAEA,IAAIsN,kBAAkB,GAAG,CAAC;;IAE1B;IACA,KAAK,IAAIhN,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG2M,WAAW,CAAC3N,MAAM,EAAE,EAAEgB,CAAC,EAAE;MACzC,IAAIiN,UAAU,GAAGN,WAAW,CAAC3M,CAAC,CAAC;;MAE/B;MACA;;MAEA;MACA,IAAI,CAACuM,WAAW,EAAEH,MAAM,CAAC,GAAG,IAAI,CAACL,sBAAsB,CACnDC,WAAW,EACXC,UAAU,EACVjM,CAAC,EACDkM,cAAc,EACdC,2BACJ,CAAC;MAED,IAAI,CAACI,WAAW,EAAE;QACd;QACA;MACJ;;MAEA;MACA;MACA;MACA;MACA;MACA;MACA,EAAES,kBAAkB;;MAGpB;MACA,KAAK,IAAI7M,KAAK,IAAIiM,MAAM,EAAE;QACtBtD,YAAY,CAAC/I,IAAI,CAACI,KAAK,CAAC,GAAG6M,kBAAkB;MACjD;MAEAH,QAAQ,CAAC5M,IAAI,CAAC;QACViN,EAAE,EAAEF,kBAAkB;QACtBG,QAAQ,EAAEF,UAAU;QACpB;QACAF,KAAK,EAAEL,WAAW,CAAC1M,CAAC;MACxB,CAAC,CAAC;;MAEF;MACA;MACA;MACA;IACJ;IAEA,OAAO,CAAC8I,YAAY,EAAE+D,QAAQ,CAAC;EACnC;;EAEA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACIO,kCAAkCA,CAC9BhP,OAAO,EACPC,SAAS,GAAG,GAAG,EACf6N,cAAc,GAAG,GAAG,EACpBC,2BAA2B,GAAG,GAAG,EACjCS,iBAAiB,GAAG,IAAI,EACxBtO,YAAY,GAAG,IAAI,EACrB;IACE,IAAIsO,iBAAiB,KAAK,IAAI,EAAE;MAC5BS,OAAO,CAACC,IAAI,CAAC,uDAAuD,CAAC;MACrEV,iBAAiB,GAAG,IAAIW,GAAG,CAAC,CAAC;IACjC;IAEA,MAAMC,oBAAoB,GAAGpP,OAAO,CAACK,MAAM,CAAC,CAAC;IAC7C,MAAMgP,oBAAoB,GAAGrP,OAAO,CAACsP,UAAU,CAAC,CAAC;;IAEjD,MAAMzB,UAAU,GAAGwB,oBAAoB,CAAC3N,OAAO,CAAC,CAAC,EAAE;;IAEnD,IAAI,CAAClB,UAAU,EAAE+O,WAAW,EAAEpC,UAAU,CAAC,GAAGiC,oBAAoB,CAACzO,IAAI;IACrEwM,UAAU,IAAI,CAAC,CAAC,CAAC;;IAEjB,IAAIjN,YAAY,KAAK,IAAI,IAAIA,YAAY,CAACU,MAAM,KAAKJ,UAAU,EAAE;MAC7D,MAAMK,KAAK,CAAC,sFAAsF,CAAC;IACvG;IAEA,IAAIC,QAAQ,GAAG,EAAE;IACjB,KAAK,IAAIC,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGP,UAAU,EAAE,EAAEO,CAAC,EAAE;MACjC,IAAIC,WAAW,GAAGd,YAAY,KAAK,IAAI,GAAGA,YAAY,CAACa,CAAC,CAAC,GAAG,IAAI;MAEhE,IAAIiM,YAAY,GAAGoC,oBAAoB,CAACrO,CAAC,CAAC;MAC1C,IAAIkM,WAAW,GAAGY,UAAU,CAAC9M,CAAC,CAAC;MAE/B,IAAI,CAACqM,eAAe,EAAEC,gBAAgB,EAAEC,gBAAgB,CAAC,GAAG,IAAI,CAACP,yBAAyB,CAACC,YAAY,EAAEC,WAAW,EAAEhN,SAAS,EAAEkN,UAAU,CAAC;MAE5I,IAAIG,gBAAgB,CAAC1M,MAAM,KAAK,CAAC,EAAE;QAC/B;QACA,IAAI,CAACd,MAAM,EAAED,KAAK,CAAC,GAAGmB,WAAW,IAAIiM,WAAW,CAACtM,IAAI,CAAC8J,KAAK,CAAC,CAAC,CAAC,CAAC;QAE/D,IAAIC,YAAY,GAAG,IAAIzL,MAAM,CACzB,OAAO,EACP,IAAI0L,UAAU,CAAC7K,MAAM,GAAGD,KAAK,CAAC,CAACoH,IAAI,CAAC,CAAC,CAAC,CAAC,EACvC,CAACnH,MAAM,EAAED,KAAK,CAClB,CAAC;QACDiB,QAAQ,CAACe,IAAI,CAAC;UACV6I,YAAY,EAAEA,YAAY;UAC1B8E,aAAa,EAAE;QACnB,CAAC,CAAC;QACF;MACJ;;MAGA;MACA,IAAI,CAAC9E,YAAY,EAAE+D,QAAQ,CAAC,GAAG,IAAI,CAACJ,gBAAgB,CAChDjB,eAAe,EACfC,gBAAgB,EAChBC,gBAAgB,EAChBQ,cAAc,EACdC,2BAA2B,EAC3BS,iBAAiB,EACjBxN,WACJ,CAAC;MAEDF,QAAQ,CAACe,IAAI,CAAC;QACV6I,YAAY,EAAEA,YAAY;QAC1B8E,aAAa,EAAEf;MACnB,CAAC,CAAC;IACN;IAEA,OAAO3N,QAAQ;EACnB;EAEA2O,kCAAkCA,CAAA,EAAG;IACjC;IACA,MAAM5O,KAAK,CAAC,qBAAqB,CAAC;EACtC;AACJ;AAEA,OAAO,MAAM6O,qBAAqB,SAASnM,qBAAqB,CAAC;EAC7D;EACAxD,6BAA6BA,CAAC,GAAGkK,IAAI,EAAE;IACnC,OAAOlK,6BAA6B,CAAC,GAAGkK,IAAI,CAAC;EACjD;AACJ;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,OAAO,MAAM0F,iBAAiB,SAASpM,qBAAqB,CAAC;EAEzD;AACJ;AACA;AACA;AACA;AACA;AACA;EACIqM,oBAAoBA,CAACC,YAAY,EAAExF,cAAc,EAAEC,oBAAoB,EAAE;IAErE;IACAuF,YAAY,GAAGC,eAAe,CAACD,YAAY,CAAC;IAC5C,IAAIE,KAAK,GAAGrR,mBAAmB,CAACmR,YAAY,CAAC;;IAE7C;IACA,IAAIE,KAAK,CAACnP,MAAM,KAAK,CAAC,EAAE;MACpB;MACAmP,KAAK,GAAG,CAAC,CAAC,EAAE,GAAGA,KAAK,CAAC;MACrBF,YAAY,GAAG,CAACA,YAAY,CAAC;IACjC,CAAC,MAAM,IAAIE,KAAK,CAACnP,MAAM,KAAK,CAAC,EAAE;MAC3B,MAAMC,KAAK,CAAC,6GAA6G,CAAC;IAC9H;;IAEA;IACA,KAAK,IAAIE,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG8O,YAAY,CAACjP,MAAM,EAAE,EAAEG,CAAC,EAAE;MAAE;MAC5C,IAAIiP,iBAAiB,GAAG3F,cAAc,CAACtJ,CAAC,CAAC;MACzC,IAAIkP,iBAAiB,GAAG3F,oBAAoB,CAACvJ,CAAC,CAAC;MAE/C,IAAImP,aAAa,GAAG,CAChBD,iBAAiB,CAAC,CAAC,CAAC,GAAGD,iBAAiB,CAAC,CAAC,CAAC,EAC3CC,iBAAiB,CAAC,CAAC,CAAC,GAAGD,iBAAiB,CAAC,CAAC,CAAC,CAC9C;MAED,KAAK,IAAI1O,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGuO,YAAY,CAAC9O,CAAC,CAAC,CAACH,MAAM,EAAE,EAAEU,CAAC,EAAE;QAAE;QAC/C,KAAK,IAAIM,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGiO,YAAY,CAAC9O,CAAC,CAAC,CAACO,CAAC,CAAC,CAACV,MAAM,EAAE,EAAEgB,CAAC,EAAE;UAAE;UAClD,KAAK,IAAIuO,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGN,YAAY,CAAC9O,CAAC,CAAC,CAACO,CAAC,CAAC,CAACM,CAAC,CAAC,CAAChB,MAAM,EAAE,EAAEuP,CAAC,EAAE;YAAE;YACrDN,YAAY,CAAC9O,CAAC,CAAC,CAACO,CAAC,CAAC,CAACM,CAAC,CAAC,CAACuO,CAAC,CAAC,IAAID,aAAa,CAACC,CAAC,CAAC;UAChD;QACJ;MACJ;IACJ;IAEA,OAAO,IAAIlR,MAAM,CACb,SAAS,EACTqD,YAAY,CAACiH,IAAI,CAACsG,YAAY,CAACO,IAAI,CAACC,QAAQ,CAAC,CAAC,EAC9CN,KACJ,CAAC;EAEL;;EAEA;AACJ;AACA;AACA;AACA;AACA;EACIO,gBAAgBA,CAACC,YAAY,EAAEV,YAAY,EAAE;IACzC,IAAIE,KAAK,GAAGrR,mBAAmB,CAAC6R,YAAY,CAAC;IAC7C,IAAIR,KAAK,CAACnP,MAAM,KAAK,CAAC,EAAE;MACpB;MACAmP,KAAK,GAAG,CAAC,CAAC,EAAE,GAAGA,KAAK,CAAC;MACrBQ,YAAY,GAAG,CAACA,YAAY,CAAC;IACjC,CAAC,MAAM,IAAIR,KAAK,CAACnP,MAAM,KAAK,CAAC,EAAE;MAC3B,MAAMC,KAAK,CAAC,6GAA6G,CAAC;IAC9H;IAEA,IAAIkP,KAAK,CAACS,IAAI,CAAC,CAACtO,CAAC,EAAEnB,CAAC,KAAKmB,CAAC,KAAK2N,YAAY,CAAClP,IAAI,CAACI,CAAC,CAAC,CAAC,EAAE;MAClD,MAAMF,KAAK,CAAC,aAAakP,KAAK,CAACnP,MAAM,oEAAoE,CAAC;IAC9G;IACA,OAAO,IAAI3B,MAAM,CACb,OAAO,EACPsR,YAAY,CAACH,IAAI,CAACC,QAAQ,CAAC,CAACpO,GAAG,CAACwO,MAAM,CAAC,EACvCV,KACJ,CAAC;EACL;EACA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACI,MAAMhG,KAAKA,CAACC,MAAM,EAAE6F,YAAY,GAAG,IAAI,EAAEU,YAAY,GAAG,IAAI,EAAE;IAC1D;IACA;IACA,MAAMG,SAAS,GAAG,MAAM,KAAK,CAAC3G,KAAK,CAACC,MAAM,CAAC;IAE3C,IAAI6F,YAAY,EAAE;MACda,SAAS,CAACb,YAAY,GAAG,IAAI,CAACD,oBAAoB,CAC9CC,YAAY,EAAEa,SAAS,CAACrG,cAAc,EAAEqG,SAAS,CAACpG,oBACtD,CAAC;IACL;IAEA,IAAIiG,YAAY,EAAE;MACd,IAAI,CAACG,SAAS,CAACb,YAAY,EAAE;QACzB,MAAMhP,KAAK,CAAC,iEAAiE,CAAC;MAClF;MACA6P,SAAS,CAACH,YAAY,GAAG,IAAI,CAACD,gBAAgB,CAACC,YAAY,EAAEG,SAAS,CAACb,YAAY,CAAC;IACxF;IAEA,OAAOa,SAAS;EACpB;;EAEA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACIC,kBAAkBA,CAACC,KAAK,EAAEvG,cAAc,EAAEC,oBAAoB,EAAE;IAC5DwD,cAAc,GAAG,GAAG;IACpB+C,QAAQ,GAAG,IAAI;IACfpM,QAAQ,GAAG;EACf,CAAC,GAAG,CAAC,CAAC,EAAE;IACJ;;IAEA,MAAMqM,YAAY,GAAG,EAAE;IAEvBrM,QAAQ,GAAGA,QAAQ,IAAI,IAAI,CAACA,QAAQ;IAEpC,MAAMsM,iBAAiB,GAAG,CAACtM,QAAQ,CAAC3E,MAAM,EAAE2E,QAAQ,CAAC5E,KAAK,CAAC;IAE3D,KAAK,IAAIkB,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGsJ,cAAc,CAACzJ,MAAM,EAAE,EAAEG,CAAC,EAAE;MAC5C,MAAM+I,aAAa,GAAGO,cAAc,CAACtJ,CAAC,CAAC;MACvC,MAAMuI,mBAAmB,GAAGgB,oBAAoB,CAACvJ,CAAC,CAAC;MAEnD,MAAMyM,IAAI,GAAGoD,KAAK,CAAC7P,CAAC,CAAC,CAAC,CAAC;;MAEvB;MACA,MAAMiQ,kBAAkB,GAAG,EAAE;MAC7B,KAAK,IAAI1P,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGkM,IAAI,CAAC7M,IAAI,CAAC,CAAC,CAAC,EAAE,EAAEW,CAAC,EAAE;QACnC,MAAM2P,CAAC,GAAGzD,IAAI,CAAClM,CAAC,CAAC,CAAC,CAAC;;QAEnB;QACA,IAAI4P,iBAAiB,GAAG9R,WAAW,CAAC6R,CAAC,EAAEF,iBAAiB,EAAE,UAAU,EAAE,KAAK,CAAC;;QAE5E;QACAG,iBAAiB,GAAGA,iBAAiB,CAACzG,KAAK,CAAC,IAAI,EAAE,CAAC,CAAC,EAAEnB,mBAAmB,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAEA,mBAAmB,CAAC,CAAC,CAAC,CAAC,CAAC;;QAE3G;QACA4H,iBAAiB,GAAG9R,WAAW,CAAC8R,iBAAiB,EAAEpH,aAAa,EAAE,UAAU,EAAE,KAAK,CAAC;QAEpF,IAAI+G,QAAQ,EAAE;UACV,MAAMM,iBAAiB,GAAG,IAAIC,UAAU,CAACF,iBAAiB,CAACvP,IAAI,CAACf,MAAM,CAAC;UACvE,KAAK,IAAIG,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGmQ,iBAAiB,CAACvP,IAAI,CAACf,MAAM,EAAE,EAAEG,CAAC,EAAE;YACpD,IAAImQ,iBAAiB,CAACvP,IAAI,CAACZ,CAAC,CAAC,GAAG+M,cAAc,EAAE;cAC5CqD,iBAAiB,CAACpQ,CAAC,CAAC,GAAG,CAAC;YAC5B;UACJ;UACAmQ,iBAAiB,GAAG,IAAIjS,MAAM,CAC1B,MAAM,EACNkS,iBAAiB,EACjBD,iBAAiB,CAACvQ,IACtB,CAAC;QACL;QAEAqQ,kBAAkB,CAACnP,IAAI,CAACqP,iBAAiB,CAAC;MAC9C;MAEAJ,YAAY,CAACjP,IAAI,CAACxC,KAAK,CAAC2R,kBAAkB,CAAC,CAAC;IAChD;IAEA,OAAOF,YAAY;EACvB;AACJ;AAEA,OAAO,MAAMO,qBAAqB,SAAS9N,qBAAqB,CAAC;EAC7D2C,SAASA,CAACC,SAAS,EAAEC,OAAO,EAAEC,OAAO,EAAEkG,OAAO,GAAG,CAAC,CAAC,EAAE;IACjD;IACA;IACA,MAAM,CAAC9F,WAAW,EAAEC,UAAU,EAAEC,aAAa,CAAC,GAAGP,OAAO;IAExD,OAAO,KAAK,CAACF,SAAS,CAACC,SAAS,EAAEC,OAAO,EAAE;MACvC;MACA;MACA;MACAvG,KAAK,EAAE6G,UAAU,GAAG,CAACL,OAAO,GAAGK,UAAU,GAAGL,OAAO,IAAIA,OAAO;MAC9DvG,MAAM,EAAE2G,WAAW,GAAG,CAACJ,OAAO,GAAGI,WAAW,GAAGJ,OAAO,IAAIA;IAC9D,CAAC,EAAE;MACCC,IAAI,EAAE,WAAW;MACjBC,MAAM,EAAE,KAAK;MACbC,eAAe,EAAE,CAAC,CAAC;MACnB,GAAG+F;IACP,CAAC,CAAC;EACN;AACJ;AAEA,OAAO,MAAM+E,sBAAsB,SAAS/N,qBAAqB,CAAC;EAC9D;AACJ;AACA;AACA;AACA;AACA;AACA;EACI,MAAMwG,KAAKA,CAACC,MAAM,EAAEuH,OAAO,EAAE;IACzB,IAAI,CAACxK,KAAK,CAACC,OAAO,CAACgD,MAAM,CAAC,EAAE;MACxBA,MAAM,GAAG,CAACA,MAAM,CAAC;IACrB;IACA,IAAI,CAACjD,KAAK,CAACC,OAAO,CAACuK,OAAO,CAAC,EAAE;MACzBA,OAAO,GAAG,CAACA,OAAO,CAAC;IACvB;IAEA,MAAMrH,SAAS,GAAG,MAAMC,OAAO,CAACC,GAAG,CAACJ,MAAM,CAAC/H,GAAG,CAACC,CAAC,IAAI,IAAI,CAAC8G,UAAU,CAAC9G,CAAC,CAAC,CAAC,CAAC;IACxE,MAAMsP,UAAU,GAAG,MAAMrH,OAAO,CAACC,GAAG,CAACmH,OAAO,CAACtP,GAAG,CAACC,CAAC,IAAI,IAAI,CAAC8G,UAAU,CAAC9G,CAAC,EAAE;MACrE6B,YAAY,EAAE,KAAK;MACnBQ,cAAc,EAAE,KAAK;MACrB0E,oBAAoB,EAAE;IAC1B,CAAC,CAAC,CAAC,CAAC;;IAGJ;IACA,MAAMY,YAAY,GAAGxK,KAAK,CAAC6K,SAAS,CAACjI,GAAG;IACpC;IACA,CAACC,CAAC,EAAEnB,CAAC,KAAK5B,GAAG,CAAC,CAAC+C,CAAC,CAAC2H,YAAY,EAAE2H,UAAU,CAACzQ,CAAC,CAAC,CAAC8I,YAAY,CAAC,EAAE,CAAC,CACjE,CAAC,EAAE,CAAC,CAAC;IAEL,OAAO;MACHA,YAAY,EAAEA,YAAY;MAE1B;MACAQ,cAAc,EAAEH,SAAS,CAACjI,GAAG,CAACC,CAAC,IAAIA,CAAC,CAAC4H,aAAa,CAAC;MAEnD;MACAQ,oBAAoB,EAAEJ,SAAS,CAACjI,GAAG,CAACC,CAAC,IAAIA,CAAC,CAACoH,mBAAmB;IAClE,CAAC;EACL;AACJ;AAEA,OAAO,MAAMmI,uBAAuB,SAASpO,gBAAgB,CAAC;EAE1Db,WAAWA,CAACc,MAAM,EAAE;IAChB,KAAK,CAACA,MAAM,CAAC;;IAEb;IACA,IAAI,CAACA,MAAM,CAACoO,WAAW,KAAKjS,eAAe,CACvCuD,IAAI,CAACC,KAAK,CAAC,CAAC,GAAG,IAAI,CAACK,MAAM,CAACqO,KAAK,GAAG,CAAC,CAAC;IAAE;IACvC,IAAI,CAACrO,MAAM,CAACsO,YAAY;IAAE;IAC1B,GAAG;IAAE;IACL,MAAM;IAAE;IACR,IAAI,CAACtO,MAAM,CAACuO,aAAa;IAAE;IAC3B,QAAQ;IAAE;IACV,QAAQ,CAAE;IACd,CAAC;IAED,IAAI,CAACC,MAAM,GAAGvS,eAAe,CAAC,IAAI,CAAC+D,MAAM,CAACqO,KAAK,EAAE,MAAM,CAAC;EAC5D;;EAEA;AACJ;AACA;AACA;AACA;EACII,uBAAuBA,CAACC,QAAQ,EAAE;IAC9B,MAAM;MAAErQ,IAAI;MAAEhB;IAAK,CAAC,GAAGnB,WAAW,CAC9BwS,QAAQ,EACR,IAAI,CAACF,MAAM;IAAE;IACb,IAAI,CAACxO,MAAM,CAACqO,KAAK;IAAE;IACnB,IAAI,CAACrO,MAAM,CAAC2O,UAAU;IAAE;IACxB;MACIC,KAAK,EAAE,GAAG;MACVR,WAAW,EAAE,IAAI,CAACpO,MAAM,CAACoO,WAAW;MACpCS,OAAO,EAAE,OAAO;MAEhB;MACAC,cAAc,EAAE,IAAI,CAAC9O,MAAM,CAAC+O,aAAa,CAAE;IAC/C,CACJ,CAAC;IAED,MAAM3M,QAAQ,GAAG5G,GAAG,CAAC6C,IAAI,CAAC,CAAC,CAAC,CAAC;IAE7B,KAAK,IAAIZ,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGY,IAAI,CAACf,MAAM,EAAE,EAAEG,CAAC,EAAE;MAClCY,IAAI,CAACZ,CAAC,CAAC,GAAG,CAACiC,IAAI,CAAClE,GAAG,CAAC6C,IAAI,CAACZ,CAAC,CAAC,EAAE2E,QAAQ,GAAG,GAAG,CAAC,GAAG,GAAG,IAAI,GAAG;IAC7D;IAEA,OAAO;MAAE/D,IAAI;MAAEhB;IAAK,CAAC;EACzB;;EAEA;AACJ;AACA;AACA;AACA;EACI,MAAMoJ,KAAKA,CAAC3H,KAAK,EAAE;IACfD,qBAAqB,CAACC,KAAK,EAAE,yBAAyB,CAAC;IAEvD,IAAI4P,QAAQ;IACZ,IAAI5P,KAAK,CAACxB,MAAM,GAAG,IAAI,CAAC0C,MAAM,CAACgP,SAAS,EAAE;MACtCrD,OAAO,CAACC,IAAI,CACR,mEAAmE,GACnE,oEAAoE,GACpE,gEACJ,CAAC;MACD8C,QAAQ,GAAG5P,KAAK,CAACqI,KAAK,CAAC,CAAC,EAAE,IAAI,CAACnH,MAAM,CAACgP,SAAS,CAAC;IACpD,CAAC,MAAM;MACH;MACAN,QAAQ,GAAG,IAAI1P,YAAY,CAAC,IAAI,CAACgB,MAAM,CAACgP,SAAS,CAAC;MAClDN,QAAQ,CAACO,GAAG,CAACnQ,KAAK,CAAC;IACvB;IAEA,MAAM;MAAET,IAAI;MAAEhB;IAAK,CAAC,GAAG,IAAI,CAACoR,uBAAuB,CAACC,QAAQ,CAAC;IAE7D,OAAO;MACHQ,cAAc,EAAE,IAAIvT,MAAM,CAAC,SAAS,EAChC0C,IAAI,EACJ,CAAC,CAAC,EAAE,GAAGhB,IAAI,CACf;IACJ,CAAC;EACL;AACJ;AAEA,OAAO,MAAM8R,wBAAwB,SAASpP,gBAAgB,CAAC;EAE3D;AACJ;AACA;AACA;EACIqP,wBAAwBA,CAACC,YAAY,EAAE;IACnC;IACA,MAAMC,GAAG,GAAGD,YAAY,CAAC7F,MAAM,CAAC,CAAC/J,CAAC,EAAEqE,CAAC,KAAKrE,CAAC,GAAGqE,CAAC,EAAE,CAAC,CAAC;IACnD,MAAM3D,IAAI,GAAGmP,GAAG,GAAGD,YAAY,CAAC/R,MAAM;IACtC,MAAMiS,QAAQ,GAAGF,YAAY,CAAC7F,MAAM,CAAC,CAAC/J,CAAC,EAAEqE,CAAC,KAAKrE,CAAC,GAAG,CAACqE,CAAC,GAAG3D,IAAI,KAAK,CAAC,EAAE,CAAC,CAAC,GAAGkP,YAAY,CAAC/R,MAAM;IAC5F,OAAO+R,YAAY,CAAC1Q,GAAG,CAACC,CAAC,IAAI,CAACA,CAAC,GAAGuB,IAAI,IAAIT,IAAI,CAAC8P,IAAI,CAACD,QAAQ,GAAG,IAAI,CAAC,CAAC;EACzE;;EAEA;AACJ;AACA;AACA;AACA;EACI,MAAM9I,KAAKA,CAAC3H,KAAK,EAAE;IACfD,qBAAqB,CAACC,KAAK,EAAE,0BAA0B,CAAC;IAExD,IAAIA,KAAK,YAAYG,YAAY,EAAE;MAC/BH,KAAK,GAAG,IAAIE,YAAY,CAACF,KAAK,CAAC;IACnC;IAEA,IAAIuQ,YAAY,GAAGvQ,KAAK;;IAExB;IACA,IAAI,IAAI,CAACkB,MAAM,CAACS,YAAY,EAAE;MAC1B4O,YAAY,GAAG,IAAI,CAACD,wBAAwB,CAACC,YAAY,CAAC;IAC9D;;IAEA;IACA,MAAM5C,KAAK,GAAG,CAAC,CAAC,EAAE4C,YAAY,CAAC/R,MAAM,CAAC;IACtC,OAAO;MACH+R,YAAY,EAAE,IAAI1T,MAAM,CAAC,SAAS,EAAE0T,YAAY,EAAE5C,KAAK,CAAC;MACxDgD,cAAc,EAAE,IAAI9T,MAAM,CAAC,OAAO,EAAE,IAAI4N,aAAa,CAAC8F,YAAY,CAAC/R,MAAM,CAAC,CAACqG,IAAI,CAAC,EAAE,CAAC,EAAE8I,KAAK;IAC9F,CAAC;EACL;AACJ;AAEA,OAAO,MAAMiD,2BAA2B,SAAS3P,gBAAgB,CAAC;EAE9Db,WAAWA,CAACc,MAAM,EAAE;IAChB,KAAK,CAACA,MAAM,CAAC;IAEb,MAAMuO,aAAa,GAAG,IAAI,CAACvO,MAAM,CAACuO,aAAa;IAC/C,MAAMH,WAAW,GAAGjS,eAAe,CAC/B,GAAG;IAAE;IACL,IAAI,CAAC6D,MAAM,CAAC2P,YAAY;IAAE;IAC1B,EAAE;IAAE;IACJjQ,IAAI,CAACC,KAAK,CAAC4O,aAAa,GAAG,CAAC,CAAC;IAAE;IAC/BA,aAAa;IAAE;IACf,IAAI;IAAE;IACN,OAAO;IAAE;IACT,IAAI,CAAE;IACV,CAAC;;IAED;IACA,KAAK,IAAI9Q,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG2Q,WAAW,CAAC9Q,MAAM,EAAE,EAAEG,CAAC,EAAE;MACzC2Q,WAAW,CAAC3Q,CAAC,CAAC,CAACc,IAAI,CAAC,CAAC,CAAC;IAC1B;IACA,IAAI,CAAC6P,WAAW,GAAGA,WAAW;IAE9B,IAAI,CAACI,MAAM,GAAGvS,eAAe,CAAC,GAAG,EAAE,OAAO,EAAE;MACxC2T,QAAQ,EAAE;IACd,CAAC,CAAC;EACN;;EAEA;AACJ;AACA;AACA;AACA;AACA;EACInB,uBAAuBA,CAACC,QAAQ,EAAEmB,UAAU,EAAE;IAC1C;;IAEA;IACA;IACAnB,QAAQ,GAAGA,QAAQ,CAAC/P,GAAG,CAAC,CAAC,qBAAsBC,CAAC,KAAKA,CAAC,GAAG,KAAK,CAAC;IAE/D,OAAO1C,WAAW,CACdwS,QAAQ,EACR,IAAI,CAACF,MAAM;IAAE;IACb,GAAG;IAAE;IACL,GAAG;IAAE;IACL;MACIsB,UAAU,EAAE,GAAG;MACflB,KAAK,EAAE,GAAG;MACV3L,MAAM,EAAE,KAAK;MACb8M,WAAW,EAAE,IAAI;MACjB3B,WAAW,EAAE,IAAI,CAACA,WAAW;MAC7BS,OAAO,EAAE,KAAK;MACdmB,SAAS,EAAE,qBAAqB;MAChCC,gBAAgB,EAAE,IAAI;MAEtB;MACAnB,cAAc,EAAEe,UAAU;MAC1BK,SAAS,EAAE;IACf,CACJ,CAAC;EACL;;EAEA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACI,MAAMzJ,KAAKA,CAAC3H,KAAK,EAAE;IACfqR,OAAO,GAAG,IAAI;IACdC,kBAAkB,GAAG,CAAC;IACtBC,yBAAyB,GAAG,IAAI;IAChCC,qBAAqB,GAAG;EAC5B,CAAC,GAAG,CAAC,CAAC,EAAE;IACJzR,qBAAqB,CAACC,KAAK,EAAE,6BAA6B,CAAC;IAE3D,IAAIyR,QAAQ,GAAG,IAAI,CAAC9B,uBAAuB,CAAC3P,KAAK,EAAE,IAAI,CAACkB,MAAM,CAAC6P,UAAU,CAAC;IAE1E,IAAIQ,yBAAyB,EAAE;MAC3B,MAAM,CAACG,YAAY,EAAElC,YAAY,CAAC,GAAGiC,QAAQ,CAAClT,IAAI;MAClD,KAAK,IAAII,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG6Q,YAAY,EAAE,EAAE7Q,CAAC,EAAE;QACnC,IAAI6R,GAAG,GAAG,CAAC;QACX,KAAK,IAAItR,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGwS,YAAY,EAAE,EAAExS,CAAC,EAAE;UACnCsR,GAAG,IAAIiB,QAAQ,CAAClS,IAAI,CAACL,CAAC,GAAGsQ,YAAY,GAAG7Q,CAAC,CAAC;QAC9C;QAEA,MAAM0C,IAAI,GAAGmP,GAAG,GAAGkB,YAAY;QAE/B,IAAIjB,QAAQ,GAAG,CAAC;QAChB,KAAK,IAAIvR,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGwS,YAAY,EAAE,EAAExS,CAAC,EAAE;UACnCuR,QAAQ,IAAI,CAACgB,QAAQ,CAAClS,IAAI,CAACL,CAAC,GAAGsQ,YAAY,GAAG7Q,CAAC,CAAC,GAAG0C,IAAI,KAAK,CAAC;QACjE;QACAoP,QAAQ,IAAIiB,YAAY,GAAG,CAAC,CAAC,CAAC;;QAE9B,MAAMnQ,GAAG,GAAGX,IAAI,CAAC8P,IAAI,CAACD,QAAQ,GAAG,IAAI,CAAC;QACtC,KAAK,IAAIvR,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGwS,YAAY,EAAE,EAAExS,CAAC,EAAE;UACnC,MAAMS,KAAK,GAAGT,CAAC,GAAGsQ,YAAY,GAAG7Q,CAAC;UAClC8S,QAAQ,CAAClS,IAAI,CAACI,KAAK,CAAC,GAAG,CAAC8R,QAAQ,CAAClS,IAAI,CAACI,KAAK,CAAC,GAAG0B,IAAI,IAAIE,GAAG;QAC9D;MACJ;IACJ;IAEA,IAAIoQ,qBAAqB;IACzB,IAAIN,OAAO,EAAE;MACT,MAAM,CAACO,UAAU,EAAEC,YAAY,CAAC,GAAGJ,QAAQ,CAAClT,IAAI;MAEhD,MAAM8D,QAAQ,GAAGuP,UAAU,GAAGN,kBAAkB;MAChD,IAAIjP,QAAQ,GAAG,CAAC,EAAE;QACd,MAAMyP,WAAW,GAAG,IAAI5R,YAAY,CAAC2R,YAAY,IAAID,UAAU,GAAGvP,QAAQ,CAAC,CAAC;QAC5EyP,WAAW,CAAC3B,GAAG,CAACsB,QAAQ,CAAClS,IAAI,CAAC;QAC9BuS,WAAW,CAACjN,IAAI,CAAC,IAAI,CAAC3D,MAAM,CAAC6Q,aAAa,EAAEN,QAAQ,CAAClS,IAAI,CAACf,MAAM,CAAC;QAEjE,MAAMwT,eAAe,GAAGJ,UAAU,GAAGvP,QAAQ;QAC7CoP,QAAQ,GAAG;UACPlS,IAAI,EAAEuS,WAAW;UACjBvT,IAAI,EAAE,CAACyT,eAAe,EAAEH,YAAY;QACxC,CAAC;QAED,IAAIL,qBAAqB,EAAE;UACvBG,qBAAqB,GAAG,IAAI9U,MAAM,CAC9B,OAAO,EACP,IAAI4N,aAAa,CAACuH,eAAe,CAAC,EAClC,CAAC,CAAC,EAAEA,eAAe,CACvB,CAAC;UACDL,qBAAqB,CAACpS,IAAI,CAACsF,IAAI,CAAC,EAAE,EAAE,CAAC,EAAE+M,UAAU,CAAC;QACtD;MACJ;IACJ;IAEA,MAAM,CAACA,UAAU,EAAEC,YAAY,CAAC,GAAGJ,QAAQ,CAAClT,IAAI;IAEhD,MAAM0T,MAAM,GAAG,IAAI,CAAC/Q,MAAM,CAAC+Q,MAAM;IACjC,MAAMC,SAAS,GAAGN,UAAU,GAAGK,MAAM;IACrC,IAAIC,SAAS,KAAK,CAAC,EAAE;MACjB,MAAM,IAAIzT,KAAK,CAAC,yBAAyBmT,UAAU,uCAAuCK,MAAM,IAAI,CAAC;IACzG;IAEA,MAAM7B,cAAc,GAAG,IAAIvT,MAAM,CAAC,SAAS,EACvC4U,QAAQ,CAAClS,IAAI,EACbkS,QAAQ,CAAClT,IACb,CAAC,CAAC4T,IAAI,CACF,CAAC,EACDvR,IAAI,CAACC,KAAK,CAAC+Q,UAAU,GAAGK,MAAM,CAAC,EAC/BJ,YAAY,GAAGI,MACnB,CAAC;IAED,MAAM3H,MAAM,GAAG;MAAE8F;IAAe,CAAC;IAEjC,IAAIoB,qBAAqB,EAAE;MACvB,MAAMY,iBAAiB,GAAGhC,cAAc,CAAC7R,IAAI,CAAC,CAAC,CAAC;MAEhD,MAAMoS,cAAc,GAAG,IAAI9T,MAAM,CAC7B,OAAO,EACP,IAAI4N,aAAa,CAAC2H,iBAAiB,CAAC,EACpC,CAAC,CAAC,EAAEA,iBAAiB,CACzB,CAAC;MACD,IAAIT,qBAAqB,EAAE;QACvB,KAAK,IAAIhT,CAAC,GAAG,CAAC,EAAEO,CAAC,GAAG,CAAC,EAAEP,CAAC,GAAGiT,UAAU,EAAEjT,CAAC,IAAIsT,MAAM,EAAE,EAAE/S,CAAC,EAAE;UACrDyR,cAAc,CAACpR,IAAI,CAACL,CAAC,CAAC,GAAGyS,qBAAqB,CAACpS,IAAI,CAACZ,CAAC,CAAC;QAC1D;MACJ,CAAC,MAAM;QACHgS,cAAc,CAACpR,IAAI,CAACsF,IAAI,CAAC,EAAE,CAAC;MAChC;MAEAyF,MAAM,CAACqG,cAAc,GAAGA,cAAc;IAC1C;IAEA,OAAOrG,MAAM;EACjB;AACJ;AAEA,OAAO,MAAM+H,mBAAmB,SAASpR,gBAAgB,CAAC;EAGtDb,WAAWA,CAACc,MAAM,EAAE;IAChB,KAAK,CAACA,MAAM,CAAC;IAEb,MAAMuO,aAAa,GAAG,IAAI,CAACvO,MAAM,CAACuO,aAAa;IAC/C,MAAMH,WAAW,GAAGjS,eAAe,CAC/B,GAAG;IAAE;IACL,IAAI,CAAC6D,MAAM,CAAC2P,YAAY;IAAE;IAC1B,EAAE;IAAE;IACJjQ,IAAI,CAACC,KAAK,CAAC4O,aAAa,GAAG,CAAC,CAAC;IAAE;IAC/BA,aAAa;IAAE;IACf,IAAI;IAAE;IACN,OAAO;IAAE;IACT,IAAI,CAAE;IACV,CAAC;;IAED;IACA,KAAK,IAAI9Q,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG2Q,WAAW,CAAC9Q,MAAM,EAAE,EAAEG,CAAC,EAAE;MACzC2Q,WAAW,CAAC3Q,CAAC,CAAC,CAACc,IAAI,CAAC,CAAC,CAAC;IAC1B;IACA,IAAI,CAAC6P,WAAW,GAAGA,WAAW;IAE9B,IAAI,CAACI,MAAM,GAAGvS,eAAe,CAAC,GAAG,EAAE,MAAM,EAAE;MACvC2T,QAAQ,EAAE;IACd,CAAC,CAAC;IAEF,IAAI,CAACzP,IAAI,GAAG,IAAI,CAACH,MAAM,CAACG,IAAI;IAC5B,IAAI,CAACE,GAAG,GAAG,IAAI,CAACL,MAAM,CAACK,GAAG;EAC9B;;EAEA;AACJ;AACA;AACA;AACA;AACA;EACIoO,uBAAuBA,CAACC,QAAQ,EAAEmB,UAAU,EAAE;IAC1C;IACA,OAAO3T,WAAW,CACdwS,QAAQ,EACR,IAAI,CAACF,MAAM;IAAE;IACb,GAAG;IAAE;IACL,GAAG;IAAE;IACL;MACIsB,UAAU,EAAE,GAAG;MACflB,KAAK,EAAE,GAAG;MACV3L,MAAM,EAAE,KAAK;MACb8M,WAAW,EAAE,IAAI;MACjB3B,WAAW,EAAE,IAAI,CAACA,WAAW;MAC7BS,OAAO,EAAE,KAAK;MACdmB,SAAS,EAAE,qBAAqB;MAChCC,gBAAgB,EAAE,IAAI;MAEtB;MACAnB,cAAc,EAAEe,UAAU;MAC1BK,SAAS,EAAE;IACf,CACJ,CAAC;EACL;;EAGA;AACJ;AACA;AACA;AACA;EACI,MAAMzJ,KAAKA,CAAC3H,KAAK,EAAE;IACfD,qBAAqB,CAACC,KAAK,EAAE,qBAAqB,CAAC;IAEnD,MAAMyR,QAAQ,GAAG,IAAI,CAAC9B,uBAAuB,CAAC3P,KAAK,EAAE,IAAI,CAACkB,MAAM,CAAC6P,UAAU,CAAC;IAC5E,IAAI,IAAI,CAAC7P,MAAM,CAACS,YAAY,EAAE;MAC1B;MACA,MAAM2Q,KAAK,GAAG,IAAI,CAAC/Q,GAAG,GAAG,CAAC;MAC1B,KAAK,IAAI5C,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG8S,QAAQ,CAAClS,IAAI,CAACf,MAAM,EAAE,EAAEG,CAAC,EAAE;QAC3C8S,QAAQ,CAAClS,IAAI,CAACZ,CAAC,CAAC,GAAG,CAAC8S,QAAQ,CAAClS,IAAI,CAACZ,CAAC,CAAC,GAAG,IAAI,CAAC0C,IAAI,IAAIiR,KAAK;MAC7D;IACJ;IAEA,OAAO;MACH/B,YAAY,EAAE,IAAI1T,MAAM,CAAC,SAAS,EAC9B4U,QAAQ,CAAClS,IAAI,EACb,CAAC,CAAC,EAAE,GAAGkS,QAAQ,CAAClT,IAAI,CACxB;IACJ,CAAC;EACL;AACJ;AAEA,OAAO,MAAMgU,oBAAoB,SAAStR,gBAAgB,CAAC;EAEvDb,WAAWA,CAACc,MAAM,EAAE;IAChB,KAAK,CAACA,MAAM,CAAC;IAEb,IAAI,CAACoO,WAAW,GAAGjS,eAAe,CAC9B,IAAI,CAAC6D,MAAM,CAACsR,iBAAiB;IAAE;IAC/B,IAAI,CAACtR,MAAM,CAACsO,YAAY;IAAE;IAC1B,IAAI,CAACtO,MAAM,CAACuR,aAAa;IAAE;IAC3B,IAAI,CAACvR,MAAM,CAACwR,aAAa;IAAE;IAC3B,IAAI,CAACxR,MAAM,CAACuO,aAAa;IAAE;IAC3B,IAAI;IAAE;IACN,KAAK,CAAE;IACX,CAAC;IAED,IAAI,CAACkD,kBAAkB,GAAGtV,eAAe,CACrC,IAAI,CAAC6D,MAAM,CAACsR,iBAAiB;IAAE;IAC/B,IAAI,CAACtR,MAAM,CAACsO,YAAY;IAAE;IAC1B,IAAI,CAACtO,MAAM,CAACuR,aAAa;IAAE;IAC3B,IAAI,CAACvR,MAAM,CAACwR,aAAa;IAAE;IAC3B,IAAI,CAACxR,MAAM,CAACuO,aAAa;IAAE;IAC3B,QAAQ;IAAE;IACV,QAAQ,CAAE;IACd,CAAC;IAED,IAAI,CAACC,MAAM,GAAGvS,eAAe,CAAC,IAAI,CAAC+D,MAAM,CAAC0R,eAAe,EAAE,MAAM,CAAC;EAEtE;;EAGA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACIC,cAAcA,CAACjD,QAAQ,EAAEmB,UAAU,EAAE+B,UAAU,EAAEzB,OAAO,EAAE;IAEtD;IACA,IAAI0B,SAAS;IACb,IAAIC,MAAM,GAAG,KAAK;IAClB,MAAMzP,IAAI,GAAGqM,QAAQ,CAACpR,MAAM,GAAGuS,UAAU;IACzC,IAAIxN,IAAI,GAAG,CAAC,EAAE;MACV,IAAIuP,UAAU,KAAK,YAAY,EAAE;QAC7BE,MAAM,GAAG,IAAI;QACb,MAAMC,GAAG,GAAGrS,IAAI,CAACC,KAAK,CAACD,IAAI,CAACsS,MAAM,CAAC,CAAC,IAAI3P,IAAI,GAAG,CAAC,CAAC,CAAC;QAClDqM,QAAQ,GAAGA,QAAQ,CAACuD,QAAQ,CAACF,GAAG,EAAEA,GAAG,GAAGlC,UAAU,CAAC;QAEnDgC,SAAS,GAAG,IAAI,CAACpD,uBAAuB,CAACC,QAAQ,EAAE,IAAI,CAAC+C,kBAAkB,EAAE,IAAI,CAACzR,MAAM,CAACkS,cAAc,CAAC;QACvGL,SAAS,CAACxU,IAAI,GAAG,CAAC,CAAC,EAAE,GAAGwU,SAAS,CAACxU,IAAI,CAAC,CAAC,CAAC;MAC7C,CAAC,MAAM;QACH;QACA,MAAM,IAAIE,KAAK,CAAC,wBAAwBqU,UAAU,mBAAmB,CAAC;MAC1E;IACJ,CAAC,MAAM;MACH,IAAIvP,IAAI,GAAG,CAAC,EAAE;QACV,IAAI8D,MAAM,GAAG,IAAIlH,YAAY,CAAC4Q,UAAU,CAAC,CAAC,CAAC;QAC3C1J,MAAM,CAAC8I,GAAG,CAACP,QAAQ,CAAC;QAEpB,IAAIyB,OAAO,KAAK,QAAQ,EAAE;UACtB,KAAK,IAAI1S,CAAC,GAAGiR,QAAQ,CAACpR,MAAM,EAAEG,CAAC,GAAGoS,UAAU,EAAEpS,CAAC,IAAIiR,QAAQ,CAACpR,MAAM,EAAE;YAChE6I,MAAM,CAAC8I,GAAG,CAACP,QAAQ,CAACuD,QAAQ,CAAC,CAAC,EAAEvS,IAAI,CAACnE,GAAG,CAACmT,QAAQ,CAACpR,MAAM,EAAEuS,UAAU,GAAGpS,CAAC,CAAC,CAAC,EAAEA,CAAC,CAAC;UAClF;QACJ,CAAC,MAAM,IAAI0S,OAAO,KAAK,WAAW,EAAE;UAChC,KAAK,IAAI1S,CAAC,GAAGiR,QAAQ,CAACpR,MAAM,EAAEG,CAAC,GAAG,CAAC4E,IAAI,EAAE5E,CAAC,IAAIiR,QAAQ,CAACpR,MAAM,EAAE;YAC3D6I,MAAM,CAAC8I,GAAG,CAACP,QAAQ,EAAEjR,CAAC,CAAC;UAC3B;QACJ;QACAiR,QAAQ,GAAGvI,MAAM;MACrB;MAEA,IAAIyL,UAAU,KAAK,QAAQ,EAAE;QACzB,MAAM,IAAIrU,KAAK,CAAC,wBAAwBqU,UAAU,mBAAmB,CAAC;MAC1E;MAEAC,SAAS,GAAG,IAAI,CAACpD,uBAAuB,CAACC,QAAQ,EAAE,IAAI,CAAC+C,kBAAkB,EAAE,IAAI,CAACzR,MAAM,CAACkS,cAAc,CAAC;MACvGL,SAAS,CAACxU,IAAI,GAAG,CAAC,CAAC,EAAE,GAAGwU,SAAS,CAACxU,IAAI,CAAC,CAAC,CAAC;IAC7C;IAEA,OAAO;MACH,GAAGwU,SAAS;MACZC;IACJ,CAAC;EACL;;EAEA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACIrD,uBAAuBA,CAACC,QAAQ,EAAEN,WAAW,EAAEyB,UAAU,GAAG,IAAI,EAAE;IAC9D;IACA,OAAO3T,WAAW,CACdwS,QAAQ,EACR,IAAI,CAACF,MAAM;IAAE;IACb,IAAI,CAACxO,MAAM,CAAC0R,eAAe;IAAE;IAC7B,IAAI,CAAC1R,MAAM,CAAC2O,UAAU;IAAE;IACxB;MACIC,KAAK,EAAE,GAAG;MACVR,WAAW;MACXS,OAAO,EAAE,IAAI;MAEb;MACAC,cAAc,EAAEe,UAAU;MAC1BzO,MAAM,EAAE,KAAK;MACb8O,SAAS,EAAE;IACf,CACJ,CAAC;EACL;;EAGA;AACJ;AACA;AACA;AACA;EACI,MAAMzJ,KAAKA,CAAC3H,KAAK,EAAE;IACf+Q,UAAU,GAAG;EACjB,CAAC,GAAG,CAAC,CAAC,EAAE;IACJhR,qBAAqB,CAACC,KAAK,EAAE,sBAAsB,CAAC;;IAEpD;IACA,MAAMqT,aAAa,GAAG,IAAI,CAACR,cAAc,CACrC7S,KAAK,EACL+Q,UAAU,IAAI,IAAI,CAAC7P,MAAM,CAACkS,cAAc,EACxC,IAAI,CAAClS,MAAM,CAAC4R,UAAU,EACtB,IAAI,CAAC5R,MAAM,CAACmQ,OAChB,CAAC;IAGD,OAAO;MACHjB,cAAc,EAAE,IAAIvT,MAAM,CAAC,SAAS,EAChCwW,aAAa,CAAC9T,IAAI,EAClB,CAAC,CAAC,EAAE,GAAG8T,aAAa,CAAC9U,IAAI,CAC7B;IACJ,CAAC;EACL;AACJ;AAIA,OAAO,MAAM+U,wBAAwB,SAASrS,gBAAgB,CAAC;;AAE/D;AACA;AACA;AACA;AACA,OAAO,MAAMsS,SAAS,SAASlX,QAAQ,CAAC;EACpC;AACJ;AACA;AACA;EACI+D,WAAWA,CAACH,iBAAiB,EAAE;IAC3B,KAAK,CAAC,CAAC;IACP,IAAI,CAACA,iBAAiB,GAAGA,iBAAiB;IAC1C;EACJ;;EAEA;AACJ;AACA;AACA;AACA;AACA;EACI,MAAM0H,KAAKA,CAAC6L,KAAK,EAAE,GAAG3L,IAAI,EAAE;IACxB,OAAO,MAAM,IAAI,CAAC5H,iBAAiB,CAACuT,KAAK,EAAE,GAAG3L,IAAI,CAAC;EACvD;AACJ;AAEA,OAAO,MAAM4L,YAAY,SAASF,SAAS,CAAC;EACxC;AACJ;AACA;EACI,MAAM5L,KAAKA,CAAC,GAAGE,IAAI,EAAE;IACjB,OAAO,MAAM,IAAI,CAAC5H,iBAAiB,CAAC,GAAG4H,IAAI,CAAC;EAChD;;EAEA;AACJ;AACA;EACI0G,kBAAkBA,CAAC,GAAG1G,IAAI,EAAE;IACxB;IACA,OAAO,IAAI,CAAC5H,iBAAiB,CAACsO,kBAAkB,CAAC,GAAG1G,IAAI,CAAC;EAC7D;EACA;AACJ;AACA;EACI2F,oBAAoBA,CAAC,GAAG3F,IAAI,EAAE;IAC1B;IACA,OAAO,IAAI,CAAC5H,iBAAiB,CAACuN,oBAAoB,CAAC,GAAG3F,IAAI,CAAC;EAC/D;AACJ;;AAEA;AACA;AACA;AACA;AACA,OAAO,MAAM6L,gBAAgB,SAASH,SAAS,CAAC;EAC5C;AACJ;AACA;AACA;AACA;EACI,MAAM5L,KAAKA,CAAC3H,KAAK,EAAE;IACf,OAAO,MAAM,IAAI,CAACC,iBAAiB,CAACD,KAAK,CAAC;EAC9C;AACJ;AAGA,OAAO,MAAM2T,uBAAuB,SAASJ,SAAS,CAAC;EACnD;AACJ;AACA;AACA;AACA;EACI,MAAM5L,KAAKA,CAAC3H,KAAK,EAAE;IACf,OAAO,MAAM,IAAI,CAACC,iBAAiB,CAACD,KAAK,CAAC;EAC9C;AACJ;AAEA,OAAO,MAAM4T,iBAAiB,SAASL,SAAS,CAAC;EAC7C;AACJ;AACA;AACA;AACA;EACI,MAAM5L,KAAKA,CAAC6L,KAAK,EAAE;IACf,OAAO,MAAM,IAAI,CAACvT,iBAAiB,CAACuT,KAAK,CAAC;EAC9C;AACJ;AAEA,OAAO,MAAMK,eAAe,SAASN,SAAS,CAAC;;AAG/C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMO,aAAa,CAAC;EACvB,OAAOC,+BAA+B,GAAG;IACrC5S,qBAAqB;IACrBkO,uBAAuB;IACvB7F,mBAAmB;IACnBI,yBAAyB;IACzBC,uBAAuB;IACvBC,sBAAsB;IACtBC,mBAAmB;IACnBd,oBAAoB;IACpBC,2BAA2B;IAC3BC,oBAAoB;IACpBC,wBAAwB;IACxBG,sBAAsB;IACtBpB,yBAAyB;IACzBY,iBAAiB;IACjBD,iBAAiB;IACjBD,mBAAmB;IACnBG,oBAAoB;IACpBiB,oBAAoB;IACpBD,oBAAoB;IACpBK,oBAAoB;IACpBiD,qBAAqB;IACrBpD,qBAAqB;IACrBE,oBAAoB;IACpBV,0BAA0B;IAE1BD,iBAAiB;IACjByF,sBAAsB;IACtB3B,iBAAiB;IACjB0B,qBAAqB;IACrBoB,wBAAwB;IACxBO,2BAA2B;IAC3B0C,wBAAwB;IACxBjB,mBAAmB;IACnBE;EACJ,CAAC;EAED,OAAOyB,uBAAuB,GAAG;IAC7BN,gBAAgB;IAChBC,uBAAuB;IACvBF,YAAY;IACZG,iBAAiB;IACjBC;EACJ,CAAC;;EAED;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACI,aAAaI,eAAeA,CAACC,6BAA6B,EAAE;IACxDC,iBAAiB,GAAG,IAAI;IACxBjT,MAAM,GAAG,IAAI;IACbkT,SAAS,GAAG,IAAI;IAChBC,gBAAgB,GAAG,KAAK;IACxBC,QAAQ,GAAG;EACf,CAAC,GAAG,CAAC,CAAC,EAAE;IAEJ,IAAIC,kBAAkB,GAAGrT,MAAM,KAAI,MAAM1E,YAAY,CAAC0X,6BAA6B,EAAE,0BAA0B,EAAE,IAAI,EAAE;MACnHC,iBAAiB;MACjBjT,MAAM;MACNkT,SAAS;MACTC,gBAAgB;MAChBC;IACJ,CAAC,CAAC;;IAEF;IACA;IACA,IAAIE,GAAG,GAAGD,kBAAkB,CAACE,sBAAsB,IAAIF,kBAAkB,CAACG,oBAAoB;IAC9F,IAAIC,uBAAuB,GAAG,IAAI,CAACZ,+BAA+B,CAACS,GAAG,CAAC;IAEvE,IAAI,CAACG,uBAAuB,EAAE;MAC1B,IAAIJ,kBAAkB,CAACzS,IAAI,KAAKS,SAAS,EAAE;QACvC;QACAsK,OAAO,CAACC,IAAI,CAAC,2BAA2B0H,GAAG,8EAA8E,CAAC;QAC1HG,uBAAuB,GAAGxT,qBAAqB;MACnD,CAAC,MAAM;QACH,MAAM,IAAI1C,KAAK,CAAC,mCAAmC+V,GAAG,EAAE,CAAC;MAC7D;IACJ;;IAEA;IACA,IAAII,eAAe,GAAG,IAAI,CAACZ,uBAAuB,CAACO,kBAAkB,CAACK,eAAe,CAAC,IAAIrB,SAAS;;IAEnG;IACA,IAAItT,iBAAiB,GAAG,IAAI0U,uBAAuB,CAACJ,kBAAkB,CAAC;IACvE,OAAO,IAAIK,eAAe,CAAC3U,iBAAiB,CAAC;EACjD;AACJ;AACA","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}